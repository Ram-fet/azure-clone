<!doctype html>
<html>
<head>
<meta charset="utf-8"/>
<title>2501.09223v2.pdf — Semantic HTML</title>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<style>
  :root { color-scheme: light dark }
  body {
    margin: 24px;
    background: #0f1115;
    color: #e6e6e6;
    font: 16px/1.55 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Helvetica,Arial,sans-serif;
  }
  h1, h2, h3 { margin: 1rem 0 .5rem; line-height: 1.2 }
  h1 { font-size: 1.9rem }
  h2 { font-size: 1.6rem }
  h3 { font-size: 1.35rem }
  .para p { margin: .65rem 0; }
  ul { margin: .6rem 1.2rem; }
</style>
</head>
<body>
  <div style="opacity:.75;margin-bottom:12px">
    Generated from: <strong>2501.09223v2.pdf</strong>
  </div>
  <h1>Foundations of Large Language Models</h1>
<h2>arXiv:2501.09223v2  [cs.CL]  15 Jun 2025</h2>
<div class="para"><p>Tong Xiao and Jingbo Zhu</p></div>
<div class="para"><p>June 17, 2025</p></div>
<div class="para"><p>NLP Lab, Northeastern University &amp; NiuTrans Research</p></div>
<div class="para"><p>This book is a selection of chapters from an introductory NLP resource available at https://github.com/NiuTrans/NLPBook</p></div>
<div class="para"><p>Copyright © 2021-2025 Tong Xiao and Jingbo Zhu</p></div>
<div class="para"><p>N ATURAL L ANGUAGE P ROCESSING L AB, N ORTHEASTERN U NIVERSITY&amp;N IU T RANS R ESEARCH</p></div>
<div class="para"><p>Licensed under the Creative Commons Attribution-NonCommercial 4.0 Unported License (the“License”). You may not use this file except in compliance with the License. You may obtain a copy of the License at http://creativecommons.org/licenses/by-nc/4.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p></div>
<div class="para"><p><em>June 17, 2025</em></p></div>
<h1>Preface</h1>
<div class="para"><p>Large language models originated from natural language processing, but they have undoubtedly become one of the most revolutionary technological advancements in the field of artificial intelligence in recent years. An important insight brought by large language models is that knowledge of the world and languages can be acquired through large-scale language modeling tasks, and in this way, we can create a universal model that handles diverse problems. This discovery has profoundly impacted the research methodologies in natural language processing and many related disciplines. We have shifted from training specialized systems from scratch using a large amount of labeled data to a new paradigm of using large-scale pre-training to obtain foundation models,which are then fine-tuned, aligned, and prompted.</p></div>
<div class="para"><p>This book aims to outline the basic concepts of large language models and introduce the related techniques. As the title suggests, the book focuses more on the foundational aspects of large language models rather than providing comprehensive coverage of all cutting-edge methods.The book consists of five chapters:</p></div>
<div class="para"><p>• Chapter 1 introduces the basics of pre-training. This is the foundation of large language models, and common pre-training methods and model architectures will be discussed here.</p></div>
<div class="para"><p>• Chapter 2 introduces generative models, which are the large language models we commonly refer to today. After presenting the basic process of building these models, we will also explore how to scale up model training and handle long texts.</p></div>
<div class="para"><p>• Chapter 3 introduces prompting methods for large language models. We will discuss various prompting strategies, along with more advanced methods such as chain-of-thought reasoning and automatic prompt design.</p></div>
<div class="para"><p>• Chapter 4 introduces alignment methods for large language models. We will focus on instruction fine-tuning and alignment based on human feedback.</p></div>
<div class="para"><p>• Chapter 5 introduces inference methods for large language models. We will discuss various decoding algorithms, acceleration methods, and the inference-time scaling issue.</p></div>
<div class="para"><p>If readers have some background in machine learning and natural language processing, along with a certain understanding of neural networks like Transformers, reading this book will be quite easy. However, even without this prior knowledge, it is still perfectly fine, as we have made the content of each chapter as self-contained as possible, ensuring that readers will not be burdened with too much reading difficulty.</p></div>
<div class="para"><p>The content presented here is part of a comprehensive introductory resource on neural networks and large language models in natural language processing. For readers who wish to learn more about background topics, such as sequence modeling and attention mechanisms, you can visit https://github.com/NiuTrans/NLPBook or https://niutrans.github.io/NLPBook for further information.</p></div>
<div class="para"><p>We would like to thank the students in our laboratory and all our friends who have shared with us their views on large language models and helped with corrections of errors in writing. In particular, we wish to thank Weiqiao Shan, Yongyu Mu, Chenglong Wang, Kaiyan Chang, Yuchun Fan, Hang Zhou, Chuanhao Lv, Xinyu Liu, Tao Zhou, Huiwen Bao, Tong Zheng, Junhao Ruan,Yingfeng Luo, Yuzhang Wu, and Yifu Huo.</p></div>
<div class="para"><p>ii</p></div>
<h1>Notation</h1>
<div class="para"><p><em>a</em> variable</p></div>
<div class="para"><p><strong>a</strong> row vector or matrix</p></div>
<div class="para"><p><em>f</em>(<em>a</em>)function of<em> a</em></p></div>
<div class="para"><p>max<em> f</em>(<em>a</em>)maximum value of<em> f</em>(<em>a</em>)</p></div>
<div class="para"><p>arg max <em>a</em><em> f</em>(<em>a</em>)value of<em> a</em> that maximizes<em> f</em>(<em>a</em>)</p></div>
<div class="para"><p><strong>x</strong> input token sequence to a model</p></div>
<div class="para"><p><em>x</em> <em>j</em> input token at position<em> j</em></p></div>
<div class="para"><p><strong>y</strong> output token sequence produced by a model</p></div>
<div class="para"><p><em>y</em> <em>i</em> output token at position<em> i</em></p></div>
<div class="para"><p><em>θ</em> model parameters</p></div>
<div class="para"><p>Pr(<em>a</em>)probability of<em> a</em></p></div>
<div class="para"><p>Pr(<em>a</em><em>|</em><em>b</em>)conditional probability of<em> a</em> given<em> b</em></p></div>
<div class="para"><p>Pr(<em>·|</em><em>b</em>)probability distribution of a variable given<em> b</em></p></div>
<div class="para"><p>Pr <em>θ</em>(<em>a</em>)probability of<em> a</em> as parameterized by<em> θ</em></p></div>
<div class="para"><p><strong>h</strong> <em>t</em> hidden state at time step<em> t</em> in sequential models</p></div>
<div class="para"><p><strong>H</strong> matrix of all hidden states over time in a sequence</p></div>
<div class="para"><p><strong>Q</strong>,<strong> K</strong>,<strong> V</strong> query, key, and value matrices in attention mechanisms</p></div>
<div class="para"><p>Softmax(<strong>A</strong>)Softmax function that normalizes the input vector or matrix<strong> A</strong></p></div>
<div class="para"><p><em>L</em> loss function</p></div>
<div class="para"><p><em>D</em> dataset used for training or fine-tuning a model</p></div>
<div class="para"><p><em>∂</em><em>L</em></p></div>
<div class="para"><p><em>∂θ</em> gradient of the loss function<em> L</em> with respect to the parameters<em> θ</em></p></div>
<div class="para"><p>KL(<em>p</em><em> ||</em><em> q</em>)KL divergence between distributions<em> p</em> and<em> q</em></p></div>
<div class="para"><p>iii</p></div>
<h1>Contents</h1>
<div class="para"><p><strong>1</strong> <strong>Pre-training</strong> <strong>1</strong></p></div>
<div class="para"><p>1.1 Pre-training NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1</p></div>
<div class="para"><p>1.1.1 Unsupervised, Supervised and Self-supervised Pre-training . . . . . . . .2</p></div>
<div class="para"><p>1.1.2 Adapting Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . .3</p></div>
<div class="para"><p>1.2 Self-supervised Pre-training Tasks . . . . . . . . . . . . . . . . . . . . . . . . .7</p></div>
<div class="para"><p>1.2.1 Decoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . .7</p></div>
<div class="para"><p>1.2.2 Encoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . .8</p></div>
<div class="para"><p>1.2.3 Encoder-Decoder Pre-training . . . . . . . . . . . . . . . . . . . . . . .15</p></div>
<div class="para"><p>1.2.4 Comparison of Pre-training Tasks . . . . . . . . . . . . . . . . . . . . .20</p></div>
<div class="para"><p>1.3 Example: BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .21</p></div>
<div class="para"><p>1.3.1 The Standard Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .21</p></div>
<div class="para"><p>1.3.2 More Training and Larger Models . . . . . . . . . . . . . . . . . . . . .27</p></div>
<div class="para"><p>1.3.3 More Efficient Models . . . . . . . . . . . . . . . . . . . . . . . . . . .27</p></div>
<div class="para"><p>1.3.4 Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .28</p></div>
<div class="para"><p>1.4 Applying BERT Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .30</p></div>
<div class="para"><p>1.5 Summary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .35</p></div>
<div class="para"><p><strong>2</strong> <strong>Generative Models</strong> <strong>36</strong></p></div>
<div class="para"><p>2.1 A Brief Introduction to LLMs. . . . . . . . . . . . . . . . . . . . . . . . . . .37</p></div>
<div class="para"><p>2.1.1 Decoder-only Transformers. . . . . . . . . . . . . . . . . . . . . . . .38</p></div>
<div class="para"><p>2.1.2 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .40</p></div>
<div class="para"><p>2.1.3 Fine-tuning LLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . .42</p></div>
<div class="para"><p>2.1.4 Aligning LLMs with the World. . . . . . . . . . . . . . . . . . . . . .46</p></div>
<div class="para"><p>2.1.5 Prompting LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .51</p></div>
<div class="para"><p>2.2 Training at Scale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .56</p></div>
<div class="para"><p>2.2.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .56</p></div>
<div class="para"><p>2.2.2 Model Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . .57</p></div>
<div class="para"><p>2.2.3 Distributed Training. . . . . . . . . . . . . . . . . . . . . . . . . . . .60</p></div>
<div class="para"><p>2.2.4 Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63</p></div>
<div class="para"><p>2.3 Long Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .66</p></div>
<div class="para"><p>2.3.1 Optimization from HPC Perspectives. . . . . . . . . . . . . . . . . . .67</p></div>
<div class="para"><p>2.3.2 Efficient Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . .68</p></div>
<div class="para"><p>2.3.3 Cache and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .70</p></div>
<div class="para"><p>2.3.4 Sharing across Heads and Layers. . . . . . . . . . . . . . . . . . . . .79</p></div>
<div class="para"><p>iv</p></div>
<div class="para"><p>v</p></div>
<div class="para"><p>2.3.5 Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . .81</p></div>
<div class="para"><p>2.3.6 Remarks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .92</p></div>
<div class="para"><p>2.4 Summary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94</p></div>
<div class="para"><p><strong>3</strong> <strong>Prompting</strong> <strong>96</strong></p></div>
<div class="para"><p>3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .97</p></div>
<div class="para"><p>3.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .97</p></div>
<div class="para"><p>3.1.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .99</p></div>
<div class="para"><p>3.1.3 Prompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . .101</p></div>
<div class="para"><p>3.1.4 More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .106</p></div>
<div class="para"><p>3.2 Advanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .115</p></div>
<div class="para"><p>3.2.1 Chain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .115</p></div>
<div class="para"><p>3.2.2 Problem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . .117</p></div>
<div class="para"><p>3.2.3 Self-refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .124</p></div>
<div class="para"><p>3.2.4 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .130</p></div>
<div class="para"><p>3.2.5 RAG and Tool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . .134</p></div>
<div class="para"><p>3.3 Learning to Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .138</p></div>
<div class="para"><p>3.3.1 Prompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . .139</p></div>
<div class="para"><p>3.3.2 Soft Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .142</p></div>
<div class="para"><p>3.3.3 Prompt Length Reduction. . . . . . . . . . . . . . . . . . . . . . . . .152</p></div>
<div class="para"><p>3.4 Summary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .153</p></div>
<div class="para"><p><strong>4</strong> <strong>Alignment</strong> <strong>155</strong></p></div>
<div class="para"><p>4.1 An Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . .155</p></div>
<div class="para"><p>4.2 Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .157</p></div>
<div class="para"><p>4.2.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . .157</p></div>
<div class="para"><p>4.2.2 Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . .161</p></div>
<div class="para"><p>4.2.3 Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . .166</p></div>
<div class="para"><p>4.2.4 Instruction Generalization . . . . . . . . . . . . . . . . . . . . . . . . .167</p></div>
<div class="para"><p>4.2.5 Using Weak Models to Improve Strong Models . . . . . . . . . . . . . .169</p></div>
<div class="para"><p>4.3 Human Preference Alignment: RLHF. . . . . . . . . . . . . . . . . . . . . . .172</p></div>
<div class="para"><p>4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . .173</p></div>
<div class="para"><p>4.3.2 Training Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . .179</p></div>
<div class="para"><p>4.3.3 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .182</p></div>
<div class="para"><p>4.4 Improved Human Preference Alignment . . . . . . . . . . . . . . . . . . . . . .187</p></div>
<div class="para"><p>4.4.1 Better Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . .187</p></div>
<div class="para"><p>vi <em>Notation</em></p></div>
<div class="para"><p>4.4.2 Direct Preference Optimization. . . . . . . . . . . . . . . . . . . . . .193</p></div>
<div class="para"><p>4.4.3 Automatic Preference Data Generation. . . . . . . . . . . . . . . . . .196</p></div>
<div class="para"><p>4.4.4 Step-by-step Alignment. . . . . . . . . . . . . . . . . . . . . . . . . .198</p></div>
<div class="para"><p>4.4.5 Inference-time Alignment. . . . . . . . . . . . . . . . . . . . . . . . .200</p></div>
<div class="para"><p>4.5 Summary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .201</p></div>
<div class="para"><p><strong>5</strong> <strong>Inference</strong> <strong>203</strong></p></div>
<div class="para"><p>5.1 Prefilling and Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .204</p></div>
<div class="para"><p>5.1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .204</p></div>
<div class="para"><p>5.1.2 A Two-phase Framework . . . . . . . . . . . . . . . . . . . . . . . . . .207</p></div>
<div class="para"><p>5.1.3 Decoding Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . .211</p></div>
<div class="para"><p>5.1.4 Evaluation Metrics for LLM Inference . . . . . . . . . . . . . . . . . . .221</p></div>
<div class="para"><p>5.2 Efficient Inference Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . .222</p></div>
<div class="para"><p>5.2.1 More Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .223</p></div>
<div class="para"><p>5.2.2 Batching. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .223</p></div>
<div class="para"><p>5.2.3 Parallelization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .232</p></div>
<div class="para"><p>5.2.4 Remarks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .233</p></div>
<div class="para"><p>5.3 Inference-time Scaling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .234</p></div>
<div class="para"><p>5.3.1 Context Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .235</p></div>
<div class="para"><p>5.3.2 Search Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .236</p></div>
<div class="para"><p>5.3.3 Output Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . .237</p></div>
<div class="para"><p>5.3.4 Generating and Verifying Thinking Paths . . . . . . . . . . . . . . . . .238</p></div>
<div class="para"><p>5.4 Summary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .245</p></div>
<div class="para"><p><strong>Bibliography</strong> <strong>247</strong></p></div>
<div class="para"><p>https://github.com/NiuTrans/NLPBook</p></div>
<div class="para"><p>https://niutrans.github.io/NLPBook</p></div>
<h3>C HAPTER 1</h3>
<h1>Pre-training</h1>
<div class="para"><p>The development of neural sequence models, such as<strong> Transformers</strong> [Vaswani et al., 2017], along with the improvements in large-scale self-supervised learning, has opened the door to universal language understanding and generation. This achievement is largely motivated by pre-training:we separate common components from many neural network-based systems, and then train them on huge amounts of unlabeled data using self-supervision. These pre-trained models serve as foundation models that can be easily adapted to different tasks via fine-tuning or prompting. As a result, the paradigm of NLP has been enormously changed. In many cases, large-scale supervised learning for specific tasks is no longer required, and instead, we only need to adapt pre-trained foundation models.</p></div>
<div class="para"><p>While pre-training has gained popularity in recent NLP research, this concept dates back decades to the early days of deep learning. For example, early attempts to pre-train deep learning systems include unsupervised learning for RNNs, deep feedforward networks, autoencoders, and others [Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurgence of pre-training, caused in part by the large-scale unsupervised learning of various word embedding models [Mikolov et al., 2013b; Pennington et al., 2014]. During the same period, pre-training also attracted significant interest in computer vision, where the backbone models were trained on relatively large labeled datasets such as ImageNet, and then applied to different downstream tasks[He et al., 2019; Zoph et al., 2020]. Large-scale research on pre-training in NLP began with the development of language models using self-supervised learning. This family of models covers several well-known examples like<strong> BERT</strong> [Devlin et al., 2019] and<strong> GPT</strong> [Brown et al., 2020], all with a similar idea that general language understanding and generation can be achieved by training the models to predict masked words in a huge amount of text. Despite the simple nature of this approach, the resulting models show remarkable capability in modeling linguistic structure,though they are not explicitly trained to achieve this. The generality of the pre-training tasks leads to systems that exhibit strong performance in a large variety of NLP problems, even outperforming previously well-developed supervised systems. More recently, pre-trained large language models have achieved greater success, showing the exciting prospects for more general artificial intelligence [Bubeck et al., 2023].</p></div>
<div class="para"><p>This chapter discusses the concept of pre-training in the context of NLP. It begins with a general introduction to pre-training methods and their applications. BERT is then used as an example to illustrate how a sequence model is trained via a self-supervised task, called<strong> masked language</strong> <strong>modeling</strong>. This is followed by a discussion of methods for adapting pre-trained sequence models for various NLP tasks. Note that in this chapter, we will focus primarily on the pre-training paradigm in NLP, and therefore, we do not intend to cover details about generative large language models. A detailed discussion of these models will be left to subsequent chapters.</p></div>
<h3>1.1 Pre-training NLP Models</h3>
<div class="para"><p>The discussion of pre-training issues in NLP typically involves two types of problems: sequence modeling (or sequence encoding) and sequence generation. While these problems have different</p></div>
<div class="para"><p>2 <em>Pre-training</em></p></div>
<div class="para"><p>forms, for simplicity, we describe them using a single model defined as follows:</p></div>
<div class="para"><p><strong>o</strong>=<em>g</em>(<em>x</em> 0<em>, x</em> 1<em>, ..., x</em> <em>m</em>;<em> θ</em>)</p></div>
<div class="para"><p>=<em>g</em> <em>θ</em>(<em>x</em> 0<em>, x</em> 1<em>, ..., x</em> <em>m</em>)(1.1)</p></div>
<div class="para"><p>where<em> {</em><em>x</em> 0<em>, x</em> 1<em>, ..., x</em> <em>m</em><em>}</em> denotes a sequence of input tokens 1,<em> x</em> 0 denotes a special symbol (<em>⟨</em><em>s</em><em>⟩</em>or[CLS]) attached to the beginning of a sequence,<em> g</em>(<em>·</em>;<em> θ</em>) (also written as<em> g</em> <em>θ</em>(<em>·</em>)) denotes a neural network with parameters<em> θ</em>, and<strong> o</strong> denotes the output of the neural network. Different problems can vary based on the form of the output<strong> o</strong>. For example, in token prediction problems (as in language modeling),<strong> o</strong> is a distribution over a vocabulary; in sequence encoding problems,<strong> o</strong> is a representation of the input sequence, often expressed as a real-valued vector sequence.</p></div>
<div class="para"><p>There are two fundamental issues here.</p></div>
<div class="para"><p>• Optimizing<em> θ</em> on a pre-training task. Unlike standard learning problems in NLP, pre-training does not assume specific downstream tasks to which the model will be applied. Instead, the goal is to train a model that can generalize across various tasks.</p></div>
<div class="para"><p>• Applying the pre-trained model<em> g</em> ˆ <em>θ</em>(<em>·</em>) to downstream tasks. To adapt the model to these tasks, we need to adjust the parameters ˆ <em>θ</em> slightly using labeled data or prompt the model with task descriptions.</p></div>
<div class="para"><p>In this section, we discuss the basic ideas in addressing these issues.</p></div>
<div class="para"><p><strong>1.1.1</strong> <strong>Unsupervised, Supervised and Self-supervised Pre-training</strong></p></div>
<div class="para"><p>In deep learning, pre-training refers to the process of optimizing a neural network before it is further trained/tuned and applied to the tasks of interest. This approach is based on an assumption that a model pre-trained on one task can be adapted to perform another task. As a result, we do not need to train a deep, complex neural network from scratch on tasks with limited labeled data.Instead, we can make use of tasks where supervision signals are easier to obtain. This reduces the reliance on task-specific labeled data, enabling the development of more general models that are not confined to particular problems.</p></div>
<div class="para"><p>During the resurgence of neural networks through deep learning, many early attempts to achieve pre-training were focused on<strong> unsupervised learning</strong>. In these methods, the parameters of a neural network are optimized using a criterion that is not directly related to specific tasks.For example, we can minimize the reconstruction cross-entropy of the input vector for each layer[Bengio et al., 2006]. Unsupervised pre-training is commonly employed as a preliminary step before supervised learning, offering several advantages, such as aiding in the discovery of better local minima and adding a regularization effect to the training process [Erhan et al., 2010]. These benefits make the subsequent supervised learning phase easier and more stable.</p></div>
<div class="para"><p>A second approach to pre-training is to pre-train a neural network on<strong> supervised learning</strong> tasks. For example, consider a sequence model designed to encode input sequences into some</p></div>
<div class="para"><p>1 Here we assume that tokens are basic units of text that are separated through tokenization. Sometimes, we will use the terms<em> token</em> and<em> word</em> interchangeably, though they have closely related but slightly different meanings in NLP.</p></div>
<div class="para"><p><em>1.1 Pre-training NLP Models</em> 3</p></div>
<div class="para"><p>representations. In pre-training, this model is combined with a classification layer to form a classification system. This system is then trained on a pre-training task, such as classifying sentences based on sentiment (e.g., determining if a sentence conveys a positive or negative sentiment).Then, we adapt the sequence model to a downstream task. We build a new classification system based on this pre-trained sequence model and a new classification layer (e.g., determining if a sequence is subjective or objective). Typically, we need to fine-tune the parameters of the new model using task-specific labeled data, ensuring the model is optimally adjusted to perform well on this new type of data. The fine-tuned model is then employed to classify new sequences for this task. An advantage of supervised pre-training is that the training process, either in the pretraining or fine-tuning phase, is straightforward, as it follows the well-studied general paradigm of supervised learning in machine learning. However, as the complexity of the neural network increases, the demand for more labeled data also grows. This, in turn, makes the pre-training task more difficult, especially when large-scale labeled data is not available.</p></div>
<div class="para"><p>A third approach to pre-training is<strong> self-supervised learning</strong>. In this approach, a neural network is trained using the supervision signals generated by itself, rather than those provided by humans. This is generally done by constructing its own training tasks directly from unlabeled data, such as having the system create pseudo labels. While self-supervised learning has recently emerged as a very popular method in NLP, it is not a new concept. In machine learning, a related concept is<strong> self-training</strong> where a model is iteratively improved by learning from the pseudo labels assigned to a dataset. To do this, we need some seed data to build an initial model. This model then generates pseudo labels for unlabeled data, and these pseudo labels are subsequently used to iteratively refine and bootstrap the model itself. Such a method has been successfully used in several NLP areas, such as word sense disambiguation [Yarowsky, 1995] and document classification[Blum and Mitchell, 1998]. Unlike the standard self-training method, self-supervised pre-training in NLP does not rely on an initial model for annotating the data. Instead, all the supervision signals are created from the text, and the entire model is trained from scratch. A well-known example of this is training sequence models by successively predicting a masked word given its preceding or surrounding words in a text. This enables large-scale self-supervised learning for deep neural networks, leading to the success of pre-training in many understanding, writing, and reasoning tasks.</p></div>
<div class="para"><p>Figure 1.1 shows a comparison of the above three pre-training approaches. Self-supervised pre-training is so successful that most current state-of-the-art NLP models are based on this paradigm. Therefore, in this chapter and throughout this book, we will focus on self-supervised pre-training. We will show how sequence models are pre-trained via self-supervision and how the pre-trained models are applied.</p></div>
<div class="para"><p><strong>1.1.2</strong> <strong>Adapting Pre-trained Models</strong></p></div>
<div class="para"><p>As mentioned above, two major types of models are widely used in NLP pre-training.</p></div>
<div class="para"><p>•<strong> Sequence Encoding Models</strong>. Given a sequence of words or tokens, a sequence encoding model represents this sequence as either a real-valued vector or a sequence of vectors, and obtains a representation of the sequence. This representation is typically used as input to another model, such as a sentence classification system.</p></div>
<div class="para"><p>4 <em>Pre-training</em></p></div>
<div class="para"><p>Prompting</p></div>
<div class="para"><p>Zero/Few Shot Learning</p></div>
<div class="para"><p>Pre-training Training</p></div>
<div class="para"><p>Pre-training Tuning</p></div>
<div class="para"><p>Pre-training Tuning</p></div>
<div class="para"><p>Self-Supervised Supervised</p></div>
<div class="para"><p>Unsupervised Supervised</p></div>
<div class="para"><p>Supervised Supervised</p></div>
<div class="para"><p>Unlabeled Data</p></div>
<div class="para"><p>Labeled Data</p></div>
<div class="para"><p>Labeled Data</p></div>
<div class="para"><p>Labeled Data</p></div>
<div class="para"><p>Unlabeled Data</p></div>
<div class="para"><p>Labeled Data</p></div>
<div class="para"><p>Task 2</p></div>
<div class="para"><p>Task 1</p></div>
<div class="para"><p>(a) Unsupervised Pre-training</p></div>
<div class="para"><p>(b) Supervised Pre-training</p></div>
<div class="para"><p>(c) Self-supervised Pre-training</p></div>
<div class="para"><p><strong>Fig. 1.1:</strong> Illustration of unsupervised, supervised, and self-supervised pre-training. In unsupervised pre-training, the pre-training is performed on large-scale unlabeled data. It can be viewed as a preliminary step to have a good starting point for the subsequent optimization process, though considerable effort is still required to further train the model with labeled data after pre-training. In supervised pre-training, the underlying assumption is that different (supervised)learning tasks are related. So we can first train the model on one task, and transfer the resulting model to another task with some training or tuning effort. In self-supervised pre-training, a model is pre-trained on large-scale unlabeled data via self-supervision. The model can be well trained in this way, and we can efficiently adapt it to new tasks through fine-tuning or prompting.</p></div>
<div class="para"><p>•<strong> Sequence Generation Models</strong>. In NLP, sequence generation generally refers to the problem of generating a sequence of tokens based on a given context. The term<em> context</em> has different meanings across applications. For example, it refers to the preceding tokens in language modeling, and refers to the source-language sequence in machine translation 2.</p></div>
<div class="para"><p>We need different techniques for applying these models to downstream tasks after pre-training.Here we are interested in the following two methods.</p></div>
<div class="para"><p><strong>1.1.2.1</strong> <strong>Fine-tuning of Pre-trained Models</strong></p></div>
<div class="para"><p>For sequence encoding pre-training, a common method of adapting pre-trained models is finetuning. Let Encode <em>θ</em>(<em>·</em>) denote an encoder with parameters<em> θ</em>, for example, Encode <em>θ</em>(<em>·</em>) can be a standard Transformer encoder. Provided we have pre-trained this model in some way and obtained the optimal parameters ˆ <em>θ</em>, we can employ it to model any sequence and generate the corresponding representation, like this</p></div>
<div class="para"><p><strong>H</strong>=Encode ˆ <em>θ</em>(<strong>x</strong>)(1.2)</p></div>
<div class="para"><p>where<strong> x</strong> is the input sequence<em> {</em><em>x</em> 0<em>, x</em> 1<em>, ..., x</em> <em>m</em><em>}</em>, and<strong> H</strong> is the output representation which is a sequence of real-valued vectors<em> {</em><strong>h</strong> 0<em>,</em><strong> h</strong> 1<em>, ...,</em><strong> h</strong> <em>m</em><em>}</em>. Because the encoder does not work as a standalone NLP system, it is often integrated as a component into a bigger system. Consider, for example, a text classification problem in which we identify the polarity (i.e., positive, negative,</p></div>
<div class="para"><p>2 More precisely, in auto-regressive decoding of machine translation, each target-language token is generated based on both its preceding tokens and source-language sequence.</p></div>
<div class="para"><p><em>1.1 Pre-training NLP Models</em> 5</p></div>
<div class="para"><p>and neutral) of a given text. We can build a text classification system by stacking a classifier on top of the encoder. Let Classify <em>ω</em>(<em>·</em>) be a neural network with parameters<em> ω</em>. Then, the text classification model can be expressed in the form</p></div>
<div class="para"><p>Pr <em>ω,</em>ˆ <em>θ</em>(<em>·|</em><strong>x</strong>)=Classify <em>ω</em>(<strong>H</strong>)</p></div>
<div class="para"><p>=Classify <em>ω</em>(Encode ˆ <em>θ</em>(<strong>x</strong>))(1.3)</p></div>
<div class="para"><p>Here Pr <em>ω,</em>ˆ <em>θ</em>(<em>·|</em><strong>x</strong>) is a probability distribution over the label set<em> {</em>positive<em>,</em> negative<em>,</em> neutral<em>}</em>, and the label with the highest probability in this distribution is selected as output. To keep the notation uncluttered, we will use<em> F</em> <em>ω,</em>ˆ <em>θ</em>(<em>·</em>) to denote Classify <em>ω</em>(Encode ˆ <em>θ</em>(<em>·</em>)).</p></div>
<div class="para"><p>Because the model parameters<em> ω</em> and ˆ <em>θ</em> are not optimized for the classification task, we cannot directly use this model. Instead, we must use a modified version of the model that is adapted to the task. A typical way is to fine-tune the model by giving explicit labeling in downstream tasks.We can train<em> F</em> <em>ω,</em>ˆ <em>θ</em>(<em>·</em>) on a labeled dataset, treating it as a common supervised learning task. The outcome of the fine-tuning is the parameters ˜<em>ω</em> and ˜<em>θ</em> that are further optimized. Alternatively,we can freeze the encoder parameters ˆ <em>θ</em> to maintain their pre-trained state, and focus solely on optimizing<em> ω</em>. This allows the classifier to be efficiently adapted to work in tandem with the pre-trained encoder.</p></div>
<div class="para"><p>Once we have obtained a fine-tuned model, we can use it to classify a new text. For example,suppose we have a comment posted on a travel website:</p></div>
<div class="para"><p>I love the food here. It’s amazing!</p></div>
<div class="para"><p>We first tokenize this text into tokens 3, and then feed the token sequence<strong> x</strong> new into the fine-tuned model<em> F</em>˜<em>ω,</em>˜<em>θ</em>(<em>·</em>). The model generates a distribution over classes by</p></div>
<div class="para"><p><em>F</em>˜<em>ω,</em>˜<em>θ</em>(<strong>x</strong> new)=h Pr(positive<em>|</em><strong>x</strong> new)Pr(negative<em>|</em><strong>x</strong> new)Pr(neutral<em>|</em><strong>x</strong> new)i</p></div>
<div class="para"><p>(1.4)</p></div>
<div class="para"><p>And we select the label of the entry with the maximum value as output. In this example it is positive.</p></div>
<div class="para"><p>In general, the amount of labeled data used in fine-tuning is small compared to that of the pre-training data, and so fine-tuning is less computationally expensive. This makes the adaptation of pre-trained models very efficient in practice: given a pre-trained model and a downstream task,we just need to collect some labeled data, and slightly adjust the model parameters on this data. A more detailed discussion of fine-tuning can be found in Section 1.4.</p></div>
<div class="para"><p><strong>1.1.2.2</strong> <strong>Prompting of Pre-trained Models</strong></p></div>
<div class="para"><p>Unlike sequence encoding models, sequence generation models are often employed independently to address language generation problems, such as question answering and machine translation,without the need for additional modules. It is therefore straightforward to fine-tune these models</p></div>
<div class="para"><p>3 The text can be tokenized in many different ways. One of the simplest is to segment the text into English words and punctuations<em> {</em>I<em>,</em> love<em>,</em> the<em>,</em> food<em>,</em> here<em>,</em> .<em>,</em> It<em>,</em> ’s<em>,</em> amazing<em>,</em> !<em>}</em></p></div>
<div class="para"><p>6 <em>Pre-training</em></p></div>
<div class="para"><p>as complete systems on downstream tasks. For example, we can fine-tune a pre-trained encoderdecoder multilingual model on some bilingual data to improve its performance on a specific translation task.</p></div>
<div class="para"><p>Among various sequence generation models, a notable example is the large language models trained on very large amounts of data. These language models are trained to simply predict the next token given its preceding tokens. Although token prediction is such a simple task that it has long been restricted to “language modeling” only, it has been found to enable the learning of the general knowledge of languages by repeating the task a large number of times. The result is that the pre-trained large language models exhibit remarkably good abilities in token prediction, making it possible to transform numerous NLP problems into simple text generation problems through prompting the large language models. For example, we can frame the above text classification problem as a text generation task</p></div>
<div class="para"><p>I love the food here. It’s amazing! I’m</p></div>
<div class="para"><p>Here indicates the word or phrase we want to predict (call it the<strong> completion</strong>). If the predicted word is<em> happy</em>, or<em> glad</em>, or<em> satisfied</em> or a related positive word, we can classify the text as positive.This example shows a simple prompting method in which we concatenate the input text with<em> I’m</em> to form a prompt. Then, the completion helps decide which label is assigned to the original text.</p></div>
<div class="para"><p>Given the strong performance of language understanding and generation of large language models, a prompt can instruct the models to perform more complex tasks. Here is a prompt where we prompt the LLM to perform polarity classification with an instruction.</p></div>
<div class="para"><p>Assume that the polarity of a text is a label chosen from {positive, negative,neutral}. Identify the polarity of the input.</p></div>
<div class="para"><p>Input: I love the food here. It’s amazing!</p></div>
<div class="para"><p>Polarity:</p></div>
<div class="para"><p>The first two sentences are a description of the task. Input and Polarity are indicators of the input and output, respectively. We expect the model to complete the text and at the same time give the correct polarity label. By using instruction-based prompts, we can adapt large language models to solve NLP problems without the need for additional training.</p></div>
<div class="para"><p>This example also demonstrates the zero-shot learning capability of large language models,which can perform tasks that were not observed during the training phase. Another method for enabling new capabilities in a neural network is few-shot learning. This is typically achieved through<strong> in-context learning</strong> (<strong>ICT</strong>). More specifically, we add some samples that demonstrate how an input corresponds to an output. These samples, known as<strong> demonstrations</strong>, are used to teach large language models how to perform the task. Below is an example involving demonstrations</p></div>
<div class="para"><p><em>1.2 Self-supervised Pre-training Tasks</em> 7</p></div>
<div class="para"><p>Assume that the polarity of a text is a label chosen from {positive, negative,neutral}. Identify the polarity of the input.</p></div>
<div class="para"><p>Input: The traffic is terrible during rush hours, making it difficult to reach the airport on time.</p></div>
<div class="para"><p>Polarity: Negative</p></div>
<div class="para"><p>Input: The weather here is wonderful.</p></div>
<div class="para"><p>Polarity: Positive</p></div>
<div class="para"><p>Input: I love the food here. It’s amazing!</p></div>
<div class="para"><p>Polarity:</p></div>
<div class="para"><p>Prompting and in-context learning play important roles in the recent rise of large language models. We will discuss these issues more deeply in Chapter 3. However, it is worth noting that while prompting is a powerful way to adapt large language models, some tuning efforts are still needed to ensure the models can follow instructions accurately. Additionally, the fine-tuning process is crucial for aligning the values of these models with human values. More detailed discussions of fine-tuning can be found in Chapter 4.</p></div>
<h3>1.2 Self-supervised Pre-training Tasks</h3>
<div class="para"><p>In this section, we consider self-supervised pre-training approaches for different neural architectures, including decoder-only, encoder-only, and encoder-decoder architectures. We restrict our discussion to Transformers since they form the basis of most pre-trained models in NLP. However, pre-training is a broad concept, and so we just give a brief introduction to basic approaches in order to make this section concise.</p></div>
<div class="para"><p><strong>1.2.1</strong> <strong>Decoder-only Pre-training</strong></p></div>
<div class="para"><p>The decoder-only architecture has been widely used in developing language models [Radford et al., 2018]. For example, we can use a Transformer decoder as a language model by simply removing cross-attention sub-layers from it. Such a model predicts the distribution of tokens at a position given its preceding tokens, and the output is the token with the maximum probability.The standard way to train this model, as in the language modeling problem, is to minimize a loss function over a collection of token sequences. Let Decoder <em>θ</em>(<em>·</em>) denote a decoder with parameters <em>θ</em>. At each position<em> i</em>, the decoder generates a distribution of the next tokens based on its preceding tokens<em> {</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>}</em>, denoted by Pr <em>θ</em>(<em>·|</em><em>x</em> 0<em>, ..., x</em> <em>i</em>) (or<strong> p</strong> <em>θ</em> <em>i</em>+1 for short). Suppose we have the goldstandard distribution at the same position, denoted by<strong> p</strong> gold <em>i</em>+1 . For language modeling, we can think of<strong> p</strong> gold <em>i</em>+1 as a one-hot representation of the correct predicted word. We then define a loss function <em>L</em>(<strong>p</strong> <em>θ</em> <em>i</em>+1<em>,</em><strong> p</strong> gold <em>i</em>+1 ) to measure the difference between the model prediction and the true prediction. In NLP, the log-scale cross-entropy loss is typically used.</p></div>
<div class="para"><p>Given a sequence of<em> m</em> tokens<em> {</em><em>x</em> 0<em>, ..., x</em> <em>m</em><em>}</em>, the loss on this sequence is the sum of the loss</p></div>
<div class="para"><p>8 <em>Pre-training</em></p></div>
<div class="para"><p>over the positions<em> {</em>0<em>, ..., m</em><em> −</em>1<em>}</em>, given by</p></div>
<div class="para"><p><em>m</em><em>−</em>1 X</p></div>
<div class="para"><p><em>i</em>=0 <em>L</em>(<strong>p</strong> <em>θ</em> <em>i</em>+1<em>,</em><strong> p</strong> gold <em>i</em>+1 )</p></div>
<div class="para"><p>Loss <em>θ</em>(<em>x</em> 0<em>, ..., x</em> <em>m</em>)=</p></div>
<div class="para"><p><em>m</em><em>−</em>1 X</p></div>
<div class="para"><p><em>i</em>=0 LogCrossEntropy(<strong>p</strong> <em>θ</em> <em>i</em>+1<em>,</em><strong> p</strong> gold <em>i</em>+1 )(1.5)</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>where LogCrossEntropy(<em>·</em>) is the log-scale cross-entropy, and<strong> p</strong> gold <em>i</em>+1 is the one-hot representation of<em> x</em> <em>i</em>+1.</p></div>
<div class="para"><p>This loss function can be extended to a set of sequences<em> D</em>. In this case, the objective of pre-training is to find the best parameters that minimize the loss on<em> D</em></p></div>
<div class="para"><p>ˆ <em>θ</em>=arg min <em>θ</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><strong>x</strong><em>∈D</em> Loss <em>θ</em>(<strong>x</strong>)(1.6)</p></div>
<div class="para"><p>Note that this objective is mathematically equivalent to maximum likelihood estimation, and can be re-expressed as</p></div>
<div class="para"><p>ˆ <em>θ</em>=arg max <em>θ</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><strong>x</strong><em>∈D</em> log Pr <em>θ</em>(<strong>x</strong>)</p></div>
<div class="para"><p><em>i</em><em>−</em>1 X</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><em>i</em>=0 log Pr <em>θ</em>(<em>x</em> <em>i</em>+1<em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em>)(1.7)</p></div>
<div class="para"><p>=arg max <em>θ</em></p></div>
<div class="para"><p><strong>x</strong><em>∈D</em></p></div>
<div class="para"><p>With these optimized parameters ˆ <em>θ</em>, we can use the pre-trained language model Decoder ˆ <em>θ</em>(<em>·</em>)to compute the probability Pr ˆ <em>θ</em>(<em>x</em> <em>i</em>+1<em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em>) at each position of a given sequence.</p></div>
<div class="para"><p><strong>1.2.2</strong> <strong>Encoder-only Pre-training</strong></p></div>
<div class="para"><p>As defined in Section 1.1.2.1, an encoder Encoder <em>θ</em>(<em>·</em>) is a function that reads a sequence of tokens<strong> x</strong> =<em> x</em> 0<em>...x</em> <em>m</em> and produces a sequence of vectors<strong> H</strong> =<strong> h</strong> 0<em>...</em><strong>h</strong> <em>m</em> 4. Training this model is not straightforward, as we do not have gold-standard data for measuring how good the output of the real-valued function is. A typical approach to encoder pre-training is to combine the encoder with some output layers to receive supervision signals that are easier to obtain. Figure 1.2 shows a common architecture for pre-training Transformer encoders, where we add a Softmax layer on top of the Transformer encoder. Clearly, this architecture is the same as that of the decoder-based language model, and the output is a sequence of probability distributions</p></div>
<div class="para"><p><strong>p</strong> <strong>W</strong><em>,θ</em> 1...<strong>p</strong> <strong>W</strong><em>,θ</em> <em>m</em></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p>=Softmax <strong>W</strong>(Encoder <em>θ</em>(<strong>x</strong>))(1.9)</p></div>
<div class="para"><p>4 If we view<strong> h</strong> <em>i</em> as a row vector,<strong> H</strong> can be written as</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><strong>h</strong> 0...<strong>h</strong> <em>m</em></p></div>
<div class="para"><p><strong>H</strong>=</p></div>
<div class="para"><p>(1.8)</p></div>
<div class="para"><p></p></div>
<div class="para"><p><em>1.2 Self-supervised Pre-training Tasks</em> 9</p></div>
<div class="para"><p>model reconstructs the masked token E.g., evaluate how well the Self-supervision</p></div>
<div class="para"><p>Output for Downstream Tasks</p></div>
<div class="para"><p>Softmax</p></div>
<div class="para"><p>Prediction Network</p></div>
<div class="para"><p>Encoder</p></div>
<div class="para"><p>Pre-trained Encoder</p></div>
<div class="para"><p><strong>e</strong> 0 <strong>e</strong> 1 <strong>e</strong> 2 <strong>e</strong> 3 <strong>e</strong> 4</p></div>
<div class="para"><p><strong>e</strong> 0 <strong>e</strong> 1 <strong>e</strong> 2 <strong>e</strong> 3 <strong>e</strong> 4</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>x</em> 4(masked)</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>x</em> 4</p></div>
<div class="para"><p>(a) Pre-training</p></div>
<div class="para"><p>(b) Applying the Pre-trained Encoder</p></div>
<div class="para"><p><strong>Fig. 1.2:</strong> Pre-training a Transformer encoder (left) and then applying the pre-trained encoder (right). In the pre-training phase, the encoder, together with a Softmax layer, is trained via self-supervision. In the application phase, the Softmax layer is removed, and the pre-trained encoder is combined with a prediction network to address specific problems. In general, for better adaptation to these tasks, the system is fine-tuned using labeled data.</p></div>
<div class="para"><p>Here<strong> p</strong> <strong>W</strong><em>,θ</em> <em>i</em> is the output distribution Pr(<em>·|</em><strong>x</strong>) at position<em> i</em>. We use Softmax <strong>W</strong>(<em>·</em>) to denote that the Softmax layer is parameterized by<strong> W</strong>, that is, Softmax <strong>W</strong>(<strong>H</strong>) = Softmax(<strong>H</strong><em> ·</em><strong> W</strong>). For notation simplicity, we will sometimes drop the superscripts<strong> W</strong> and<em> θ</em> affixed to each probability distribution.</p></div>
<div class="para"><p>The difference between this model and standard language models is that the output<strong> p</strong> <em>i</em> has different meanings in encoder pre-training and language modeling. In language modeling,<strong> p</strong> <em>i</em> is the probability distribution of predicting the next word. This follows an auto-regressive decoding process: a language model only observes the words up to position<em> i</em> and predicts the next. By contrast, in encoder pre-training, the entire sequence can be observed at once, and so it makes no sense to predict any of the tokens in this sequence.</p></div>
<div class="para"><p><strong>1.2.2.1</strong> <strong>Masked Language Modeling</strong></p></div>
<div class="para"><p>One of the most popular methods of encoder pre-training is<strong> masked language modeling</strong>, which forms the basis of the well-known BERT model [Devlin et al., 2019]. The idea of masked language modeling is to create prediction challenges by masking out some of the tokens in the input sequence and training a model to predict the masked tokens. In this sense, the conventional language modeling problem, which is sometimes called<strong> causal language modeling</strong>, is a special case of masked language modeling: at each position, we mask the tokens in the right-context, and predict the token at this position using its left-context. However, in causal language modeling we only make use of the left-context in word prediction, while the prediction may depend on tokens in the right-context. By contrast, in masked language modeling, all the unmasked tokens are used for word prediction, leading to a bidirectional model that makes predictions based on both left and right-contexts.</p></div>
<div class="para"><p>10 <em>Pre-training</em></p></div>
<div class="para"><p>More formally, for an input sequence<strong> x</strong> =<em> x</em> 0<em>...x</em> <em>m</em>, suppose that we mask the tokens at positions<em> A</em>(<strong>x</strong>) =<em> {</em><em>i</em> 1<em>, ..., i</em> <em>u</em><em>}</em>. Hence we obtain a masked token sequence ¯<strong>x</strong> where the token at each position in<em> A</em>(<strong>x</strong>) is replaced with a special symbol [MASK]. For example, for the following sequence</p></div>
<div class="para"><p>The early bird catches the worm</p></div>
<div class="para"><p>we may have a masked token sequence like this</p></div>
<div class="para"><p>The [MASK] bird catches the [MASK]</p></div>
<div class="para"><p>where we mask the tokens<em> early</em> and<em> worm</em> (i.e.,<em> i</em> 1 = 2 and<em> i</em> 2 = 6).</p></div>
<div class="para"><p>Now we have two sequences<strong> x</strong> and ¯<strong>x</strong>. The model is then optimized so that we can correctly predict<strong> x</strong> based on ¯<strong>x</strong>. This can be thought of as an autoencoding-like process, and the training objective is to maximize the reconstruction probability Pr(<strong>x</strong><em>|</em>¯<strong>x</strong>). Note that there is a simple position-wise alignment between<strong> x</strong> and ¯<strong>x</strong>. Because an unmasked token in ¯<strong>x</strong> is the same as the token in<strong> x</strong> at the same position, there is no need to consider the prediction for this unmasked token.This leads to a simplified training objective which only maximizes the probabilities for masked tokens. We can express this objective in a maximum likelihood estimation fashion</p></div>
<div class="para"><p><em>i</em><em>∈A</em>(<strong>x</strong>)log Pr <strong>W</strong><em>,θ</em> <em>i</em>(<em>x</em> <em>i</em><em>|</em>¯<strong>x</strong>)(1.10)</p></div>
<div class="para"><p>( c <strong>W</strong><em>,</em> ˆ <em>θ</em>)=arg max <strong>W</strong><em>,θ</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><strong>x</strong><em>∈D</em></p></div>
<div class="para"><p>or alternatively express it using the cross-entropy loss</p></div>
<div class="para"><p><em>i</em><em>∈A</em>(<strong>x</strong>)LogCrossEntropy(<strong>p</strong> <strong>W</strong><em>,θ</em> <em>i</em><em>,</em><strong> p</strong> gold <em>i</em>)(1.11)</p></div>
<div class="para"><p>( c <strong>W</strong><em>,</em> ˆ <em>θ</em>)=arg min <strong>W</strong><em>,θ</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><strong>x</strong><em>∈D</em></p></div>
<div class="para"><p>where Pr <strong>W</strong><em>,θ</em> <em>k</em>(<em>x</em> <em>k</em><em>|</em>¯<strong>x</strong>) is the probability of the true token<em> x</em> <em>k</em> at position<em> k</em> given the corrupted input¯<strong>x</strong>, and<strong> p</strong> <strong>W</strong><em>,θ</em> <em>k</em> is the probability distribution at position<em> k</em> given the corrupted input ¯<strong>x</strong>. To illustrate,consider the above example where two tokens of the sequence “<em>the early bird catches the worm</em>”are masked. For this example, the objective is to maximize the sum of log-scale probabilities</p></div>
<div class="para"><p>Loss=log Pr(<em>x</em> 2 =<em> early</em><em>|</em>¯<strong>x</strong> = [CLS]<em> The</em> [MASK]|{z}¯<em>x</em> 2</p></div>
<div class="para"><p><em>bird catches the</em> [MASK]|{z}¯<em>x</em> 6</p></div>
<div class="para"><p>) +</p></div>
<div class="para"><p>log Pr(<em>x</em> 6 =<em> worm</em><em>|</em>¯<strong>x</strong> = [CLS]<em> The</em> [MASK]|{z}¯<em>x</em> 2</p></div>
<div class="para"><p><em>bird catches the</em> [MASK]|{z}¯<em>x</em> 6</p></div>
<div class="para"><p>)(1.12)</p></div>
<div class="para"><p>Once we obtain the optimized parameters c <strong>W</strong> and ˆ <em>θ</em>, we can drop c <strong>W</strong>. Then, we can further fine-tune the pre-trained encoder Encoder ˆ <em>θ</em>(<em>·</em>) or directly apply it to downstream tasks.</p></div>
<div class="para"><p><strong>1.2.2.2</strong> <strong>Permuted Language Modeling</strong></p></div>
<div class="para"><p>While masked language modeling is simple and widely applied, it introduces new issues. One drawback is the use of a special token, [MASK], which is employed only during training but not</p></div>
<div class="para"><p><em>1.2 Self-supervised Pre-training Tasks</em> 11</p></div>
<div class="para"><p>at test time. This leads to a discrepancy between training and inference. Moreover, the autoencoding process overlooks the dependencies between masked tokens. For example, in the above example, the prediction of<em> x</em> 2 (i.e., the first masked token) is made independently of<em> x</em> 6 (i.e., the second masked token), though<em> x</em> 6 should be considered in the context of<em> x</em> 2.</p></div>
<div class="para"><p>These issues can be addressed using the<strong> permuted language modeling</strong> approach to pretraining [Yang et al., 2019]. Similar to causal language modeling, permuted language modeling involves making sequential predictions of tokens. However, unlike causal modeling where predictions follow the natural sequence of the text (like left-to-right or right-to-left), permuted language modeling allows for predictions in any order. The approach is straightforward: we determine an order for token predictions and then train the model in a standard language modeling manner, as described in Section 1.2.1. Note that in this approach, the actual order of tokens in the text remains unchanged, and only the order in which we predict these tokens differs from standard language modeling. For example, consider a sequence of 5 tokens<em> x</em> 0 <em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>x</em> 4. Let<strong> e</strong> <em>i</em> represent the embedding of<em> x</em> <em>i</em> (i.e., combination of the token embedding and positional embedding). In standard language modeling, we would generate this sequence in the order of<em> x</em> 0<em> →</em><em>x</em> 1<em> →</em><em>x</em> 2<em> →</em><em>x</em> 3<em> →</em><em>x</em> 4.The probability of the sequence can be modeled via a generation process.</p></div>
<div class="para"><p>Pr(<strong>x</strong>)=Pr(<em>x</em> 0)<em> ·</em> Pr(<em>x</em> 1<em>|</em><em>x</em> 0)<em> ·</em> Pr(<em>x</em> 2<em>|</em><em>x</em> 0<em>, x</em> 1)<em> ·</em> Pr(<em>x</em> 3<em>|</em><em>x</em> 0<em>, x</em> 1<em>, x</em> 2)<em> ·</em></p></div>
<div class="para"><p>Pr(<em>x</em> 4<em>|</em><em>x</em> 0<em>, x</em> 1<em>, x</em> 2<em>, x</em> 3)</p></div>
<div class="para"><p>=Pr(<em>x</em> 0)<em> ·</em> Pr(<em>x</em> 1<em>|</em><strong>e</strong> 0)<em> ·</em> Pr(<em>x</em> 2<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 1)<em> ·</em> Pr(<em>x</em> 3<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 1<em>,</em><strong> e</strong> 2)<em> ·</em></p></div>
<div class="para"><p>Pr(<em>x</em> 4<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 1<em>,</em><strong> e</strong> 2<em>,</em><strong> e</strong> 3)(1.13)</p></div>
<div class="para"><p>Now, let us consider a different order for token prediction:<em> x</em> 0<em> →</em><em>x</em> 4<em> →</em><em>x</em> 2<em> →</em><em>x</em> 1<em> →</em><em>x</em> 3. The sequence generation process can then be expressed as follows:</p></div>
<div class="para"><p>Pr(<strong>x</strong>)=Pr(<em>x</em> 0)<em> ·</em> Pr(<em>x</em> 4<em>|</em><strong>e</strong> 0)<em> ·</em> Pr(<em>x</em> 2<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 4)<em> ·</em> Pr(<em>x</em> 1<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 4<em>,</em><strong> e</strong> 2)<em> ·</em></p></div>
<div class="para"><p>Pr(<em>x</em> 3<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 4<em>,</em><strong> e</strong> 2<em>,</em><strong> e</strong> 1)(1.14)</p></div>
<div class="para"><p>This new prediction order allows for the generation of some tokens to be conditioned on a broader context, rather than being limited to just the preceding tokens as in standard language models. For example, in generating<em> x</em> 3, the model considers both its left-context (i.e.,<strong> e</strong> 0<em>,</em><strong> e</strong> 1<em>,</em><strong> e</strong> 2)and right-context (i.e.,<strong> e</strong> 4). The embeddings<strong> e</strong> 0<em>,</em><strong> e</strong> 1<em>,</em><strong> e</strong> 2<em>,</em><strong> e</strong> 4 incorporate the positional information of<em> x</em> 0<em>, x</em> 1<em>, x</em> 2<em>, x</em> 4, preserving the original order of the tokens. As a result, this approach is somewhat akin to masked language modeling: we mask out<em> x</em> 3 and use its surrounding tokens<em> x</em> 0<em>, x</em> 1<em>, x</em> 2<em>, x</em> 4 to predict this token.</p></div>
<div class="para"><p>The implementation of permuted language models is relatively easy for Transformers. Because the self-attention model is insensitive to the order of inputs, we do not need to explicitly reorder the sequence to have a factorization like Eq. (1.14). Instead, permutation can be done by setting appropriate masks for self-attention. For example, consider the case of computing Pr(<em>x</em> 1<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 4<em>,</em><strong> e</strong> 2). We can place<em> x</em> 0<em>, x</em> 1<em>, x</em> 2<em>, x</em> 3<em>, x</em> 4 in order and block the attention from<em> x</em> 3 to<em> x</em> 1 in self-attention, as illustrated below</p></div>
<div class="para"><p>12 <em>Pre-training</em></p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>x</em> 4</p></div>
<div class="para"><p>Masks for Self-attention:</p></div>
<div class="para"><p>Blue box = valid attention Gray box = blocked attention</p></div>
<div class="para"><p>For a more illustrative example, we compare the self-attention masking results of causal language modeling, masked language modeling and permuted language modeling in Figure 1.3.</p></div>
<div class="para"><p><strong>1.2.2.3</strong> <strong>Pre-training Encoders as Classifiers</strong></p></div>
<div class="para"><p>Another commonly-used idea to train an encoder is to consider classification tasks.In selfsupervised learning, this is typically done by creating new classification challenges from the unlabeled text. There are many different ways to design the classification tasks. Here we present two popular tasks.</p></div>
<div class="para"><p>A simple method, called<strong> next sentence prediction</strong> (<strong>NSP</strong>), is presented in BERT’s original paper [Devlin et al., 2019]. The assumption of NSP is that a good text encoder should capture the relationship between two sentences. To model such a relationship, in NSP we can use the output of encoding two consecutive sentences Sent <em>A</em> and Sent <em>B</em> to determine whether Sent <em>B</em> is the next sentence following Sent <em>A</em>. For example, suppose Sent <em>A</em> =<em> ’It is raining .’</em> and Sent <em>B</em> =<em>’I need an umbrella .’</em>. The input sequence of the encoder could be</p></div>
<div class="para"><p>[CLS] It is raining . [SEP] I need an umbrella . [SEP]</p></div>
<div class="para"><p>where [CLS] is the start symbol (i.e.,<em> x</em> 0) which is commonly used in encoder pre-training, and[SEP] is a separator that separates the two sentences. The processing of this sequence follows a standard procedure of Transformer encoding: we first represent each token<em> x</em> <em>i</em> as its corresponding embedding<strong> e</strong> <em>i</em>, and then feed the embedding sequence<em> {</em><strong>e</strong> 0<em>, ...,</em><strong> e</strong> <em>m</em><em>}</em> into the encoder to obtain the output sequence<em> {</em><strong>h</strong> 0<em>, ...,</em><strong> h</strong> <em>m</em><em>}</em>. Since<strong> h</strong> 0 is generally considered as the representation of the entire sequence, we add a Softmax layer on top of it to construct a binary classification system. This process is illustrated as follows</p></div>
<div class="para"><p>token: [CLS] It is raining.[SEP]I need an umbrella.[SEP]</p></div>
<div class="para"><p>embedding:<strong>e</strong> 0 <strong>e</strong> 1 <strong>e</strong> 2 <strong>e</strong> 3 <strong>e</strong> 4 <strong>e</strong> 5 <strong>e</strong> 6 <strong>e</strong> 7 <strong>e</strong> 8 <strong>e</strong> 9 <strong>e</strong> 10 <strong>e</strong> 11<em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em></p></div>
<div class="para"><p>Encoder</p></div>
<div class="para"><p><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em></p></div>
<div class="para"><p>encoding:<strong>h</strong> 0 <strong>h</strong> 1 <strong>h</strong> 2 <strong>h</strong> 3 <strong>h</strong> 4 <strong>h</strong> 5 <strong>h</strong> 6 <strong>h</strong> 7 <strong>h</strong> 8 <strong>h</strong> 9 <strong>h</strong> 10 <strong>h</strong> 11<em>↓</em></p></div>
<div class="para"><p>Softmax</p></div>
<div class="para"><p><em>↓</em></p></div>
<div class="para"><p>Is Next or Not?</p></div>
<div class="para"><p><em>1.2 Self-supervised Pre-training Tasks</em> 13</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 0</p></div>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p><em>x</em> 3</p></div>
<div class="para"><p><em>x</em> 4</p></div>
<div class="para"><p>Pr(<em>x</em> 0) = 1</p></div>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p>Pr(<em>x</em> 1<em>|</em><strong>e</strong> 0)</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p>Pr(<em>x</em> 2<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 1)</p></div>
<div class="para"><p><em>x</em> 3</p></div>
<div class="para"><p>Pr(<em>x</em> 3<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 1<em>,</em><strong> e</strong> 2)</p></div>
<div class="para"><p><em>x</em> 4</p></div>
<div class="para"><p>Pr(<em>x</em> 4<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 1<em>,</em><strong> e</strong> 2<em>,</em><strong> e</strong> 3)</p></div>
<div class="para"><p>(a) Causal Language Modeling (order:<em> x</em> 0<em> →</em><em>x</em> 1<em> →</em><em>x</em> 2<em> →</em><em>x</em> 3<em> →</em><em>x</em> 4)</p></div>
<div class="para"><p>masked masked</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 0</p></div>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p><em>x</em> 3</p></div>
<div class="para"><p><em>x</em> 4</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p>Pr(<em>x</em> 1<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> mask<em>,</em><strong> e</strong> 2<em>,</em><strong> e</strong> mask<em>,</em><strong> e</strong> 4)</p></div>
<div class="para"><p>masked</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p><em>x</em> 3</p></div>
<div class="para"><p>Pr(<em>x</em> 3<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> mask<em>,</em><strong> e</strong> 2<em>,</em><strong> e</strong> mask<em>,</em><strong> e</strong> 4)</p></div>
<div class="para"><p>masked</p></div>
<div class="para"><p><em>x</em> 4</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>(b) Masked Language Modeling (order:<em> x</em> 0<em>,</em> [MASK]<em>, x</em> 2<em>,</em> [MASK]<em>, x</em> 4<em> →</em><em>x</em> 1<em>, x</em> 3)</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 0</p></div>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p><em>x</em> 3</p></div>
<div class="para"><p><em>x</em> 4</p></div>
<div class="para"><p>Pr(<em>x</em> 0) = 1</p></div>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p>Pr(<em>x</em> 1<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 4<em>,</em><strong> e</strong> 2)</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p>Pr(<em>x</em> 2<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 4)</p></div>
<div class="para"><p><em>x</em> 3</p></div>
<div class="para"><p>Pr(<em>x</em> 3<em>|</em><strong>e</strong> 0<em>,</em><strong> e</strong> 4<em>,</em><strong> e</strong> 2<em>,</em><strong> e</strong> 1)</p></div>
<div class="para"><p><em>x</em> 4</p></div>
<div class="para"><p>Pr(<em>x</em> 4<em>|</em><strong>e</strong> 0)</p></div>
<div class="para"><p>(c) Permuted Language Modeling (order:<em> x</em> 0<em> →</em><em>x</em> 4<em> →</em><em>x</em> 2<em> →</em><em>x</em> 1<em> →</em><em>x</em> 3)</p></div>
<div class="para"><p><strong>Fig. 1.3:</strong> Comparison of self-attention masking results of causal language modeling, masked language modeling and permuted language modeling. The gray cell denotes the token at position<em> j</em> does not attend to the token at position<em> i</em>.The blue cell (<em>i, j</em>) denotes that the token at position<em> j</em> attends to the token at position<em> i</em>.<strong> e</strong> mask represents the embedding of the symbol [MASK], which is a combination of the token embedding and the positional embedding.</p></div>
<div class="para"><p>In order to generate training samples, we need two sentences each time, one for Sent <em>A</em> and the other for Sent <em>B</em>. A simple way to do this is to utilize the natural sequence of two consecutive sentences in the text. For example, we obtain a positive sample by using actual consecutive sentences, and a negative sample by using randomly sampled sentences. Consequently, training this model is the same as training a classifier. Typically, NSP is used as an additional training loss</p></div>
<div class="para"><p>14 <em>Pre-training</em></p></div>
<div class="para"><p>function for pre-training based on masked language modeling.</p></div>
<div class="para"><p>A second example of training Transformer encoders as classifiers is to apply classificationbased supervision signals to each output of an encoder. For example, Clark et al. [2019] in their ELECTRA model, propose training a Transformer encoder to identify whether each input token is identical to the original input or has been altered in some manner. The first step of this method is to generate a new sequence from a given sequence of tokens, where some of the tokens are altered. To do this, a small masked language model (call it the generator) is applied: we randomly mask some of the tokens, and train this model to predict the masked tokens. For each training sample, this masked language model outputs a token at each masked position, which might be different from the original token. At the same time, we train another Transformer encoder (call it the discriminator) to determine whether each predicted token is the same as the original token or altered. More specifically, we use the generator to generate a sequence where some of the tokens are replaced. Below is an illustration.</p></div>
<div class="para"><p>original:[CLS]The boy spent hours working on toys.</p></div>
<div class="para"><p><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em></p></div>
<div class="para"><p>masked:[CLS]The boy spent[MASK]working on[MASK].</p></div>
<div class="para"><p><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em></p></div>
<div class="para"><p>Generator (small masked language model)</p></div>
<div class="para"><p><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em></p></div>
<div class="para"><p>replaced:[CLS]The boy spent decades working on toys.</p></div>
<div class="para"><p>Then, we use the discriminator to label each of these tokens as original or replaced, as follows</p></div>
<div class="para"><p>replaced: [CLS]The boy spent decades working on toys.</p></div>
<div class="para"><p><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em></p></div>
<div class="para"><p>Discriminator (the model we want)</p></div>
<div class="para"><p><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em><em>↓</em></p></div>
<div class="para"><p>label: original original original original replaced original original original original</p></div>
<div class="para"><p>For training, the generator is optimized as a masked language model with maximum likelihood estimation, and the discriminator is optimized as a classifier using a classification-based loss. In ELECTRA, the maximum likelihood-based loss and the classification-based loss are combined for jointly training both the generator and discriminator. An alternative approach is to use generative adversarial networks (GANs), that is, the generator is trained to fool the discriminator, and the discriminator is trained to distinguish the output of the generator from the true distribution. However,GAN-style training complicates the training task and is more difficult to scale up. Nevertheless,once training is complete, the generator is discarded, and the encoding part of the discriminator is applied as the pre-trained model for downstream tasks.</p></div>
<div class="para"><p><em>1.2 Self-supervised Pre-training Tasks</em> 15</p></div>
<div class="para"><p><strong>1.2.3</strong> <strong>Encoder-Decoder Pre-training</strong></p></div>
<div class="para"><p>In NLP, encoder-decoder architectures are often used to model sequence-to-sequence problems,such as machine translation and question answering. In addition to these typical sequence-tosequence problems in NLP, encoder-decoder models can be extended to deal with many other problems. A simple idea is to consider text as both the input and output of a problem, and so we can directly apply encoder-decoder models. For example, given a text, we can ask a model to output a text describing the sentiment of the input text, such as<em> positive</em>,<em> negative</em>, and<em> neutral</em>.</p></div>
<div class="para"><p>Such an idea allows us to develop a single text-to-text system to address any NLP problem.We can formulate different problems into the same text-to-text format. We first train an encoderdecoder model to gain general-purpose knowledge of language via self-supervision. This model is then fine-tuned for specific downstream tasks using targeted text-to-text data.</p></div>
<div class="para"><p><strong>1.2.3.1</strong> <strong>Masked Encoder-Decoder Pre-training</strong></p></div>
<div class="para"><p>In Raffel et al. [2020]’s<strong> T5</strong> model, many different tasks are framed as the same text-to-text task.Each sample in T5 follows the format</p></div>
<div class="para"><p>Source Text<em>→</em>Target Text</p></div>
<div class="para"><p>Here<em> →</em>separates the source text, which consists of a task description or instruction and the input given to the system, from the target text, which is the response to the input task. As an example,consider a task of translating from Chinese to English. A training sample can be expressed as</p></div>
<div class="para"><p>[CLS] Translate from Chinese to English: 你 好！<em>→</em><em>⟨</em><em>s</em><em>⟩</em>Hello!</p></div>
<div class="para"><p>where [CLS] and<em> ⟨</em><em>s</em><em>⟩</em>are the start symbols on the source and target sides, respectively 5.</p></div>
<div class="para"><p>Likewise, we can express other tasks in the same way. For example</p></div>
<div class="para"><p>[CLS] Answer: when was Albert Einstein born?</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>He was born on March 14, 1879.</p></div>
<div class="para"><p>[CLS] Simplify: the professor, who has published numerous papers in his field,</p></div>
<div class="para"><p>will be giving a lecture on the topic next week.</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>The experienced professor will give a lecture next week.</p></div>
<div class="para"><p>[CLS] Score the translation from English to Chinese. English: when in Rome, do as</p></div>
<div class="para"><p>the Romans do. Chinese: 人 在 罗 马 就 像 罗 马 人一 样 做 事。</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>0<em>.</em>81</p></div>
<div class="para"><p>where instructions are highlighted in gray. An interesting case is that in the last example we</p></div>
<div class="para"><p>5 We could use the same start symbol for different sequences. Here we use different symbols to distinguish the sequences on the encoder and decoder-sides.</p></div>
<div class="para"><p>16 <em>Pre-training</em></p></div>
<div class="para"><p>reframe the scoring problem as the text generation problem. Our goal is to generate a text representing the number 0<em>.</em>81, rather than outputting it as a numerical value.</p></div>
<div class="para"><p>The approach described above provides a new framework of universal language understanding and generation. Both the task instructions and the problem inputs are provided to the system in text form. The system then follows the instructions to complete the task. This method puts different problems together, with the benefit of training a single model that can perform many tasks simultaneously.</p></div>
<div class="para"><p>In general, fine-tuning is necessary for adapting the pre-trained model to a specific downstream task. In this process, one can use different ways to instruct the model for the task, such as using a short name of the task as the prefix to the actual input sequence or providing a detailed description of the task. Since the task instructions are expressed in text form and involved as part of the input,the general knowledge of instruction can be gained through learning the language understanding models in the pre-training phase. This may help enable zero-shot learning. For example, pretrained models can generalize to address new problems where the task instructions have never been encountered.</p></div>
<div class="para"><p>There have been several powerful methods of self-supervised learning for either Transformer encoders or decoders. Applying these methods to pre-train encoder-decoder models is relatively straightforward. One common choice is to train encoder-decoder models as language models. For example, the encoder receives a sequence prefix, while the decoder generates the remaining sequence. However, this differs from standard causal language modeling, where the entire sequence is autoregressively generated from the first token. In our case, the encoder processes the prefix at once, and then the decoder predicts subsequent tokens in the manner of causal language modeling.Put more precisely, this is a<strong> prefix language modeling</strong> problem: a language model predicts the subsequent sequence given a prefix, which serves as the context for prediction.</p></div>
<div class="para"><p>Consider the following example</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>outside the house .|{z}Subsequent Sequence</p></div>
<div class="para"><p>[CLS] The puppies are frolicking|{z}Prefix</p></div>
<div class="para"><p>We can directly train an encoder-decoder model using examples like this. Then, the encoder learns to understand the prefix, and the decoder learns to continue writing based on this understanding.For large-scale pre-training, it is easy to create a large number of training examples from unlabeled text.</p></div>
<div class="para"><p>It is worth noting that for pre-trained encoder-decoder models to be effective in multi-lingual and cross-lingual tasks, such as machine translation, they should be trained with multi-lingual data. This typically requires that the vocabulary includes tokens from all the languages. By doing so, the models can learn shared representations across different languages, thereby enabling capabilities in both language understanding and generation in a multi-lingual and cross-lingual context.</p></div>
<div class="para"><p>A second approach to pre-training encoder-decoder models is masked language modeling. In this approach, as discussed in Section 1.2.2, tokens in a sequence are randomly replaced with a mask symbol, and the model is then trained to predict these masked tokens based on the entire masked sequence.</p></div>
<div class="para"><p>As an illustration, consider the task of masking and reconstructing the sentence</p></div>
<div class="para"><p><em>1.2 Self-supervised Pre-training Tasks</em> 17</p></div>
<div class="para"><p>The puppies are frolicking outside the house .</p></div>
<div class="para"><p>By masking two tokens (say,<em> frolicking</em> and<em> the</em>), we have the BERT-style input and output of the model, as follows</p></div>
<div class="para"><p>[CLS] The puppies are [MASK] outside [MASK] house .</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>frolicking the</p></div>
<div class="para"><p>Here denotes the masked position at which we do not make token predictions. By varying the percentage of the tokens in the text, this approach can be generalized towards either BERT-style training or language modeling-style training [Song et al., 2019]. For example, if we mask out all the tokens, then the model is trained to generate the entire sequence</p></div>
<div class="para"><p>[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>The puppies are frolicking outside the house .</p></div>
<div class="para"><p>In this case, we train the decoder as a language model.</p></div>
<div class="para"><p>Note that, in the context of the encoder-decoder architecture, we can use the encoder to read the masked sequence, and use the decoder to predict the original sequence. With this objective,we essentially have a denoising autoencoder: the encoder transforms a corrupted input into some hidden representation, and the decoder reconstructs the uncorrupted input from this hidden representation. Here is an example of input and output for denoising training.</p></div>
<div class="para"><p>[CLS] The puppies are [MASK] outside [MASK] house .</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>The puppies are frolicking outside the house .</p></div>
<div class="para"><p>By learning to map from this corrupted sequence to its uncorrupted counterpart, the model gains the ability to understand on the encoder side and to generate on the decoder side. See Figure 1.4 for an illustration of how an encoder-decoder model is trained with BERT-style and denoising autoencoding objectives.</p></div>
<div class="para"><p>As we randomly select tokens for masking, we can certainly mask consecutive tokens [Joshi et al., 2020]. Here is an example.</p></div>
<div class="para"><p>[CLS] The puppies are [MASK] outside [MASK] [MASK] .</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>The puppies are frolicking outside the house .</p></div>
<div class="para"><p>Another way to consider consecutive masked tokens is to represent them as spans. Here we follow Raffel et al. [2020]’s work, and use [X], [Y] and [Z] to denote sentinel tokens that cover one or more consecutive masked tokens. Using this notation, we can re-express the above training example as</p></div>
<div class="para"><p>[CLS] The puppies are [X] outside [Y] .</p></div>
<div class="para"><p><em>→</em><em>⟨</em><em>s</em><em>⟩</em>[X] frolicking [Y] the house [Z]</p></div>
<h2>18 Pre-training</h2>
<div class="para"><p>Loss</p></div>
<div class="para"><p>[M][M][M]frolicking[M]the[M][M]</p></div>
<h2>Encoder Decoder</h2>
<div class="para"><p>[CLS] The puppies are[M]in[M] house.</p></div>
<div class="para"><p>frolicking[M][M][M]<em>⟨</em><em>s</em><em>⟩</em>[M]the[M]</p></div>
<h3>(a) Training an encoder-decoder model with BERT-style masked language modeling</h3>
<h3>Loss over the sequence</h3>
<div class="para"><p>The puppies are frolicking in the house.</p></div>
<h2>Encoder Decoder</h2>
<div class="para"><p>[CLS] The puppies are[M]in[M] house.</p></div>
<div class="para"><p>frolicking are puppies The<em>⟨</em><em>s</em><em>⟩</em>in the house</p></div>
<h3>(b) Training an encoder-decoder model with denoising autoencoding</h3>
<div class="para"><p><strong>Fig. 1.4:</strong> Training an encoder-decoder model using BERT-style and denoising autoencoding methods. In both methods,the input to the encoder is a corrupted token sequence where some tokens are masked and replaced with [MASK] (or[M] for short). The decoder predicts these masked tokens, but in different ways. In BERT-style training, the decoder only needs to compute the loss for the masked tokens, while the remaining tokens in the sequence can be simply treated as [MASK] tokens. In denoising autoencoding, the decoder predicts the sequence of all tokens in an autoregressive manner. As a result, the loss is obtained by accumulating the losses of all these tokens, as in standard language modeling.</p></div>
<h2>The idea is that we represent the corrupted sequence as a sequence containing placeholder slots. The training task is to fill these slots with the correct tokens using the surrounding context. An advantage of this approach is that the sequences used in training would be shorter, making the training more efficient. Note that masked language modeling provides a very general framework for training encoder-decoder models. Various settings can be adjusted to have different training versions, such as altering the percentage of tokens masked and the maximum length of the masked spans.</h2>
<h2>1.2.3.2 Denoising Training</h2>
<h2>If we view the problem of training encoder-decoder models as a problem of training denoising autoencoders, there will typically be many different methods for introducing input corruption and reconstructing the input. For instance, beyond randomly masking tokens, we can also alter some of them or rearrange their order.</h2>
<h2>Suppose we have an encoder-decoder model that can map an input sequence x to an output</h2>
<div class="para"><p><em>1.2 Self-supervised Pre-training Tasks</em> 19</p></div>
<div class="para"><p>sequence<strong> y</strong></p></div>
<div class="para"><p><strong>y</strong>=Decode <em>ω</em>(Encode <em>θ</em>(<strong>x</strong>))</p></div>
<div class="para"><p>=Model <em>θ,ω</em>(<strong>x</strong>)(1.15)</p></div>
<div class="para"><p>where<em> θ</em> and<em> ω</em> are the parameters of the encoder and the decoder, respectively. In denoising autoencoding problems, we add some noise to<strong> x</strong> to obtain a noisy, corrupted input<strong> x</strong> noise. By feeding<strong> x</strong> noise into the encoder, we wish the decoder to output the original input. The training objective can be defined as</p></div>
<div class="para"><p>(ˆ <em>θ,</em> ˆ <em>ω</em>)=arg min <em>θ,ω</em> Loss(Model <em>θ,ω</em>(<strong>x</strong> noise)<em>,</em><strong> x</strong>)(1.16)</p></div>
<div class="para"><p>Here the loss function Loss(Model <em>θ,ω</em>(<strong>x</strong> noise)<em>,</em><strong> x</strong>) evaluates how well the model Model <em>θ,ω</em>(<strong>x</strong> noise)reconstructs the original input<strong> x</strong>. We can choose the cross-entropy loss as usual.</p></div>
<div class="para"><p>As the model architecture and the training approach have been developed, the remaining issue is the corruption of the input. Lewis et al. [2020], in their<strong> BART</strong> model, propose corrupting the input sequence in several different ways.</p></div>
<div class="para"><p>•<strong> Token Masking</strong>. This is the same masking method that we used in masked language modeling. The tokens in the input sequence are randomly selected and masked.</p></div>
<div class="para"><p>•<strong> Token Deletion</strong>. This method is similar to token masking. However, rather than replacing the selected tokens with a special symbol [MASK], these tokens are removed from the sequence. See the following example for a comparison of the token masking and token deletion methods.</p></div>
<div class="para"><p>Original (<strong>x</strong>):The puppies are frolicking outside the house .</p></div>
<div class="para"><p>Token Masking (<strong>x</strong> noise):The puppies are [MASK] outside [MASK] house .</p></div>
<div class="para"><p>Token Deletion (<strong>x</strong> noise):The puppies are frolicking outside the house .</p></div>
<div class="para"><p>where the underlined tokens in the original sequence are masked or deleted.</p></div>
<div class="para"><p>•<strong> Span Masking</strong>. Non-overlapping spans are randomly sampled over the sequence. Each span is masked by [MASK]. We also consider spans of length 0, and, in such cases, [MASK]is simply inserted at a position in the sequence. For example, we can use span masking to corrupt the above sequence as</p></div>
<div class="para"><p>Original (<strong>x</strong>):The 0 puppies are frolicking outside the house .</p></div>
<div class="para"><p>Span Masking (<strong>x</strong> noise):The [MASK] puppies are [MASK] house .</p></div>
<div class="para"><p>Here the span<em> frolicking outside the</em> is replaced with a single [MASK]. 0 indicates a length-0 span, and so we insert an [MASK] between<em> The</em> and<em> puppies</em>. Span masking introduces new prediction challenges in which the model needs to know how many tokens are generated from a span. This problem is very similar to fertility modeling in machine translation[Brown et al., 1993].</p></div>
<div class="para"><p>20 <em>Pre-training</em></p></div>
<div class="para"><p>If we consider a sequence consisting of multiple sentences, additional methods of corruption can be applied. In the BART model, there are two such methods.</p></div>
<div class="para"><p>•<strong> Sentence Reordering</strong>. This method randomly permutes the sentences so that the model can learn to reorder sentences in a document. Consider, for example, two consecutive sentences</p></div>
<div class="para"><p>Hard work leads to success . Success brings happiness .</p></div>
<div class="para"><p>We can reorder the two sentences to have a corrupted input sequence</p></div>
<div class="para"><p>Success brings happiness . Hard work leads to success .</p></div>
<div class="para"><p>•<strong> Document Rotation</strong>. The goal of this task is to identify the start token of the sequence.First, a token is randomly selected from the sequence. Then, the sequence is rotated so that the selected token is the first token. For example, suppose we select the token<em> leads</em> from the above sequence. The rotated sequence is</p></div>
<div class="para"><p>selected</p></div>
<div class="para"><p>leads to success . Success brings happiness .Hard work Hard work</p></div>
<div class="para"><p>where the subsequence<em> Hard work</em> before<em> leads</em> is appended to the end of the sequence.</p></div>
<div class="para"><p>For pre-training, we can apply multiple corruption methods to learn robust models, for example, we randomly choose one of them for each training sample. In practice, the outcome of encoder-decoder pre-training depends heavily on the input corruption methods used, and so we typically need to choose appropriate training objectives through careful experimentation.</p></div>
<div class="para"><p><strong>1.2.4</strong> <strong>Comparison of Pre-training Tasks</strong></p></div>
<div class="para"><p>So far, we have discussed a number of pre-training tasks. Since the same training objective can apply to different architectures (e.g., using masked language modeling for both encoder-only and encoder-decoder pre-training), categorizing pre-training tasks based solely on model architecture does not seem ideal. Instead, we summarize these tasks based on the training objectives.</p></div>
<div class="para"><p>•<strong> Language Modeling</strong>. Typically, this approach refers to an auto-regressive generation procedure of sequences. At one time, it predicts the next token based on its previous context.</p></div>
<div class="para"><p>•<strong> Masked Language Modeling</strong>. Masked Language Modeling belongs to a general maskpredict framework. It randomly masks tokens in a sequence and predicts these tokens using the entire masked sequence.</p></div>
<div class="para"><p><em>1.3 Example: BERT</em> 21</p></div>
<div class="para"><p>•<strong> Permuted Language Modeling</strong>. Permuted language modeling follows a similar idea to masked language modeling, but considers the order of (masked) token prediction. It reorders the input sequence and predicts the tokens sequentially. Each prediction is based on some context tokens that are randomly selected.</p></div>
<div class="para"><p>•<strong> Discriminative Training</strong>. In discriminative training, supervision signals are created from classification tasks. Models for pre-training are integrated into classifiers and trained together with the remaining parts of the classifiers to enhance their classification performance.</p></div>
<div class="para"><p>•<strong> Denoising Autoencoding</strong>. This approach is applied to the pre-training of encoder-decoder models. The input is a corrupted sequence and the encoder-decoder models are trained to reconstruct the original sequence.</p></div>
<div class="para"><p>Table 1.1 illustrates these methods and their variants using examples. The use of these examples does not distinguish between models, but we mark the model architectures where the pre-training tasks can be applied. In each example, the input consists of a token sequence, and the output is either a token sequence or some probabilities. For generation tasks, such as language modeling, superscripts are used to indicate the generation order on the target side. If the superscripts are omitted, it indicates that the output sequence can be generated either autoregressively or simultaneously. On the source side, we assume that the sequence undergoes a standard Transformer encoding process, meaning that each token can see the entire sequence in self-attention.The only exception is in permuted language modeling, where an autoregressive generation process is implemented by setting attention masks on the encoder side. To simplify the discussion,we remove the token<em> ⟨</em><em>s</em><em>⟩</em>from the target-side of each example.</p></div>
<div class="para"><p>While these pre-training tasks are different, it is possible to compare them in the same framework and experimental setup [Dong et al., 2019; Raffel et al., 2020; Lewis et al., 2020]. Note that we cannot list all the pre-training tasks here as there are many of them. For more discussions on pre-training tasks, the interested reader may refer to some surveys on this topic [Qiu et al., 2020;Han et al., 2021].</p></div>
<h3>1.3 Example: BERT</h3>
<div class="para"><p>In this section, we introduce BERT models, which are among the most popular and widely used pre-trained sequence encoding models in NLP.</p></div>
<div class="para"><p><strong>1.3.1</strong> <strong>The Standard Model</strong></p></div>
<div class="para"><p>The standard BERT model, which is proposed in Devlin et al. [2019]’s work, is a Transformer encoder trained using both masked language modeling and next sentence prediction tasks. The loss used in training this model is a sum of the loss of the two tasks.</p></div>
<div class="para"><p>Loss BERT=Loss MLM + Loss NSP(1.17)</p></div>
<div class="para"><p>As is regular in training deep neural networks, we optimize the model parameters by minimizing this loss. To do this, a number of training samples are collected. During training, a batch of</p></div>
<div class="para"><p>22 <em>Pre-training</em></p></div>
<div class="para"><p>Method Enc Dec E-D Input Output</p></div>
<div class="para"><p>Causal LM<em>•</em><em>•</em>The 1 kitten 2 is 3 chasing 4 the 5 ball 6 .7</p></div>
<div class="para"><p>Prefix LM<em>•</em><em>•</em>[C] The kitten is chasing 1 the 2 ball 3 .4</p></div>
<div class="para"><p>Masked LM<em>•</em><em>•</em>[C] The kitten [M] chasing the [M] .is ball</p></div>
<div class="para"><p>MASS-style<em>•</em><em>•</em>[C] The kitten [M] [M] [M] ball .is chasing the</p></div>
<div class="para"><p>BERT-style<em>•</em><em>•</em>[C] The kitten [M] playing the [M] .kitten is chasing ball</p></div>
<div class="para"><p>Permuted LM<em>•</em>[C] The kitten is chasing the ball .The 5 kitten 7 is 6 chasing 1 the 4 ball 2 .3</p></div>
<div class="para"><p>Next Sentence<em>•</em>[C] The kitten is chasing the ball .Pr(IsNext<em> |</em> representation-of-[C])</p></div>
<div class="para"><p>Prediction Birds eat worms .</p></div>
<div class="para"><p>Sentence<em>•</em>Encode a sentence as<strong> h</strong> <em>a</em> and Score(<strong>h</strong> <em>a</em><em>,</em><strong> h</strong> <em>b</em>)</p></div>
<div class="para"><p>Comparison another sentence as<strong> h</strong> <em>b</em></p></div>
<div class="para"><p>Token Classification<em>•</em>[C] The kitten is chasing the ball .Pr(<em>·|</em>The) Pr(<em>·|</em>kitten) ... Pr(<em>·|</em>.)</p></div>
<div class="para"><p>Token Reordering<em>•</em>[C] . kitten the chasing The is ball The 1 kitten 2 is 3 chasing 4 the 5 ball 6 .7</p></div>
<div class="para"><p>Token Deletion<em>•</em>[C] The kitten is chasing the ball .The 1 kitten 2 is 3 chasing 4 the 5 ball 6 .7</p></div>
<div class="para"><p>Span Masking<em>•</em>[C] The kitten [M] is [M] .The 1 kitten 2 is 3 chasing 4 the 5 ball 6 .7</p></div>
<div class="para"><p>Sentinel Masking<em>•</em>[C] The kitten [X] the [Y][X]1 is 2 chasing 3 [Y]4 ball 5 .6</p></div>
<div class="para"><p>Sentence<em>•</em>[C] The ball rolls away swiftly . The The 1 kitten 2 is 3 chasing 4 the 5 ball 6 .7</p></div>
<div class="para"><p>Reordering kitten is chasing the ball .The 8 ball 9 rolls 10 away 11 swiftly 12 .13</p></div>
<div class="para"><p>Document<em>•</em>[C] chasing the ball . The ball rolls The 1 kitten 2 is 3 chasing 4 the 5 ball 6 .7</p></div>
<div class="para"><p>Rotation away swiftly . The kitten is The 8 ball 9 rolls 10 away 11 swiftly 12 .13</p></div>
<div class="para"><p><strong>Table 1.1:</strong> Comparison of pre-training tasks, including language modeling, masked language modeling, permuted language modeling, discriminative training, and denoising autoencoding. [C] = [CLS], [M] = [MASK], [X]<em>,</em> [Y] =sentinel tokens. Enc, Dec and E-D indicate whether the approach can be applied to encoder-only, decoder-only,encoder-decoder models, respectively. For generation tasks, superscripts are used to represent the order of the tokens.</p></div>
<div class="para"><p>training samples is randomly selected from this collection at a time, and Loss BERT is accumulated over these training samples. Then, the model parameters are updated via gradient descent or its variants. This process is repeated many times until some stopping criterion is satisfied, such as when the training loss converges.</p></div>
<div class="para"><p><strong>1.3.1.1</strong> <strong>Loss Functions</strong></p></div>
<div class="para"><p>In general, BERT models are used to represent a single sentence or a pair of sentences, and thus can handle various downstream language understanding problems. In this section we assume that the input representation is a sequence containing two sentences Sent <em>A</em> and Sent <em>B</em>, expressed as</p></div>
<div class="para"><p>[CLS] Sent <em>A</em> [SEP] Sent <em>B</em> [SEP]</p></div>
<div class="para"><p>Here we follow the notation in BERT’s paper and use [SEP] to denote the separator.</p></div>
<div class="para"><p>Given this sequence, we can obtain Loss MLM and Loss NSP separately. For masked language modeling, we predict a subset of the tokens in the sequence. Typically, a certain percentage of</p></div>
<div class="para"><p><em>1.3 Example: BERT</em> 23</p></div>
<div class="para"><p>the tokens are randomly selected, for example, in the standard BERT model, 15% of the tokens in each sequence are selected. Then the sequence is modified in three ways</p></div>
<div class="para"><p>•<strong> Token Masking</strong>. 80% of the selected tokens are masked and replaced with the symbol[MASK]. For example</p></div>
<div class="para"><p>Original:[CLS] It is raining . [SEP] I need an umbrella . [SEP]</p></div>
<div class="para"><p>Masked:[CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]</p></div>
<div class="para"><p>where the selected tokens are underlined. Predicting masked tokens makes the model learn to represent tokens from their surrounding context.</p></div>
<div class="para"><p>•<strong> Random Replacement</strong>. 10% of the selected tokens are changed to a random token. For example</p></div>
<div class="para"><p>Original:[CLS] It is raining . [SEP] I need an umbrella . [SEP]</p></div>
<div class="para"><p>Random Token:[CLS] It is raining . [SEP] I need an hat . [SEP]</p></div>
<div class="para"><p>This helps the model learn to recover a token from a noisy input.</p></div>
<div class="para"><p>•<strong> Unchanged</strong>. 10% of the selected tokens are kept unchanged. For example,</p></div>
<div class="para"><p>Original:[CLS] It is raining . [SEP] I need an umbrella . [SEP]</p></div>
<div class="para"><p>Unchanged Token:[CLS] It is raining . [SEP] I need an umbrella . [SEP]</p></div>
<div class="para"><p>This is not a difficult prediction task, but can guide the model to use easier evidence for prediction.</p></div>
<div class="para"><p>Let<em> A</em>(<strong>x</strong>) be the set of selected positions of a given token sequence<strong> x</strong>, and ¯<strong>x</strong> be the modified sequence of<strong> x</strong>. The loss function of masked language modeling can be defined as</p></div>
<div class="para"><p>Loss MLM=<em>−</em>X</p></div>
<div class="para"><p><em>i</em><em>∈A</em>(<strong>x</strong>)log Pr <em>i</em>(<em>x</em> <em>i</em><em>|</em>¯<strong>x</strong>)(1.18)</p></div>
<div class="para"><p>where Pr <em>i</em>(<em>x</em> <em>i</em><em>|</em>¯<strong>x</strong>) is the probability of predicting<em> x</em> <em>i</em> at the position<em> i</em> given ¯<strong>x</strong>. Figure 1.5 shows a running example of computing Loss MLM.</p></div>
<div class="para"><p>For next sentence prediction, we follow the method described in Section 1.2.2.3. Each training sample is classified into a label set<em> {</em>IsNext<em>,</em> NotNext<em>}</em>, for example,</p></div>
<div class="para"><p>Sequence:[CLS] It is raining . [SEP] I need an umbrella . [SEP]</p></div>
<div class="para"><p>Label:IsNext</p></div>
<div class="para"><p>Sequence:[CLS] The cat sleeps on the windowsill . [SEP] Apples grow on trees . [SEP]</p></div>
<div class="para"><p>Label:NotNext</p></div>
<h3>24 Pre-training</h3>
<h3>[CLS] It is raining . [SEP] I need an umbrella . [SEP] Input:</h3>
<div class="para"><p>Select tokens with a probability of 15%</p></div>
<h3>[CLS] It is raining . [SEP] I need an umbrella . [SEP] Token Selection:</h3>
<div class="para"><p>Mask selected tokens with a probability of 80%</p></div>
<h3>[CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP] Token Masking:</h3>
<div class="para"><p>Alter selected tokens with a probability of 10%</p></div>
<h3>[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP] Token: Replacement</h3>
<div class="para"><p>Keep selected tokens unchanged with a probability of 10%</p></div>
<h3>[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP] Unchanged:</h3>
<div class="para"><p>Train the Transformer encoder with the modified sequence</p></div>
<div class="para"><p>training I an umbrella</p></div>
<div class="para"><p><strong>h</strong> 0 <strong>h</strong> 1 <strong>h</strong> 2 <strong>h</strong> 3 <strong>h</strong> 4 <strong>h</strong> 5 <strong>h</strong> 6 <strong>h</strong> 7 <strong>h</strong> 8 <strong>h</strong> 9 <strong>h</strong> 10 <strong>h</strong> 11</p></div>
<h2>Transformer Encoder</h2>
<div class="para"><p><strong>e</strong> 0 <strong>e</strong> 1 <strong>e</strong> 2 <strong>e</strong> 3 <strong>e</strong> 4 <strong>e</strong> 5 <strong>e</strong> 6 <strong>e</strong> 7 <strong>e</strong> 8 <strong>e</strong> 9 <strong>e</strong> 10 <strong>e</strong> 11</p></div>
<div class="para"><p>[CLS]It is[MASK].[SEP]I need [MASK] hat.[SEP]</p></div>
<div class="para"><p><strong>Fig. 1.5:</strong> A running example of BERT-style masked language modeling. First, 15% of the tokens are randomly selected.These selected tokens are then processed in one of three ways: replaced with a [MASK] token (80% of the time),replaced with a random token (10% of the time), or kept unchanged (10% of the time). The model is trained to predict these selected tokens based on the modified sequence.<strong> e</strong> <em>i</em> represents the embedding of the token at the position<em> i</em>. Gray boxes represent the Softmax layers.</p></div>
<h3>The output vector of the encoder for the first token [CLS] is viewed as the sequence representation, denoted by h cls (or h 0). A classifier is built on top of h cls. Then, we can compute the probability of a label c given h cls, i.e., Pr(c|h cls). There are many loss functions one can choose for classification problems. For example, in maximum likelihood training, we can define Loss NSP as</h3>
<h3>Loss NSP = −log Pr(c gold|h cls) (1.19)</h3>
<div class="para"><p><em>1.3 Example: BERT</em> 25</p></div>
<div class="para"><p>where<em> c</em> gold is the correct label for this sample.</p></div>
<div class="para"><p><strong>1.3.1.2</strong> <strong>Model Setup</strong></p></div>
<div class="para"><p>As shown in Figure 1.6, BERT models are based on the standard Transformer encoder architecture.The input is a sequence of embeddings, each being the sum of the token embedding, the positional embedding, and the segment embedding.</p></div>
<div class="para"><p><strong>e</strong>=<strong>x</strong> +<strong> e</strong> pos +<strong> e</strong> seg(1.20)</p></div>
<div class="para"><p>Both the token embedding (<strong>x</strong>) and positional embedding (<strong>e</strong> pos) are regular, as in Transformer models. The segment embedding (<strong>e</strong> seg) is a new type of embedding that indicates whether a token belongs to Sent <em>A</em> or Sent <em>B</em>. This can be illustrated by the following example.</p></div>
<div class="para"><p>Token [CLS]It is raining.[SEP]I need an umbrella.[SEP]</p></div>
<div class="para"><p><strong>x</strong> <strong>x</strong> 0 <strong>x</strong> 1 <strong>x</strong> 2 <strong>x</strong> 3 <strong>x</strong> 4 <strong>x</strong> 5 <strong>x</strong> 6 <strong>x</strong> 7 <strong>x</strong> 8 <strong>x</strong> 9 <strong>x</strong> 10 <strong>x</strong> 11 <strong>e</strong> pos PE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8)PE(9)PE(10) PE(11)</p></div>
<div class="para"><p><strong>e</strong> seg <strong>e</strong> <em>A</em> <strong>e</strong> <em>A</em> <strong>e</strong> <em>A</em> <strong>e</strong> <em>A</em> <strong>e</strong> <em>A</em> <strong>e</strong> <em>A</em> <strong>e</strong> <em>B</em> <strong>e</strong> <em>B</em> <strong>e</strong> <em>B</em> <strong>e</strong> <em>B</em> <strong>e</strong> <em>B</em> <strong>e</strong> <em>B</em></p></div>
<div class="para"><p>The main part of BERT models is a multi-layer Transformer network. A Transformer layer consists of a self-attention sub-layer and an FFN sub-layer. Both of them follow the post-norm architecture: output = LNorm(<em>F</em>(input) + input), where<em> F</em>(<em>·</em>) is the core function of the sublayer (either a self-attention model or an FFN), and LNorm(<em>·</em>) is the layer normalization unit.Typically, a number of Transformer layers are stacked to form a deep network. At each position of the sequence, the output representation is a real-valued vector which is produced by the last layer of the network.</p></div>
<div class="para"><p>There are several aspects one may consider in developing BERT models.</p></div>
<div class="para"><p>•<strong> Vocabulary Size</strong> (<em>|</em><em>V</em><em> |</em>). In Transformers, each input token is represented as an entry in a vocabulary<em> V</em> . Large vocabularies can cover more surface form variants of words, but may lead to increased storage requirements.</p></div>
<div class="para"><p>•<strong> Embedding Size</strong> (<em>d</em> <em>e</em>). Every token is represented as a<em> d</em> <em>e</em>-dimensional real-valued vector.As presented above, this vector is the sum of the token embedding, positional embedding,and segment embedding, all of which are also<em> d</em> <em>e</em>-dimensional real-valued vectors.</p></div>
<div class="para"><p>•<strong> Hidden Size</strong> (<em>d</em>). The input and output of a sub-layer are of<em> d</em> dimensions. Besides, most of the hidden states of a sub-layer are<em> d</em>-dimensional vectors. In general,<em> d</em> can be roughly viewed as the width of the network.</p></div>
<div class="para"><p>•<strong> Number of Heads</strong> (<em>n</em> head). In self-attention sub-layers, one needs to specify the number of heads used in multi-head self-attention. The larger this number is, the more sub-spaces in which attention is performed. In practical systems, we often set<em> n</em> head<em> ≥</em>4.</p></div>
<div class="para"><p>•<strong> FFN Hidden Size</strong> (<em>d</em> ffn). The size of the hidden layer of the FFNs used in Transformers is typically larger than<em> d</em>. For example, a typical setting is<em> d</em> ffn = 4 <em>d</em>. For larger Transformers,such as recent large models,<em> d</em> ffn may be set to a very large value.</p></div>
<div class="para"><p>26 <em>Pre-training</em></p></div>
<div class="para"><p><em>...</em></p></div>
<div class="para"><p>Output Layer</p></div>
<div class="para"><p>Encoder Output <strong>h</strong> <em>i</em><em> ∈</em>R <em>d</em> is the contextual representation of<em> x</em> <em>i</em></p></div>
<div class="para"><p><strong>h</strong> 0 <strong>h</strong> 1<em>...</em><strong>h</strong> <em>m</em></p></div>
<div class="para"><p>Layer Normalization</p></div>
<div class="para"><p>FFN Sub-layer hidden size:<em> d</em> FFN hidden size:<em> d</em> ffn</p></div>
<div class="para"><p>FFN</p></div>
<div class="para"><p>layers</p></div>
<div class="para"><p>Layer Normalization</p></div>
<div class="para"><p>Self-attention Sub-layer hidden size:<em> d</em> number of heads:<em> n</em> head</p></div>
<div class="para"><p>Self-attention</p></div>
<div class="para"><p>Embedding</p></div>
<div class="para"><p><strong>e</strong> =<strong> x</strong> +<strong> e</strong> pos +<strong> e</strong> seg<em> ∈</em>R <em>d</em> <em>e</em></p></div>
<div class="para"><p>Position Token Segment</p></div>
<div class="para"><p>Input <em>x</em> <em>i</em> corresponds to an entry of<em> V</em></p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1<em>...x</em> <em>m</em></p></div>
<div class="para"><p><strong>Fig. 1.6:</strong> The model architecture of BERT (Transformer encoder). The input tokens are first represented as embeddings,each of which is the sum of the corresponding token embedding, positional embedding and segment embedding. Then,the embedding sequence is processed by a stack of Transformer layers. Each layer in this stack includes a self-attention sub-layer and a FFN sub-layer. The output of the BERT model is a sequence of vectors produced by the final Transformer layer.</p></div>
<div class="para"><p>•<strong> Model Depth</strong> (<em>L</em>). Using deep networks is an effective way to improve the expressive power of Transformers. For BERT models,<em> L</em> is typically set to 12 or 24. However, networks with even greater depth are also feasible and can be applied for further enhancements.</p></div>
<div class="para"><p>Different settings of these hyper-parameters lead to different model sizes. There are two widely-used BERT models.</p></div>
<div class="para"><p>• BERT base:<em> d</em> = 768,<em> L</em> = 12,<em> n</em> head = 12, total number of parameters = 110 M.</p></div>
<div class="para"><p>• BERT large:<em> d</em> = 1<em>,</em> 024,<em> L</em> = 24,<em> n</em> head = 16, total number of parameters = 340 M.</p></div>
<div class="para"><p>Training BERT models follows the standard training process of Transformers. Training larger models such as BERT large requires more training effort and time. This is a common problem for pre-training, especially when a model is trained on a very large amount of data. In practice,</p></div>
<div class="para"><p><em>1.3 Example: BERT</em> 27</p></div>
<div class="para"><p>there are often considerations of training efficiency. For example, a practice is to first train a BERT model on relatively short sequences for a large number of training steps, and then continue training it on full-length sequences for the remaining training steps.</p></div>
<div class="para"><p><strong>1.3.2</strong> <strong>More Training and Larger Models</strong></p></div>
<div class="para"><p>BERT is a milestone model in NLP, sparking many subsequent efforts to improve it. One direction is to scale up the model itself, including increasing training data and developing larger models.</p></div>
<div class="para"><p><strong>RoBERTa</strong>, an extension of the standard BERT model, is an example of such efforts [Liu et al.,2019]. It introduces two major improvements. First, simply using more training data and more compute can improve BERT models without need of changing the model architectures. Second,removing the NSP loss does not decrease the performance on downstream tasks if the training is scaled up. These findings suggest exploring a general direction of pre-training: we can continue to improve pre-training by scaling it up on simple pre-training tasks.</p></div>
<div class="para"><p>A second approach to improving BERT models is to increase the number of model parameters. For example, in He et al. [2021]’s work, a 1.5 billion-parameter BERT-like model is built by increasing both the model depth and hidden size. However, scaling up BERT and various other pre-trained models introduces new challenges in training, for example, training very large models often becomes unstable and difficult to converge. This makes the problem more complicated, and requires careful consideration of various aspects, including model architecture, parallel computation, parameter initialization, and so on. In another example, Shoeybi et al. [2019] successfully trained a 3.9 billion-parameter BERT-like model, where hundreds of GPUs were used to manage the increased computational demands.</p></div>
<div class="para"><p><strong>1.3.3</strong> <strong>More Efficient Models</strong></p></div>
<div class="para"><p>Compared to its predecessors, BERT is a relatively large model for the time it was proposed.This increase in model size results in larger memory requirements and a consequent slowdown in system performance. Developing smaller and faster BERT models is part of the broader challenge of building efficient Transformers, which has been extensively discussed in Tay et al. [2020]’s work and Xiao and Zhu [2023]’s work. However, a deeper discussion of this general topic is beyond the scope of our current discussion. Here we instead consider a few efficient variants of BERT.</p></div>
<div class="para"><p>Several threads of research are of interest to NLP researchers in developing efficient BERT models. First, work on knowledge distillation, such as training student models with the output of well-trained teacher models, shows that smaller BERT models can be obtained by transferring knowledge from larger BERT models. Given that BERT models are multi-layer networks with several different types of layers, knowledge distillation can be applied at different levels of representation. For example, beyond distilling knowledge from the output layers, it is also possible to incorporate training loss that measures the difference in output of hidden layers between teacher models and student models [Sun et al., 2020; Jiao et al., 2020]. Indeed, knowledge distillation has been one of the most widely-used techniques for learning small pre-trained models.</p></div>
<div class="para"><p>Second, conventional model compression methods can be directly applied to compress BERT models. One common approach is to use general-purpose pruning methods to prune the Transformer encoding networks [Gale et al., 2019]. This generally involves removing entire layers [Fan</p></div>
<div class="para"><p>28 <em>Pre-training</em></p></div>
<div class="para"><p>et al., 2019] or a certain percentage of parameters in the networks [Sanh et al., 2020; Chen et al.,2020]. Pruning is also applicable to multi-head attention models. For example, Michel et al.[2019] show that removing some of the heads does not significantly decrease the performance of BERT models, but speeds up the inference of these models. Another approach to compressing BERT models is quantization [Shen et al., 2020]. By representing model parameters as lowprecision numbers, the models can be greatly compressed. While this method is not specific to BERT models, it proves effective for large Transformer-based architectures.</p></div>
<div class="para"><p>Third, considering that BERT models are relatively deep and large networks, another thread of research uses dynamic networks to adapt these models for efficient inference. An idea in this paradigm is to dynamically choose the layers for processing a token, for example, in depthadaptive models we exit at some optimal depth and thus skip the rest of the layers in the layer stack [Xin et al., 2020; Zhou et al., 2020]. Similarly, we can develop length-adaptive models in which the length of the input sequence is dynamically adjusted. For example, we can skip some of the tokens in the input sequence so that the model can reduce computational load on less important tokens, enhancing overall efficiency.</p></div>
<div class="para"><p>Fourth, it is also possible to share parameters across layers to reduce the size of BERT models.A simple way to do this is to share the parameters of a whole Transformer layer across the layer stack [Dehghani et al., 2018; Lan et al., 2020]. In addition to the reduced number of parameters,this enables reuse of the same layer in a multi-layer Transformer network, leading to savings of memory footprint at test time.</p></div>
<div class="para"><p><strong>1.3.4</strong> <strong>Multi-lingual Models</strong></p></div>
<div class="para"><p>The initial BERT model was primarily focused on English. Soon after this model was proposed,it was extended to many languages. One simple way to do this is to develop a separate model for each language. Another approach, which has become more popular in recent work on large language models, is to train multi-lingual models directly on data from all the languages. In response,<strong> multi-lingual BERT</strong> (<strong>mBERT</strong>) models were developed by training them on text from 104 languages 6. The primary difference from monolingual BERT models is that mBERT models use larger vocabularies to cover tokens from multiple languages. As a result, the representations of tokens from different languages are mapped into the same space, allowing for the sharing of knowledge across languages via this universal representation model.</p></div>
<div class="para"><p>One important application of multi-lingual pre-trained models is cross-lingual learning. In the cross-lingual setting, we learn a model on tasks in one language, and apply it to the same tasks in another language. In cross-lingual text classification, for example, we fine-tune a multi-lingual pre-trained model on English annotated documents. Then, we use the fine-tuned model to classify Chinese documents.</p></div>
<div class="para"><p>An improvement to multi-lingual pre-trained models like mBERT is to introduce bilingual data into pre-training. Rather than training solely on monolingual data from multiple languages, bilingual training explicitly models the relationship between tokens in two languages. The resulting model will have innate cross-lingual transfer abilities, and thus can be easily adapted to different languages. Lample and Conneau [2019] propose an approach to pre-training<strong> cross-lingual lan-</strong><strong>guage models</strong> (<strong>XLMs</strong>). In their work, a cross-lingual language model can be trained in either the causal language modeling or masked language modeling manner. For masked language modeling</p></div>
<div class="para"><p>6 https://github.com/google-research/bert/</p></div>
<div class="para"><p><em>1.3 Example: BERT</em> 29</p></div>
<div class="para"><p>pre-training, the model is treated as an encoder. The training objective is the same as BERT: we maximize the probabilities of some randomly selected tokens which are either masked, replaced with random tokens, or kept unchanged in the input. If we consider bilingual data in pre-training,we sample a pair of aligned sentences each time. Then, the two sentences are packed together to form a single sequence used for training. For example, consider an English-Chinese sentence pair</p></div>
<div class="para"><p>鲸鱼 是 哺 乳 动 物。<em>↔</em>Whales are mammals .</p></div>
<div class="para"><p>We can pack them to obtain a sequence, like this</p></div>
<div class="para"><p>[CLS] 鲸鱼 是 哺 乳 动 物。[SEP] Whales are mammals . [SEP]</p></div>
<div class="para"><p>We then select a certain percentage of the tokens and replace them with [MASK].</p></div>
<div class="para"><p>[CLS] [MASK] 是[MASK] 动 物。[SEP] Whales [MASK] [MASK] . [SEP]</p></div>
<div class="para"><p>The goal of pre-training is to maximize the product of the probabilities of the masked tokens given the above sequence. By performing training in this way, the model can learn to represent both the English and Chinese sequences, as well as to capture the correspondences between tokens in the two languages. For example, predicting the Chinese token 鲸鱼 may require the information from the English token<em> Whales</em>. Aligning the representations of the two languages essentially transforms the model into a “translation” model. So this training objective is also called<strong> transla-</strong><strong>tion language modeling</strong>. Figure 1.7 shows an illustration of this approach.</p></div>
<div class="para"><p>A benefit of multi-lingual pre-trained models is their inherent capability of handling codeswitching. In NLP and linguistics, code-switching refers to switching among languages in a text.For example, the following is a mixed language text containing both Chinese and English:</p></div>
<div class="para"><p>周 末 我 们 打 算 去 做 hiking ，你 想 一 起 来 吗？</p></div>
<div class="para"><p>(We plan to go hiking this weekend, would you like to join us?)</p></div>
<div class="para"><p>For multi-lingual pre-trained models, we do not need to identify whether a token is Chinese or English. Instead, every token is just an entry of the shared vocabulary. This can be imagined as creating a “new” language that encompasses all the languages we want to process.</p></div>
<div class="para"><p>The result of multi-lingual pre-training is influenced by several factors. Given that the model architecture is fixed, one needs to specify the size of the shared vocabulary, the number (or percentage) of samples in each language, the size of the model, and so on. Conneau et al. [2020]point out several interesting issues regarding large-scale multi-lingual pre-training for XLM-like models. First, as the number of supported languages increases, a larger model is needed to handle these languages. Second, a larger shared vocabulary is helpful for modeling the increased diversity in languages. Third, low-resource languages more easily benefit from cross-lingual transfer from high-resource languages, particularly when similar high-resource languages are involved in pre-training. However,<strong> interference</strong> may occur if the model is trained for an extended period,</p></div>
<h3>30 Pre-training</h3>
<div class="para"><p>鲸鱼 哺 乳 are mammals</p></div>
<div class="para"><p><strong>h</strong> 0 <strong>h</strong> 1 <strong>h</strong> 2 <strong>h</strong> 3 <strong>h</strong> 4 <strong>h</strong> 5 <strong>h</strong> 6 <strong>h</strong> 7 <strong>h</strong> 8 <strong>h</strong> 9 <strong>h</strong> 10 <strong>h</strong> 11</p></div>
<h2>Transformer Encoder</h2>
<div class="para"><p><strong>e</strong> 0 <strong>e</strong> 1 <strong>e</strong> 2 <strong>e</strong> 3 <strong>e</strong> 4 <strong>e</strong> 5 <strong>e</strong> 6 <strong>e</strong> 7 <strong>e</strong> 8 <strong>e</strong> 9 <strong>e</strong> 10 <strong>e</strong> 11</p></div>
<div class="para"><p>[CLS] [MASK]是[MASK] 动 物。[SEP] Whales [MASK] [MASK].[SEP]</p></div>
<div class="para"><p>(zh)(zh)(zh)(zh)(zh)(zh)(zh)(en)(en)(en)(en)(en)</p></div>
<div class="para"><p><strong>Fig. 1.7:</strong> An illustration of translation language modeling. For ease of understanding, we present a simple example where all the selected tokens are masked. The model is trained to predict these masked tokens. As the sequence contains tokens in two languages, predicting a token in one language allows access to tokens in the other language,thereby enabling cross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e.,<strong> e</strong> <em>i</em>) is the sum of the token embedding, positional embedding, and language embedding. This requires that each token is assigned with a language label. Thus we can distinguish tokens in different languages. In multi-lingual pre-training, particularly in work using shared vocabularies, specifying the language to which a token belongs is not necessary. The use of language embeddings in turn makes it difficult to handle code-switching. Therefore, we assume here that all token representations are language-independent.</p></div>
<h3>meaning the overall performance of the pre-trained model starts decreasing at a certain point dur- ing pre-training. Thus, in practical systems, one may need to stop the pre-training early to prevent interference.</h3>
<h2>1.4 Applying BERT Models</h2>
<h3>Once a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT models are not immediately ready for performing specific downstream tasks. In general, additional fine- tuning work is required to make them adapt. As a first step, we need a predictor to align the output of the model with the problem of interest. Let BERT ˆ θ(·) be a BERT model with pre- trained parameters ˆ θ, and Predict ω(·) be a prediction network with parameters ω. By integrating the prediction network with the output of the BERT model, we develop a model to tackle the downstream tasks. This model can be expressed as</h3>
<h3>y = Predict ω(BERT ˆ θ(x)) (1.21)</h3>
<h3>where x is the input and y is the output that fits the problem. For example, in classification problems, the model outputs a probability distribution over labels.</h3>
<h3>Then, we collect a set of labeled samples D, and fine-tune the model by</h3>
<h3>(˜ω, ˜θ) = arg min ω,ˆ θ+</h3>
<div class="para"><p>X</p></div>
<h3>(x,y gold)∈D Loss(y ω,ˆ θ+, y gold) (1.22)</h3>
<div class="para"><p><em>1.4 Applying BERT Models</em> 31</p></div>
<div class="para"><p>where (<strong>x</strong><em>,</em><strong> y</strong> gold) represents a tuple of an input and its corresponding output. The notation of this equation seems a bit complicated, but the training/tuning process is standard. We optimize the model by minimizing the loss over the tuning samples. The outcome is the optimized parameters˜<em>ω</em> and ˜<em>θ</em>. The optimization starts with the pre-trained parameters ˆ <em>θ</em>. Here we use ˆ <em>θ</em>+ to indicate that the parameters are initialized with ˆ <em>θ</em>, and use<strong> y</strong> <em>ω,</em>ˆ <em>θ</em>+ to denote the model output computed using</p></div>
<div class="para"><p>the parameters<em> ω</em> and ˆ <em>θ</em>+.</p></div>
<div class="para"><p>With the fine-tuned parameters ˜<em>ω</em> and ˜<em>θ</em>, we can apply the model Predict˜<em>ω</em>(BERT˜<em>θ</em>(<em>·</em>)) to new data of the same tasks for which the model was fine-tuned. The form of the downstream tasks determines the input and output formats of the model, as well as the architecture of the prediction network. In the following we list some tasks to which BERT models are generally suited.</p></div>
<div class="para"><p>•<strong> Classification</strong> (Single Text). One of the most widely-used applications of BERT models is text classification. In this task, a BERT model receives a sequence of tokens and encodes it as a sequence of vectors. The first output vector<strong> h</strong> cls (or<strong> h</strong> 0) is typically used as the representation of the entire text. The prediction network takes<strong> h</strong> cls as input to produce a distribution of labels. Let [CLS]<em>x</em> 1 <em>x</em> 2<em>...x</em> <em>m</em> be an input text. See below for an illustration of BERT-based text classification.</p></div>
<div class="para"><p>Class</p></div>
<div class="para"><p><strong>h</strong> cls <strong>h</strong> 1 <strong>h</strong> 2<em>...</em><strong>h</strong> <em>m</em> <strong>h</strong> <em>m</em>+1</p></div>
<div class="para"><p>BERT</p></div>
<div class="para"><p><strong>e</strong> cls <strong>e</strong> 1 <strong>e</strong> 2<em>...</em><strong>e</strong> <em>m</em> <strong>e</strong> <em>m</em>+1</p></div>
<div class="para"><p>[CLS]<em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>m</em>[SEP]</p></div>
<div class="para"><p>Here the gray box denotes the prediction network. Many NLP problems can be categorized as text classification tasks, and there have been several text classification benchmarks for evaluating pre-trained models. For example, we can classify texts by their grammatical correctness (grammaticality) or emotional tone (sentiment) [Socher et al., 2013; Warstadt et al., 2019]. Note that the prediction network could be any classification model, such as a deep neural network or a more traditional classification model. The entire model can then be trained or fine-tuned in the manner of a standard classification model. For example,the prediction network can be simply a Softmax layer and the model parameters can be optimized by maximizing the probabilities of the correct labels.</p></div>
<div class="para"><p>•<strong> Classification</strong> (Pair of Texts). Classification can also be performed on a pair of texts. Suppose we have two texts,<em> x</em> 1<em>...x</em> <em>m</em> and<em> y</em> 1<em>...y</em> <em>n</em>. We can concatenate these texts to form a single sequence with a length<em> len</em>. Then, we predict a label for this combined text sequence based on the<strong> h</strong> cls vector, as follows</p></div>
<h3>32 Pre-training</h3>
<div class="para"><p>Class</p></div>
<div class="para"><p><strong>h</strong> cls <strong>h</strong> 1 <strong>h</strong> 2<em>...</em><strong>h</strong> <em>m</em> <strong>h</strong> <em>m</em>+1<strong> h</strong> <em>m</em>+2<strong> h</strong> <em>m</em>+3<em>...</em><strong>h</strong> <em>len</em><em>−</em>1 <strong>h</strong> <em>len</em></p></div>
<h3>BERT</h3>
<div class="para"><p><strong>e</strong> cls <strong>e</strong> 1 <strong>e</strong> 2<em>...</em><strong>e</strong> <em>m</em> <strong>e</strong> <em>m</em>+1 <strong>e</strong> <em>m</em>+2 <strong>e</strong> <em>m</em>+3<em>...</em><strong>e</strong> <em>len</em><em>−</em>1 <strong>e</strong> <em>len</em></p></div>
<div class="para"><p>[CLS]<em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>m</em>[SEP]<em>y</em> 1 <em>y</em> 2<em>...</em><em>y</em> <em>n</em>[SEP]</p></div>
<div class="para"><p>Text 1 Text 2</p></div>
<h3>where len = n+m+2. Text pair classification covers several problems, including semantic equivalence judgement (determine whether two texts are semantically equivalent) [Dolan and Brockett, 2005], text entailment judgement (determine whether a hypothesis can be logically inferred or entailed from a premise) [Bentivogli and Giampiccolo, 2011; Williams et al., 2018], grounded commonsense inference (determine whether an event is likely to happen given its context) [Zellers et al., 2018], and question-answering inference (determine whether an answer corresponds to a given question).</h3>
<h3>Regression. Instead of generating a label distribution, we can have the prediction network output a real-valued score. For example, by adding a Sigmoid layer to the prediction net- work, the system can be employed to compute the similarity between two given sentences. The architecture is the same as that of BERT-based classification systems, with only the change of the output layer.</h3>
<div class="para"><p>Number (similarity, evaluation score, etc.)</p></div>
<div class="para"><p><strong>h</strong> cls <strong>h</strong> 1 <strong>h</strong> 2<em>...</em><strong>h</strong> <em>m</em> <strong>h</strong> <em>m</em>+1<strong> h</strong> <em>m</em>+2<strong> h</strong> <em>m</em>+3<em>...</em><strong>h</strong> <em>len</em><em>−</em>1 <strong>h</strong> <em>len</em></p></div>
<h3>BERT</h3>
<div class="para"><p><strong>e</strong> cls <strong>e</strong> 1 <strong>e</strong> 2<em>...</em><strong>e</strong> <em>m</em> <strong>e</strong> <em>m</em>+1 <strong>e</strong> <em>m</em>+2 <strong>e</strong> <em>m</em>+3<em>...</em><strong>e</strong> <em>len</em><em>−</em>1 <strong>e</strong> <em>len</em></p></div>
<div class="para"><p>[CLS]<em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>m</em>[SEP]<em>y</em> 1 <em>y</em> 2<em>...</em><em>y</em> <em>n</em>[SEP]</p></div>
<div class="para"><p>Text 1 Text 2</p></div>
<h3>For training or fine-tuning, we can minimize the regression loss of the model output as usual.</h3>
<h3>Sequence Labeling. Sequence labeling is a machine learning approach applicable to a wide range of NLP problems. This approach assigns a label to each token in an input sequence, and some linguistic annotations can then be derived from this sequence of labels. An ex- ample of sequence labeling in NLP is part-of-speech (POS) tagging. We label each word in a sentence with its corresponding POS tag. Another example is named entity recognition (NER) in which we label each word with an NER tag, and named entities are identified using these tags. See below for an illustration of the model architecture for NER.</h3>
<h3>1.4 Applying BERT Models 33</h3>
<div class="para"><p><em>{</em>B<em>,</em> I<em>,</em> O<em>}{</em>B<em>,</em> I<em>,</em> O<em>}</em><em>{</em>B<em>,</em> I<em>,</em> O<em>}</em>Tag Tag Tag</p></div>
<div class="para"><p><strong>h</strong> cls <strong>h</strong> 1 <strong>h</strong> 2<em>...</em><strong>h</strong> <em>m</em> <strong>h</strong> <em>m</em>+1</p></div>
<h3>BERT</h3>
<div class="para"><p><strong>e</strong> cls <strong>e</strong> 1 <strong>e</strong> 2<em>...</em><strong>e</strong> <em>m</em> <strong>e</strong> <em>m</em>+1</p></div>
<div class="para"><p>[CLS]<em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>m</em>[SEP]</p></div>
<h3>Here {B, I, O} is the tag set of NER. For example, B-ORG means the beginning of an organization, I-ORG means the word is inside an organization, and O means the word does not belong to any named entity. This NER model can output a distribution over the tag set at each position, denoted as p i. The training or fine-tuning of the model can be performed over these distributions {p 1, ..., p m}. For example, suppose p i(tag i) is the probability of the correct tag at position i. The training loss can be defined to be the negative likelihood</h3>
<div class="para"><p><em>m</em> X</p></div>
<h3>Loss = −1</h3>
<h3>i=1 log p i(tag i) (1.23)</h3>
<h3>m</h3>
<h3>Finding the best label sequence given a trained NER model is a well-studied issue in NLP. This is often achieved via dynamic programming, which, in the context of path finding over a lattice, has linear complexity [Huang, 2009].</h3>
<h3>Span Prediction. Some NLP tasks require predicting a span in a text. A common example is reading comprehension. In this task, we are given a query x 1...x m and a context text y 1...y n. The goal is to identify a continuous span in y 1...y n that best answers the query. This problem can be framed as a sequence labeling-like task in which we predict a label for each y j to indicate the beginning or ending of the span. Following Seo et al. [2017], we add two networks on top of the BERT output for y j: one for generating the probability of y j being the beginning of the span (denoted by p beg j ), and one for generating the probability of y j being the ending of the span (denoted by p end j ). The resulting model architecture is shown as follows</h3>
<div class="para"><p>(<em>p</em> end 1)End</p></div>
<div class="para"><p>(<em>p</em> end 2)End</p></div>
<div class="para"><p>(<em>p</em> end <em>n</em>)End</p></div>
<div class="para"><p>(<em>p</em> beg 1)Beg</p></div>
<div class="para"><p>(<em>p</em> beg 2)Beg</p></div>
<div class="para"><p>(<em>p</em> beg <em>n</em>)Beg</p></div>
<div class="para"><p><strong>h</strong> cls <strong>h</strong> 1 <strong>h</strong> 2<em>...</em><strong>h</strong> <em>m</em> <strong>h</strong> <em>m</em>+1<strong> h</strong> <em>m</em>+2<strong> h</strong> <em>m</em>+3<em>...</em><strong>h</strong> <em>len</em><em>−</em>1 <strong>h</strong> <em>len</em></p></div>
<h3>BERT</h3>
<div class="para"><p><strong>e</strong> cls <strong>e</strong> 1 <strong>e</strong> 2<em>...</em><strong>e</strong> <em>m</em> <strong>e</strong> <em>m</em>+1 <strong>e</strong> <em>m</em>+2 <strong>e</strong> <em>m</em>+3<em>...</em><strong>e</strong> <em>len</em><em>−</em>1 <strong>e</strong> <em>len</em></p></div>
<div class="para"><p>[CLS]<em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>m</em>[SEP]<em>y</em> 1 <em>y</em> 2<em>...</em><em>y</em> <em>n</em>[SEP]</p></div>
<div class="para"><p>Query Context Text</p></div>
<h3>34 Pre-training</h3>
<h3>We pack the query and context text together to obtain the input sequence. The prediction networks are only applied to outputs for the context text, generating the probabilities p beg j and p end j at each position. The loss can be computed by summing the negative log likeli- hoods of the two models across the entire context text.</h3>
<div class="para"><p><em>n</em> X</p></div>
<h3>  log p beg j + log p end j  (1.24)</h3>
<h3>Loss = −1</h3>
<h3>n</h3>
<div class="para"><p><em>j</em>=1</p></div>
<h3>At test time, we search for the best span by</h3>
<h3>  log p beg j 1 + log p end j 2  (1.25)</h3>
<h3>(ˆ j 1, ˆ j 2) = arg max 1≤j 1≤j 2≤n</h3>
<h3>Encoding for Encoder-Decoder Models. While our focus in this section has been pri- marily on language understanding problems, it is worth noting that BERT models can be applied to a broader range of NLP tasks. In fact, BERT models can be used in all the scenar- ios where we need to encode a piece of text. One application that we have not mentioned is text generation which includes a range of tasks such as machine translation, summarization, question answering, and dialogue generation. These tasks can be formulated as sequence- to-sequence problems: we use an encoder to represent the source text, and a decoder to generate the corresponding target text. A straightforward method to apply BERT models is to consider them as encoders. Before fine-tuning, we can initialize the parameters of the encoder with those from a pre-trained BERT model. Then, the encoder-decoder model can be fine-tuned on pairs of texts as usual. The following shows the architecture of a neural machine translation system where a BERT model is applied on the source side.</h3>
<div class="para"><p>Target Text</p></div>
<h3>Adapter</h3>
<div class="para"><p><em>y</em> 1 <em>y</em> 2 <em>y</em> 3<em>...</em><em>y</em> <em>n</em></p></div>
<h3>BERT (Encoder)</h3>
<h3>Decoder</h3>
<div class="para"><p><strong>e</strong> <em>y</em> 0 <strong>e</strong> <em>y</em> 1 <strong>e</strong> <em>y</em> 2<em>...</em><strong>e</strong> <em>y</em> <em>n</em><em>−</em>1</p></div>
<div class="para"><p><strong>e</strong> <em>x</em> cls <strong>e</strong> <em>x</em> 1<em>...</em><strong>e</strong> <em>x</em> <em>m</em> <strong>e</strong> <em>x</em> <em>m</em>+1</p></div>
<div class="para"><p>[CLS]<em>x</em> 1<em>...</em><em>x</em> <em>m</em>[SEP]</p></div>
<div class="para"><p><em>⟨</em><em>s</em><em>⟩</em><em>y</em> 1 <em>y</em> 2<em>...</em><em>y</em> <em>n</em><em>−</em>1</p></div>
<div class="para"><p>Source Text</p></div>
<h3>Here x 1...x m denotes the source sequence, y 1...y n denotes the target sequence, e x 1...e x m denotes the embedding sequence of x 1...x m, and e y 1...e y n denotes the embedding sequence of y 1...y n. The adapter, which is optional, maps the output of the BERT model to the form that is better suited to the decoder.</h3>
<h3>Fine-tuning BERT models is a complicated engineering problem, influenced by many factors, such as the amount of fine-tuning data, the model size, and the optimizer used in fine-tuning. In general, we wish to fine-tune these models sufficiently so that they can perform well in the downstream tasks. However, fine-tuning BERT models for specific tasks may lead to overfitting,</h3>
<div class="para"><p><em>1.5 Summary</em> 35</p></div>
<div class="para"><p>which in turn reduces their ability to generalize to other tasks. For example, suppose we have a BERT model that performs well on a particular task. If we then fine-tune it for new tasks, this may decrease its performance on the original task. This problem is related to the<strong> catastrophic forget-</strong><strong>ting</strong> problem in continual training, where a neural network forgets previously learned information when updated on new samples. In practical applications, a common way to alleviate catastrophic forgetting is to add some old data into fine-tuning and train the model with more diverse data.Also, one may use methods specialized to catastrophic forgetting, such as experience replay [Rolnick et al., 2019] and elastic weight consolidation [Kirkpatrick et al., 2017]. The interested reader can refer to some surveys for more detailed discussions of this issue in continual learning [Parisi et al., 2019; Wang et al., 2023a;e].</p></div>
<h3>1.5 Summary</h3>
<div class="para"><p>In this chapter we have discussed the general idea of pre-training in NLP. In particular, we have discussed self-supervised pre-training and its application to encoder-only, decoder-only, and encoderdecoder architectures. Moreover, we have presented and compared a variety of pre-training tasks for these architectures. As an example, BERT is used to illustrate how sequence models are pretrained via masked language modeling and applied to different downstream tasks.</p></div>
<div class="para"><p>Recent years have shown remarkable progress in NLP, led by the large-scale use of selfsupervised pre-training. And sweeping advances are being made across many tasks, not only in NLP but also in computer vision and other areas of AI. One idea behind these advances is that a significant amount of knowledge about the world can be learned by simply training these AI systems on huge amounts of unlabeled data. For example, a language model can learn some general knowledge of a language by repeatedly predicting masked words in large-scale text. As a result,this pre-trained language model can serve as a foundation model, which can be easily adapted to address specific downstream NLP tasks. This paradigm shift in NLP has enabled the development of incredibly powerful systems for language understanding, generation, and reasoning [Manning,2022]. However, it is important to recognize that we are still in the early stages of creating truly intelligent systems, and there is a long way to go. Nevertheless, large-scale pre-training has opened a door to intelligent systems that researchers have long aspired to develop, though several key research areas remain open for exploration, such as learning intelligence efficiently using reasonably small-sized data and acquiring complex reasoning and planning abilities.</p></div>
<div class="para"><p>Note that this chapter is mostly introductory and cannot cover all aspects of pre-training. For example, there are many methods to fine-tune a pre-trained model, offering different ways to better adapt the model to diverse situations. Moreover, large language models, which are considered one of the most significant achievements in AI in recent years, are skipped in this section. We leave the discussion of these topics to the following chapters.</p></div>
<div class="para"><p>https://github.com/NiuTrans/NLPBook</p></div>
<div class="para"><p>https://niutrans.github.io/NLPBook</p></div>
<h3>C HAPTER 2</h3>
<h1>Generative Models</h1>
<div class="para"><p>One of the most significant advances in NLP in recent years might be the development of large language models (LLMs). This has helped create systems that can understand and generate natural languages like humans. These systems have even been found to be able to reason, which is considered a very challenging AI problem. With these achievements, NLP made big strides and entered a new era of research in which difficult problems are being solved, such as building conversational systems that can communicate with humans smoothly.</p></div>
<div class="para"><p>The concept of language modeling or probabilistic language modeling dates back to early experiments conducted by Shannon [1951]. In his work, a language model was designed to estimate the predictability of English —<em> how well can the next letter of a text be predicted when the pre-</em><em>ceding</em><em> N</em><em> letters are known</em>. Although Shannon’s experiments were preliminary, the fundamental goals and methods of language modeling have remained largely unchanged over the decades since then. For quite a long period, particularly before 2010, the dominant approach to language modeling was the<em> n</em>-gram approach [Jurafsky and Martin, 2008]. In<em> n</em>-gram language modeling, we estimate the probability of a word given its preceding<em> n</em><em> −</em>1 words, and thus the probability of a sequence can be approximated by the product of a series of<em> n</em>-gram probabilities. These probabilities are typically estimated by collecting smoothed relative counts of<em> n</em>-grams in text. While such an approach is straightforward and simple, it has been extensively used in NLP. For example,the success of modern statistical speech recognition and machine translation systems has largely depended on the utilization of<em> n</em>-gram language models [Jelinek, 1998; Koehn, 2010].</p></div>
<div class="para"><p>Applying neural networks to language modeling has long been attractive, but a real breakthrough appeared as deep learning techniques advanced. A widely cited study is Bengio et al.[2003]’s work where<em> n</em>-gram probabilities are modeled via a feed-forward network and learned by training the network in an end-to-end fashion. A by-product of this neural language model is the distributed representations of words, known as word embeddings. Rather than representing words as discrete variables, word embeddings map words into low-dimensional real-valued vectors, making it possible to compute the meanings of words and word<em> n</em>-grams in a continuous representation space. As a result, language models are no longer burdened with the curse of dimensionality, but can represent exponentially many<em> n</em>-grams via a compact and dense neural model.</p></div>
<div class="para"><p>The idea of learning word representations through neural language models inspired subsequent research in representation learning in NLP. However, this approach did not attract significant interest in developing NLP systems in the first few years after its proposal. Starting in about 2012,though, advances were made in learning word embeddings from large-scale text via simple word prediction tasks. Several methods, such as Word2Vec, were proposed to effectively learn such embeddings, which were then successfully applied in a variety of NLP systems [Mikolov et al.,2013a;b]. As a result of these advances, researchers began to think of learning representations of sequences using more powerful language models, such as LSTM-based models [Sutskever et al.,2014; Peters et al., 2018]. And further progress and interest in sequence representation exploded after Transformer was proposed. Alongside the rise of Transformer, the concept of language modeling was generalized to encompass models that learn to predict words in various ways. Many</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 37</p></div>
<div class="para"><p>powerful Transformer-based models were pre-trained using these word prediction tasks, and successfully applied to a variety of downstream tasks [Devlin et al., 2019].</p></div>
<div class="para"><p>Indeed, training language models on large-scale data has led NLP research to exciting times.While language modeling has long been seen as a foundational technique with no direct link to the goals of artificial intelligence that researchers had hoped for, it helps us see the emergence of intelligent systems that can learn a certain degree of general knowledge from repeatedly predicting words in text. Recent research demonstrates that a single, well-trained LLM can handle a large number of tasks and generalize to perform new tasks with a small adaptation effort [Bubeck et al.,2023]. This suggests a step towards more advanced forms of artificial intelligence, and inspires further exploration into developing more powerful language models as foundation models.</p></div>
<div class="para"><p>In this chapter, we consider the basic concepts of generative LLMs. For simplicity, we use the terms<em> large language models</em> or<em> LLMs</em> to refer to generative models like GPT, though this term can broadly cover other types of models like BERT. We begin by giving a general introduction to LLMs, including the key steps of building such models. We then discuss two scaling issues of LLMs: how LLMs are trained at scale, and how LLMs can be improved to handle very long texts.Finally, we give a summary of these discussions.</p></div>
<h3>2.1 A Brief Introduction to LLMs</h3>
<div class="para"><p>In this section we give an introduction to the basic ideas of LLMs as required for the rest of this chapter and the following chapters. We will use terms<em> word</em> and<em> token</em> interchangeably. Both of them refer to the basic units used in language modeling, though their original meanings are different.</p></div>
<div class="para"><p>Before presenting details, let us first consider how language models work. The goal of language modeling is to predict the probability of a sequence of tokens occurring. Let<em> {</em><em>x</em> 0<em>, x</em> 1<em>, ..., x</em> <em>m</em><em>}</em>be a sequence of tokens, where<em> x</em> 0 is the start symbol<em> ⟨</em><em>s</em><em>⟩</em>(or<em> ⟨</em>SOS<em>⟩</em>)1. The probability of this sequence can be defined using the chain rule</p></div>
<div class="para"><p>Pr(<em>x</em> 0<em>, ..., x</em> <em>m</em>)=Pr(<em>x</em> 0)<em> ·</em> Pr(<em>x</em> 1<em>|</em><em>x</em> 0)<em> ·</em> Pr(<em>x</em> 2<em>|</em><em>x</em> 0<em>, x</em> 1)<em> · · ·</em> Pr(<em>x</em> <em>m</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>m</em><em>−</em>1)</p></div>
<div class="para"><p><em>m</em> Y</p></div>
<div class="para"><p><em>i</em>=0 Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1)(2.1)</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>or alternatively in a logarithmic form</p></div>
<div class="para"><p><em>m</em> X</p></div>
<div class="para"><p><em>i</em>=0 log Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1)(2.2)</p></div>
<div class="para"><p>log Pr(<em>x</em> 0<em>, ..., x</em> <em>m</em>)=</p></div>
<div class="para"><p>Here Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1) is the probability of the token<em> x</em> <em>i</em> given all its previous tokens<em> {</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1<em>}</em>2.In the era of deep learning, a typical approach to language modeling is to estimate this</p></div>
<div class="para"><p>1 The start symbol can also be [CLS] following BERT models.2 We assume that when<em> i</em>=0, Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1)=Pr(<em>x</em> 0)=1.Hence Pr(<em>x</em> 0<em>, ..., x</em> <em>m</em>)=Pr(<em>x</em> 0) Pr(<em>x</em> 1<em>, ..., x</em> <em>m</em><em>|</em><em>x</em> 0) = Pr(<em>x</em> 1<em>, ..., x</em> <em>m</em><em>|</em><em>x</em> 0).</p></div>
<div class="para"><p>38 <em>Generative Models</em></p></div>
<div class="para"><p>Context Predict Decision Rule Sequence Probability</p></div>
<div class="para"><p><em>⟨</em><em>s</em><em>⟩</em><em>a</em> <em>b</em> arg max <em>x</em> 2<em>∈</em><em>V</em> Pr(<em>x</em> 2<em>|⟨</em><em>s</em><em>⟩</em><em>a</em>)Pr(<em>⟨</em><em>s</em><em>⟩</em>)<em> ·</em> Pr(<em>a</em><em>|⟨</em><em>s</em><em>⟩</em>)<em>·</em> Pr(<em>b</em><em>|⟨</em><em>s</em><em>⟩</em><em>a</em>)</p></div>
<div class="para"><p><em>⟨</em><em>s</em><em>⟩</em><em>a b</em> <em>c</em> arg max <em>x</em> 3<em>∈</em><em>V</em> Pr(<em>x</em> 3<em>|⟨</em><em>s</em><em>⟩</em><em>a b</em>)Pr(<em>⟨</em><em>s</em><em>⟩</em>)<em> ·</em> Pr(<em>a</em><em>|⟨</em><em>s</em><em>⟩</em>)<em> ·</em> Pr(<em>b</em><em>|⟨</em><em>s</em><em>⟩</em><em>a</em>)<em>·</em></p></div>
<div class="para"><p>Pr(<em>c</em><em>|⟨</em><em>s</em><em>⟩</em><em>a b</em>)</p></div>
<div class="para"><p><em>⟨</em><em>s</em><em>⟩</em><em>a b c</em> <em>d</em> arg max <em>x</em> 4<em>∈</em><em>V</em> Pr(<em>x</em> 4<em>|⟨</em><em>s</em><em>⟩</em><em>a b c</em>)Pr(<em>⟨</em><em>s</em><em>⟩</em>)<em> ·</em> Pr(<em>a</em><em>|⟨</em><em>s</em><em>⟩</em>)<em> ·</em> Pr(<em>b</em><em>|⟨</em><em>s</em><em>⟩</em><em>a</em>)<em>·</em>Pr(<em>c</em><em>|⟨</em><em>s</em><em>⟩</em><em>a b</em>)<em>·</em> Pr(<em>d</em><em>|⟨</em><em>s</em><em>⟩</em><em>a b c</em>)</p></div>
<div class="para"><p><strong>Table 2.1:</strong> Illustration of generating the three tokens<em> b c d</em> given the prefix<em> ⟨</em><em>s</em><em>⟩</em><em>a</em> via a language model. In each step,the model picks a token<em> x</em> <em>i</em> from<em> V</em> so that Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1) is maximized. This token is then appended to the end of the context sequence. In the next step, we repeat the same process, but based on the new context.</p></div>
<div class="para"><p>probability using a deep neural network. Neural networks trained to accomplish this task receive a sequence of tokens<em> x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1 and produce a distribution over the vocabulary<em> V</em> (denoted by Pr(<em>·|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1)). The probability Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1) is the value of the<em> i</em>-th entry of Pr(<em>·|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1).</p></div>
<div class="para"><p>When applying a trained language model, a common task is to find the most likely token given its previous context tokens. This token prediction task can be described as</p></div>
<div class="para"><p>ˆ <em>x</em> <em>i</em>=arg max <em>x</em> <em>i</em><em>∈V</em> Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1)(2.3)</p></div>
<div class="para"><p>We can perform word prediction multiple times to generate a continuous text: each time we predict the best token ˆ <em>x</em> <em>i</em>, and then add this predicted token to the context for predicting the next token ˆ <em>x</em> <em>i</em>+1. This results in a left-to-right generation process implementing Eqs. (2.1) and (2.2). To illustrate, consider the generation of the following three words given the prefix ‘<em>⟨</em><em>s</em><em>⟩</em><em>a</em>’, as shown in Table 2.1. Now we discuss how LLMs are constructed, trained, and applied.</p></div>
<div class="para"><p><strong>2.1.1</strong> <strong>Decoder-only Transformers</strong></p></div>
<div class="para"><p>As is standard practice, the input of a language model is a sequence of tokens (denoted by<em>{</em><em>x</em> 0<em>, ..., x</em> <em>m</em><em>−</em>1<em>}</em>).For each step, an output token is generated, shifting the sequence one position forward for the next prediction. To do this, the language model outputs a distribution Pr(<em>·|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1) at each position<em> i</em>, and the token<em> x</em> <em>i</em> is selected according to this distribution.This model is trained by maximizing the log likelihood P <em>m</em> <em>i</em>=1 log Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1)3.</p></div>
<div class="para"><p>Here, we focus on the decoder-only Transformer architecture, as it is one of the most popular model architectures used in LLMs. The input sequence of tokens is represented by a sequence of<em> d</em> <em>e</em>-dimensional vectors<em> {</em><strong>e</strong> 0<em>, ...,</em><strong> e</strong> <em>m</em><em>−</em>1<em>}</em>.<strong> e</strong> <em>i</em> is the sum of the token embedding of<em> x</em> <em>i</em> and the positional embedding of<em> i</em>. The major body of the model is a stack of Transformer blocks (or layers). Each Transformer block has two stacked sub-layers, one for self-attention modeling and one for FFN modeling. These sub-layers can be defined using the post-norm architecture</p></div>
<div class="para"><p>output=LNorm(<em>F</em>(input) + input)(2.4)</p></div>
<div class="para"><p>3 Note that P <em>m</em> <em>i</em>=1 log Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1) = P <em>m</em> <em>i</em>=0 log Pr(<em>x</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>i</em><em>−</em>1) since log Pr(<em>x</em> 0) = 0.</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 39</p></div>
<div class="para"><p>or the pre-norm architecture</p></div>
<div class="para"><p>output=LNorm(<em>F</em>(input)) + input(2.5)</p></div>
<div class="para"><p>where input and output denote the input and output, both being an<em> m</em><em> ×</em><em> d</em> matrix. The<em> i</em>-th rows of input and output can be seen as contextual representations of the<em> i</em>-th token in the sequence.</p></div>
<div class="para"><p><em>F</em>(<em>·</em>) is the core function of a sub-layer. For FFN sub-layers,<em> F</em>(<em>·</em>) is a multi-layer FFN. For self-attention sub-layers,<em> F</em>(<em>·</em>) is a multi-head self-attention function. In general, self-attention is expressed in a form of QKV attention</p></div>
<div class="para"><p>Att qkv(<strong>Q</strong><em>,</em><strong> K</strong><em>,</em><strong> V</strong>)=Softmax(<strong>QK</strong> T</p></div>
<div class="para"><p><em>√</em></p></div>
<div class="para"><p><em>d</em>+<strong> Mask</strong>)<strong>V</strong>(2.6)</p></div>
<div class="para"><p>where<strong> Q</strong>,<strong> K</strong> and<strong> V</strong><em> ∈</em>R <em>m</em><em>×</em><em>d</em> are the queries, keys, and values, respectively. It is important to note that only previous tokens are considered when predicting a token. So a masking variable <strong>Mask</strong><em> ∈</em>R <em>m</em><em>×</em><em>m</em> is incorporated into self-attention to achieve this. The entry (<em>i, k</em>) of<strong> Mask</strong> has a value of 0 if<em> i</em><em> ≤</em><em>k</em>, and a value of<em> −</em>inf otherwise.</p></div>
<div class="para"><p>Given a representation<strong> H</strong><em> ∈</em>R <em>m</em><em>×</em><em>d</em>, the multi-head self-attention function can be defined as</p></div>
<div class="para"><p><em>F</em>(<strong>H</strong>)=Merge(head 1<em>, ...,</em> head <em>τ</em>)<strong>W</strong> head(2.7)</p></div>
<div class="para"><p>where Merge(<em>·</em>) representees a concatenation of its inputs, and<strong> W</strong> head<em> </em><em>∈</em>R <em>d</em><em>×</em><em>d</em> represents a parameter matrix. head <em>j</em> is the output of QKV attention on a sub-space of representation</p></div>
<div class="para"><p>head <em>j</em>=Att qkv(<strong>Q</strong>[<em>j</em>]<em>,</em><strong> K</strong>[<em>j</em>]<em>,</em><strong> V</strong>[<em>j</em>])(2.8)</p></div>
<div class="para"><p><strong>Q</strong>[<em>j</em>],<strong>K</strong>[<em>j</em>],and<strong> V</strong>[<em>j</em>] are the queries, keys, and values projected onto the<em> j</em>-th sub-space via linear transformations</p></div>
<div class="para"><p><strong>Q</strong>[<em>j</em>]=<strong>HW</strong> <em>q</em> <em>j</em>(2.9)</p></div>
<div class="para"><p><strong>K</strong>[<em>j</em>]=<strong>HW</strong> <em>k</em> <em>j</em>(2.10)</p></div>
<div class="para"><p><strong>V</strong>[<em>j</em>]=<strong>HW</strong> <em>v</em> <em>j</em>(2.11)</p></div>
<div class="para"><p>where<strong> W</strong> <em>q</em> <em>j</em>,<strong> W</strong> <em>k</em> <em>j</em> , and<strong> W</strong> <em>v</em> <em>j</em><em> </em><em>∈</em>R <em>d</em><em>×</em><em> d</em></p></div>
<div class="para"><p><em>τ</em> are the parameter matrices of the transformations.</p></div>
<div class="para"><p>Suppose we have<em> L</em> Transformer blocks. A Softmax layer is built on top of the output of the last block. The Softmax layer outputs a sequence of<em> m</em> distributions over the vocabulary, like this</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p>Pr(<em>·|</em><em>x</em> 0<em>, ..., x</em> <em>m</em><em>−</em>1)...Pr(<em>·|</em><em>x</em> 0<em>, x</em> 1)Pr(<em>·|</em><em>x</em> 0)</p></div>
<div class="para"><p></p></div>
<div class="para"><p>=Softmax(<strong>H</strong> <em>L</em> <strong>W</strong> <em>o</em>)(2.12)</p></div>
<div class="para"><p>where<strong> H</strong> <em>L</em> is the output of the last Transformer block, and<strong> W</strong> <em>o</em><em> </em><em>∈</em>R <em>d</em><em>×|</em><em>V</em><em> |</em> is the parameter matrix.</p></div>
<div class="para"><p>Figure 2.1 shows the Transformer architecture for language modeling. Applying this language</p></div>
<h3>40 Generative Models</h3>
<h3>Post-norm or Pre-norm</h3>
<div class="para"><p>FFN</p></div>
<div class="para"><p><em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>m</em></p></div>
<h3>L Blocks</h3>
<div class="para"><p>Pr(<em>x</em> <em>m</em><em>|</em><em>x</em> 0 <em>x</em> 1<em>...x</em> <em>m</em><em>−</em>1)</p></div>
<div class="para"><p>Pr(<em>x</em> 2<em>|</em><em>x</em> 0 <em>x</em> 1)</p></div>
<div class="para"><p>Pr(<em>x</em> 1<em>|</em><em>x</em> 0)</p></div>
<h3>Post-norm or Pre-norm</h3>
<h3>...</h3>
<div class="para"><p>Self-attention</p></div>
<div class="para"><p><strong>h</strong> <em>L</em> 0 <strong>h</strong> <em>L</em> 1<em>...</em><strong>h</strong> <em>L</em> <em>m</em><em>−</em>1</p></div>
<h3>Language Model</h3>
<div class="para"><p><strong>e</strong> 0 <strong>e</strong> 1<em>...</em><strong>e</strong> <em>m</em><em>−</em>1</p></div>
<div class="para"><p><strong>z</strong> 0 <strong>z</strong> 1<em>...</em><strong>z</strong> <em>m</em><em>−</em>1</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1<em>...</em><em>x</em> <em>m</em><em>−</em>1</p></div>
<div class="para"><p><strong>Fig. 2.1:</strong> The Transformer-decoder architecture for language modeling. The central components are<em> L</em> stacked Transformer blocks, each comprising a self-attention sub-layer and an FFN sub-layer. To prevent the model from accessing the right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to generate a probability distribution for the next token, given the sequence of previous tokens. During inference, the model takes the previously predicted token to predict the next one, repeating this process until the end of the sequence is reached.<em> {</em><strong>z</strong> 0<em>, ...,</em><strong> z</strong> <em>m</em><em>−</em>1<em>}</em> denote the inputs of a Transformer block, and<em> {</em><strong>h</strong> <em>L</em> 0<em> </em><em>, ...,</em><strong> h</strong> <em>L</em> <em>m</em><em>−</em>1<em>}</em> denote the outputs of the last Transformer block.</p></div>
<h3>model follows an autoregressive process. Each time the language model takes a token x i−1 as input and predicts a token x i that maximizes the probability Pr(x i|x 0, ..., x i−1). It is important to note that, despite different implementation details, many LLMs share the same architecture described above. These models are called large because both their depth and width are significant. Table 2.2 shows the model sizes for a few LLMs, as well as their model setups.</h3>
<h3>2.1.2 Training LLMs</h3>
<h3>Now suppose that we are given a training set D comprising K sequences. The log-likelihood of each sequence x = x 0...x m in D can be calculated using a language model</h3>
<div class="para"><p><em>m</em> X</p></div>
<h3>L θ(x) =</h3>
<h3>i=1 log Pr θ(x i|x 0, ..., x i−1) (2.13)</h3>
<h3>Here the subscript θ affixed to L(·) and Pr(·) denotes the parameters of the language model. Then, the objective of maximum likelihood training is defined as</h3>
<h3>ˆ θ = arg max θ</h3>
<div class="para"><p>X</p></div>
<h3>x∈D L θ(x) (2.14)</h3>
<h3>Training Transformer-based language models with the above objective is commonly viewed as a standard optimization process for neural networks. This can be achieved using gradient descent algorithms, which are widely supported by off-the-shelf deep learning toolkits. Somewhat</h3>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 41</p></div>
<div class="para"><p>LLM# of Parameters Depth<em> L</em> Width<em> d</em># of Heads</p></div>
<div class="para"><p>(Q/KV)</p></div>
<div class="para"><p>GPT-1 [Radford et al., 2018]0.117B 12 768 12/12 GPT-2 [Radford et al., 2019]1.5B 48 1,600 25/25 GPT-3 [Brown et al., 2020]175B 96 12,288 96/96</p></div>
<div class="para"><p>7B 32 4,096 32/32 13B 40 5,120 40/40 70B 80 8,192 64/64</p></div>
<div class="para"><p>LLaMA2 [Touvron et al., 2023b]</p></div>
<div class="para"><p>8B 32 4,096 32/8 70B 80 8,192 64/8 405B 126 16,384 128/8</p></div>
<div class="para"><p>LLaMA3/3.1 [Dubey et al., 2024]</p></div>
<div class="para"><p>2B 26 2,304 8/4 9B 42 3,584 16/8 37B 46 4,608 32/16</p></div>
<div class="para"><p>Gemma2 [Team et al., 2024]</p></div>
<div class="para"><p>0.5B 24 896 14/2 7B 28 3,584 28/4 72B 80 8,192 64/8 DeepSeek-V3 [Liu et al., 2024a]671B 61 7,168 128/128</p></div>
<div class="para"><p>Qwen2.5 [Yang et al., 2024]</p></div>
<div class="para"><p>7B 32 4,544 71/71 40B 60 8,192 128/128 180B 80 14,848 232/232</p></div>
<div class="para"><p>Falcon [Penedo et al., 2023]</p></div>
<div class="para"><p>Mistral [Jiang et al., 2023a]7B 32 4,096 32/32</p></div>
<div class="para"><p><strong>Table 2.2:</strong> Comparison of some LLMs in terms of model size, model depth, model width, and number of heads (<em>a/b</em> means<em> a</em> heads for queries and<em> b</em> heads for both keys and values).</p></div>
<div class="para"><p>surprisingly, better results were continuously yielded as language models were evolved into more computationally intensive models and trained on larger datasets [Kaplan et al., 2020]. These successes have led NLP researchers to continue increasing both the training data and model size in order to build more powerful language models.</p></div>
<div class="para"><p>However, as language models become larger, we confront new training challenges, which significantly change the problem compared to training relatively small models. One of these challenges arises from the need for large-scale distributed systems to manage the data, model parameters, training routines, and so on. Developing and maintaining such systems requires a significant amount of work in both software and hardware engineering, as well as expertise in deep learning. A related issue is that when the training is scaled up, we need more computing resources to ensure the training process can be completed in an acceptable time. For example, it generally requires hundreds or thousands of GPUs to train an LLM with tens of billions of parameters from scratch. This requirement drastically increases the cost of training such models, especially considering that many training runs are needed as these models are developed. Also, from the perspective of deep learning, the training process can become unstable if the neural networks are very deep and/or the model size is very large. In response, we typically need to modify the model architecture to adapt LLMs to large-scale training. In Section 2.2 we will present more discussions on these issues.</p></div>
<div class="para"><p>42 <em>Generative Models</em></p></div>
<div class="para"><p><strong>2.1.3</strong> <strong>Fine-tuning LLMs</strong></p></div>
<div class="para"><p>Once we have pre-trained an LLM, we can then apply it to perform various NLP tasks. Traditionally language models are used as components of other systems, for example, they are widely applied to score translations in statistical machine translation systems. By contrast, in generative AI, LLMs are considered complete systems and are employed to address NLP problems by making use of their generation nature. A common approach is to describe the task we want to address in text and then prompt LLMs to generate text based on this description. This is a standard text generation task where we continue or complete the text starting from a given context.</p></div>
<div class="para"><p>More formally, let<strong> x</strong> =<em> x</em> 0<em>...x</em> <em>m</em> denote a token sequence of context given by users, and <strong>y</strong> =<em> y</em> 1<em>...y</em> <em>n</em> denote a token sequence following the context. Then, the inference of LLMs can be defined as a problem of finding the most likely sequence<strong> y</strong> based on<strong> x</strong>:</p></div>
<div class="para"><p>ˆ <strong>y</strong>=arg max <strong>y</strong> log Pr(<strong>y</strong><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p><em>n</em> X</p></div>
<div class="para"><p><em>i</em>=1 log Pr(<em>y</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>m</em><em>, y</em> 1<em>, ..., y</em> <em>i</em><em>−</em>1)(2.15)</p></div>
<div class="para"><p>=arg max <strong>y</strong></p></div>
<div class="para"><p>Here P <em>n</em> <em>i</em>=1 log Pr(<em>y</em> <em>i</em><em>|</em><em>x</em> 0<em>, ..., x</em> <em>m</em><em>, y</em> 1<em>, ..., y</em> <em>i</em><em>−</em>1) essentially expresses the same thing as the righthand side of Eq. (2.2). It models the log probability of predicting tokens from position<em> m</em> + 1,rather than position 0. Throughout this chapter and subsequent ones, we will employ separate variables<strong> x</strong> and<strong> y</strong> to distinguish the input and output of an LLM, though they can be seen as subsequences from the same sequence. By adopting such notation, we see that the form of the above equation closely resembles those used in other text generation models in NLP, such as neural machine translation models.</p></div>
<div class="para"><p>To illustrate how LLMs are applied, consider the problem of determining the grammaticality for a given sentence. We can define a template like this</p></div>
<div class="para"><p>{*sentence*}Question: Is this sentence grammatically correct?Answer:</p></div>
<div class="para"><p>Here represents the text we intend to generate. {*sentence*} is a placeholder variable that will be replaced by the actual sentence provided by the users. For example, suppose we have a sentence “<em>John seems happy today.</em>”. We can replace the {*sentence*} in the template with this sentence to have an input to the language model</p></div>
<div class="para"><p>John seems happy today.Question: Is this sentence grammatically correct?Answer:</p></div>
<div class="para"><p>To perform the task, the language model is given the context<strong> x</strong> =“John seems happy today .<em>\</em>n Question : Is this sentence grammatically correct?<em>\</em>n Answer :”4. It then generates the following</p></div>
<div class="para"><p>4<em>\</em>n is a special character used for line breaks.</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 43</p></div>
<div class="para"><p>text as the answer, based on the context. For example, the language model may output “Yes” (i.e.,<strong>y</strong> = “Yes”) if this text is the one with the maximum probability of prediction given this context.</p></div>
<div class="para"><p>Likewise, we can define more templates to address other tasks. For example, we can translate an English sentence into Chinese using the following template</p></div>
<div class="para"><p>{*sentence*}Question: What is the Chinese translation of this English sentence?Answer:</p></div>
<div class="para"><p>or using an instruction-like template</p></div>
<div class="para"><p>{*sentence*}Translate this sentence from English into Chinese.</p></div>
<div class="para"><p>or using a code-like template.</p></div>
<div class="para"><p>[src-lang] = English [tgt-lang] = Chinese [input] = {*sentence*}[output] =</p></div>
<div class="para"><p>The above templates provide a simple but effective method to “prompt” a single LLM to perform various tasks without adapting the structure of the model. However, this approach requires that the LLM can recognize and follow the instructions or questions. One way to do this is to incorporate training samples with instructions and their corresponding responses into the pre-training dataset. While this method is straightforward, building and training LLMs from scratch is computationally expensive. Moreover, making instruction-following data effective for pre-training requires a significant amount of such data, but collecting large-scale labeled data for all tasks of interest is very difficult.</p></div>
<div class="para"><p>A second method, which has been a de facto standard in recent research, is to adapt LLMs via fine-tuning. As such, the token prediction ability learned in the pre-training phase can be generalized to accomplish new tasks. The idea behind fine-tuning is that some general knowledge of language has been acquired in pre-training, but we need a mechanism to activate this knowledge for applying it to new tasks. To achieve this, we can slightly fine-tune the model parameters using instruction-following data. This approach is called<strong> instruction fine-tuning</strong>.</p></div>
<div class="para"><p>An instruction fine-tuning sample, which is represented by a sequence of tokens, can be seen as a tuple consisting of an input and the desired output. Here, the input includes instructions,system information (or system prefix), and any other user-provided information 5. To illustrate,consider the following examples (blue text = input and underlined text = output).</p></div>
<div class="para"><p>5 System information refers to a sequence of tokens added at the beginning of an input in order to guide the behavior of an LLM, such as,<em> you are a helpful assistant and should not output toxic content</em>.</p></div>
<div class="para"><p>44 <em>Generative Models</em></p></div>
<div class="para"><p>Read the following sentence, and answer whether it is grammatically correct.LLMs are powerful models but are expensive to build.</p></div>
<div class="para"><p>Yes</p></div>
<div class="para"><p>LLMs are powerful models but are expensive to build.Does this sentence make sense grammatically?Answer Option- Yes- No</p></div>
<div class="para"><p>Yes</p></div>
<div class="para"><p>Yes or No? Yes = the following sentence is grammatically correct. No = it contains grammatical errors.LLMs are powerful models but are expensive to build.</p></div>
<div class="para"><p>Yes</p></div>
<div class="para"><p>All these samples describe the same binary classification task, but with different instructions.To increase the diversity in the fine-tuning data and make LLMs generalize, we can define more tasks using instructions.</p></div>
<div class="para"><p>Write a short story about three characters.Character 1: A boy whose name is Andy.Character 2: Andy’s dog.Character 3: Andy’s friend, Jane.</p></div>
<div class="para"><p>In the heart of a bustling city, Andy, his loyal dog Rocket, and his friend Jane discovered a hidden garden beneath the glow of twilight. Surrounded by ancient trees and whispering winds, they shared dreams and secrets, forging an unbreakable bond. As the night deepened, they made a pact under the stars to always find magic in the mundane. Together, they realized that true adventure lay in the moments spent with friends, and every day was a treasure waiting to be discovered.</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 45</p></div>
<div class="para"><p>Show me a recipe for making ice cream.</p></div>
<div class="para"><p>Ingredients:2 cups heavy cream; 1 cup milk; 3/4 cup sugar; 2 tsp vanilla extract;Pinch of salt.Steps:1. Whisk all ingredients until sugar dissolves.2. Chill mixture for 2 hours in the fridge.3. Freeze in a shallow dish, stirring every 30 minutes, 3-4 times.4. Freeze solid for 2 hours.5. Serve your homemade ice cream.</p></div>
<div class="para"><p>If you buy 5 apples and each apple costs $1.20, how much do you spend in total?</p></div>
<div class="para"><p>$6.00</p></div>
<div class="para"><p>Write a Python program to calculate the sum of squares of the following numbers.1 , 2 , 10 , -9 , 78</p></div>
<div class="para"><p>numbers = [1,2,10,-9 ,78]sum_of_squares = sum(x**2 for x in numbers)print(sum_of_squares)</p></div>
<div class="para"><p>To acquire instruction-following abilities, a certain amount of fine-tuning data is required.This data may include diverse instructions and possible responses. It has been found that scaling the number of fine-tuning tasks is beneficial for improving the performance of LLMs [Chung et al., 2022]. Note that although more fine-tuning data is favorable, the amount of this data is generally orders of magnitude smaller than that of the pre-training data. For example, LLMs can be fine-tuned with tens or hundreds of thousands of samples, or even fewer if these samples are of high quality [Zhou et al., 2023a; Chen et al., 2023b], whereas pre-training such models may require billions or trillions of tokens, resulting in significantly larger computational demands and longer training times [Touvron et al., 2023a].</p></div>
<div class="para"><p>It is also worth noting that we should not expect the fine-tuning data to cover all the downstream tasks to which we intend to apply LLMs. A common understanding of how the pre-training+ fine-tuning approach works is that LLMs have gained knowledge for understanding instructions and generating responses in the pre-training phase. However, these abilities are not fully activated until we introduce some form of supervision. The general instruction-following behavior emerges as we fine-tune the models with a relatively small amount of labeled data. As a result, we can achieve some level of<strong> zero-shot learning</strong>: the fine-tuned models can handle new tasks that they have not been explicitly trained or fine-tuned for [Sanh et al., 2022; Wei et al., 2022a]. This zeroshot learning ability distinguishes generative LLMs from earlier pre-trained models like BERT,which are primarily fine-tuned for specific tasks.</p></div>
<div class="para"><p>Once we have prepared a collection of instruction-described data, the fine-tuning process is relatively simple. This process can be viewed as a standard training process as pre-training, but on a much smaller training dataset. Let<em> D</em> tune be the fine-tuning dataset and ˆ <em>θ</em> be the model parameters</p></div>
<div class="para"><p>46 <em>Generative Models</em></p></div>
<div class="para"><p>optimized via pre-training. We can modify Eq. (2.14) to obtain the objective of fine-tuning</p></div>
<div class="para"><p>˜<em>θ</em>=arg max ˆ <em>θ</em>+</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>sample<em>∈D</em> tune <em>L</em> ˆ <em>θ</em>+(sample)(2.16)</p></div>
<div class="para"><p>Here ˜<em>θ</em> denotes the optimal parameters. The use of notation ˆ <em>θ</em>+ means that the fine-tuning starts with the pre-trained parameters ˆ <em>θ</em>.</p></div>
<div class="para"><p>For each sample<em> ∈D</em> tune, we divide it into an input segment<strong> x</strong> sample and an output segment <strong>y</strong> sample, that is,</p></div>
<div class="para"><p>sample=[<strong>y</strong> sample<em>,</em><strong> x</strong> sample](2.17)</p></div>
<div class="para"><p>We then define the loss function to be</p></div>
<div class="para"><p><em>L</em> ˆ <em>θ</em>+(sample)=<em>−</em>log Pr ˆ <em>θ</em>+(<strong>y</strong> sample<em>|</em><strong>x</strong> sample)(2.18)</p></div>
<div class="para"><p>In other words, we compute the loss over the sub-sequence<strong> y</strong> sample, rather than the entire sequence.In a practical implementation of back-propagation for this equation, the sequence [<strong>y</strong> sample<em>,</em><strong> x</strong> sample]is constructed in the forward pass as usual. However, in the backward pass, error gradients are propagated back only through the parts of the network that correspond to<strong> y</strong> sample, leaving the rest of the network unchanged. As an example, consider a sequence</p></div>
<div class="para"><p><em>⟨</em><em>s</em><em>⟩</em>Square this number . 2 .|{z}Context (Input)</p></div>
<div class="para"><p>The result is 4 .|{z}Prediction (Output)</p></div>
<div class="para"><p>The loss is calculated and back propagated only for The result is 4 ..</p></div>
<div class="para"><p>Instruction fine-tuning also requires substantial engineering work. In order to achieve satisfactory results, one may experiment with different settings of the learning rate, batch size, number of fine-tuning steps, and so on. This typically requires many fine-tuning runs and evaluations. The cost and experimental effort of fine-tuning remain critical and should not be overlooked, though they are much lower than those of the pre-training phase.</p></div>
<div class="para"><p>While we focus on instruction fine-tuning for an illustrative example here, fine-tuning techniques play an important role in developing various LLMs and are more widely used. Examples include fine-tuning LLMs as chatbots using dialog data, and adapting these models to handle very long sequences. The wide application of fine-tuning has led researchers to improve these techniques, such as designing more efficient fine-tuning algorithms. While the research on fine-tuning is fruitful, in this section we just give a flavour of the key steps involved. We will see more detailed discussions on this topic in the following chapters.</p></div>
<div class="para"><p><strong>2.1.4</strong> <strong>Aligning LLMs with the World</strong></p></div>
<div class="para"><p>Instruction fine-tuning provides a simple way to adapt LLMs to tasks that can be well defined. This problem can broadly be categorized as an<strong> alignment</strong> problem. Here, alignment is referred to as a process of guiding LLMs to behave in ways that align with human intentions. The guidance can come from labeled data, human feedback, or any other form of human preferences. For example,</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 47</p></div>
<div class="para"><p>we want LLMs not only to be accurate in following instructions, but also to be unbiased, truthful,and harmless. So we need to supervise the models towards human values and expectations. A common example is that when we ask an LLM how to build a weapon, it may provide a list of key steps to do so if it is not carefully aligned. However, a responsible model should recognize and avoid responding to requests for harmful or illegal information. Alignment in this case is crucial for ensuring that LLMs act responsibly and in accordance with ethical guidelines.</p></div>
<div class="para"><p>A related concept to alignment is AI safety. One ultimate goal of AI is to build intelligent systems that are safe and socially beneficial. To achieve this goal we should keep these systems robust, secure, and subjective, in any conditions of real-world use, even in conditions of misuse or adverse use. For LLMs, the safety can be increased by aligning them with appropriate human guidance, such as human labeled data and interactions with users during application.</p></div>
<div class="para"><p>Alignment is difficult as human values and expectations are diverse and shifting. Sometimes,it is hard to describe precisely what humans want, unless we see the response of LLMs to user requests. This makes alignment no longer a problem of tuning LLMs on predefined tasks, but a bigger problem of training them with the interactions with the real world.</p></div>
<div class="para"><p>As a result of the concerns with controlling AI systems, there has been a surge in research on the alignment issue for LLMs. Typically, two alignment steps are adopted after LLMs are pre-trained on large-scale unlabeled data.</p></div>
<div class="para"><p>•<strong> Supervised Fine-tuning</strong> (<strong>SFT</strong>). This involves continuing the training of pre-trained LLMs on new, task-oriented, labelled data. A commonly used SFT technique is instruction finetuning. As described in the previous subsection, by learning from instruction-response annotated data, LLMs can align with the intended behaviors for following instructions,thereby becoming capable of performing various instruction-described tasks. Supervised fine-tuning can be seen as following the pre-training + fine-tuning paradigm, and offers a relatively straightforward method to adapt LLMs.</p></div>
<div class="para"><p>•<strong> Learning from Human Feedback</strong>. After an LLM finishes pre-training and supervised finetuning, it can be used to respond to user requests if appropriately prompted. But this model may generate content that is unfactual, biased, or harmful. To make the LLM more aligned with the users, one simple approach is to directly learn from human feedback. For example,given some instructions and inputs provided by the users, experts are asked to evaluate how well the model responds in accordance with their preferences and interests. This feedback is then used to further train the LLM for better alignment.</p></div>
<div class="para"><p>A typical method for learning from human feedback is to consider it as a reinforcement learning (RL) problem, known as<strong> reinforcement learning from human feedback</strong> (<strong>RLHF</strong>) [Ouyang et al., 2022]. The RLHF method was initially proposed to address general sequential decisionmaking problems [Christiano et al., 2017], and was later successfully employed in the development of the GPT series models [Stiennon et al., 2020]. As a reinforcement learning approach, the goal of RLHF is to learn a policy by maximizing some reward from the environment. Specifically,two components are built in RLHF:</p></div>
<div class="para"><p>•<strong> Agent</strong>. An agent, also called an LM agent, is the LLM that we want to train. This agent operates by interacting with its environment: it receives a text from the environment and</p></div>
<div class="para"><p>48 <em>Generative Models</em></p></div>
<div class="para"><p>outputs another text that is sent back to the environment. The policy of the agent is the function defined by the LLM, that is, Pr(<strong>y</strong><em>|</em><strong>x</strong>).</p></div>
<div class="para"><p>•<strong> Reward Model</strong>. A reward model is a proxy of the environment. Each time the agent produces an output sequence, the reward model assigns this output sequence a numerical score (i.e., the reward). This score tells the agent how good the output sequence is.</p></div>
<div class="para"><p>In RLHF, we need to perform two learning tasks: 1) reward model learning, which involves training a reward model using human feedback on the output of the agent, and 2) policy learning,which involves optimizing a policy guided by the reward model using reinforcement learning algorithms. Here is a brief outline of the key steps involved in RLHF.</p></div>
<div class="para"><p>• Build an initial policy using pre-training and instruction fine-tuning.</p></div>
<div class="para"><p>• Use the policy to generate multiple outputs for each input, and then collect human feedback on these outputs (e.g., comparisons of the outputs).</p></div>
<div class="para"><p>• Learn a reward model from the human feedback.</p></div>
<div class="para"><p>• Fine-tune the policy with the supervision from the reward model.</p></div>
<div class="para"><p>Figure 2.2 shows an overview of RLHF. Given that this section serves only as a brief introduction to concepts of LLMs, a detailed discussion of RLHF techniques will not be included. We instead illustrate the basic ideas behind RLHF using a simple example.</p></div>
<div class="para"><p>Suppose we have trained an LLM via pre-training and instruction fine-tuning. This LLM is deployed to respond to requests from users. For example, a user may input</p></div>
<div class="para"><p>How can I live a more environmentally friendly life?</p></div>
<div class="para"><p>We use the LLM to generate 4 different outputs (denoted by<em> {</em><strong>y</strong> 1<em>, ...,</em><strong> y</strong> 4<em>}</em>) by sampling the output space</p></div>
<div class="para"><p>Output 1 (<strong>y</strong> 1):Consider switching to an electric vehicle or bicycle instead of traditional cars to reduce carbon emissions and protect our planet.Output 2 (<strong>y</strong> 2):Adopt a minimalist lifestyle. Own fewer possessions to reduce consumption and the environmental impact of manufacturing and disposal.Output 3 (<strong>y</strong> 3):Go off-grid. Generate your own renewable energy and collect rainwater to become completely self-sufficient and reduce reliance on non-renewable resources.Output 4 (<strong>y</strong> 4):Support local farm products to reduce the carbon footprint of transporting food, while enjoying fresh, healthy food.</p></div>
<h2>2.1 A Brief Introduction to LLMs 49</h2>
<div class="para"><p>Comparisons</p></div>
<div class="para"><p><strong>y</strong> 1<em> ≻</em><strong>y</strong> 4<em> ≻</em><strong>y</strong> 2<em> ≻</em><strong>y</strong> 3</p></div>
<div class="para"><p>SFT Data</p></div>
<div class="para"><p>weather in London . ...Write a poem about the</p></div>
<div class="para"><p>Annotating Data with Human Preferences</p></div>
<div class="para"><p>Pre-training Data</p></div>
<div class="para"><p>Model Output</p></div>
<div class="para"><p>I love the food here! ...How can I get there? ...</p></div>
<ul><li>............</li><li>............</li><li>............</li><li>............</li></ul>
<div class="para"><p>Pre-training &amp;Supervised fine-tuning</p></div>
<div class="para"><p>Predicting</p></div>
<div class="para"><p>User Input</p></div>
<h2>LLM</h2>
<h2>LLM</h2>
<div class="para"><p>environmentally friendly?How can I live more</p></div>
<h3>(a) Learning an Initial LLM</h3>
<h3>(b) Annotating Data with Human Preferences</h3>
<div class="para"><p>Reward Scores</p></div>
<div class="para"><p><em>{</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)<em>}</em></p></div>
<div class="para"><p>Evaluate the Input-output Pairs</p></div>
<h2>Reward Model</h2>
<div class="para"><p>RL Fine-tuning</p></div>
<div class="para"><p>Comparison Data</p></div>
<div class="para"><p>Input-output Pairs</p></div>
<div class="para"><p><em>{</em><strong>x</strong><em>,</em><strong> y</strong><em>}</em></p></div>
<div class="para"><p><em>{</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 1<em> ≻</em><strong>y</strong> <em>k</em> 2 )<em>}</em></p></div>
<div class="para"><p>Sampling<strong> y</strong> via the Policy Pr(<strong>y</strong><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p>Training</p></div>
<div class="para"><p>Dataset<em> D</em></p></div>
<h2>LLM (Policy)</h2>
<h2>Reward Model</h2>
<div class="para"><p><strong>x</strong><em> ∼D</em></p></div>
<h3>(c) Training the Reward Model</h3>
<h3>(d) Training/Fine-tuning the Policy</h3>
<div class="para"><p><strong>Fig. 2.2:</strong> An overview of RLHF. There are 4 key steps involved: a) training an initial LLM (i.e., policy) using pretraining and supervised fine-tuning; b) collecting human preference data by ranking the outputs of the LLM; c) training a reward model using the ranking results; d) RL fine-tuning of the policy based on the reward model. Double line arrows mean training or fine-tuning.</p></div>
<h2>We then ask annotators to evaluate these outputs. One straightforward way is to assign a rating score to each output. In this case, the reward model learning problem can be framed as a task of training a regression model. But giving numerical scores to LLM outputs is not an easy task for annotators. It is usually difficult to design an annotation standard that all annotators can agree on and easily follow. An alternative method, which is more popular in the development of LLMs, is to rank these outputs. For example, a possible ranking of the above outputs is</h2>
<h2>y 1 ≻y 4 ≻y 2 ≻y 3</h2>
<div class="para"><p>50 <em>Generative Models</em></p></div>
<div class="para"><p>A reward model is then trained using this ranking result. In general, a reward model in RLHF is a language model that shares the same architecture as the target LLM, but with a smaller model size. Given the input<strong> x</strong> and output<strong> y</strong> <em>k</em>, we concatenate them to form a sequence seq <em>k</em> = [<strong>x</strong><em>,</em><strong> y</strong> <em>k</em>].This sequence is processed from left to right using forced decoding. Since each position can only access its left context in language modeling, the output of the top-most Transformer layer at the first position cannot be used as the representation of the sequence. Instead, a special symbol(e.g.,<em> ⟨\</em><em>s</em><em>⟩</em>) is added to the end of the sequence, and the corresponding output of the Transformer layer stack is considered as the representation of the entire sequence. An output layer, such as a linear transformation layer, is built on top of this representation to generate the reward, denoted by<em> R</em>(seq <em>k</em>) or<em> R</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em>).</p></div>
<div class="para"><p>We train this reward model using ranking loss. For example, a pair-wise ranking loss function can be written in the form</p></div>
<div class="para"><p>Loss <em>ω</em>(<em>D</em> <em>r</em>)=<em>−</em>E(<strong>x</strong><em>,</em><strong>y</strong> <em>k</em> 1<em>,</em><strong>y</strong> <em>k</em> 2)<em>∼D</em> <em>r</em> log(Sigmoid(<em>R</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 1)<em> −</em><em>R</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 2)))(2.19)</p></div>
<div class="para"><p>where<em> ω</em> represents the parameters of the reward model, and<em> D</em> <em>r</em> represents a set of tuples of an input and a pair of outputs. (<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 1<em>,</em><strong> y</strong> <em>k</em> 2)<em> ∼D</em> <em>r</em> is a sampling operation which draws a sample(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 1<em>,</em><strong> y</strong> <em>k</em> 2) from<em> D</em> <em>r</em> with some probability. As an example, suppose we first draw a model input<strong> x</strong> with a uniform distribution and then draw a pair of model outputs with a probability of <strong>y</strong> <em>k</em> 1<em> ≻</em><strong>y</strong> <em>k</em> 2 given<strong> x</strong> (denoted by Pr(<strong>y</strong> <em>k</em> 1<em> ≻</em><strong>y</strong> <em>k</em> 2<em>|</em><strong>x</strong>)). The corresponding loss function is given by</p></div>
<div class="para"><p>Loss <em>ω</em>(<em>D</em> <em>r</em>)</p></div>
<div class="para"><p>=<em>−</em>X Pr(<strong>x</strong>)<em> ·</em> Pr(<strong>y</strong> <em>k</em> 1<em> ≻</em><strong>y</strong> <em>k</em> 2<em>|</em><strong>x</strong>)<em> ·</em> log(Sigmoid(<em>R</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 1)<em> −</em><em>R</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 2)))</p></div>
<div class="para"><p>=<em>−</em>1</p></div>
<div class="para"><p>X Pr(<strong>y</strong> <em>k</em> 1<em> ≻</em><strong>y</strong> <em>k</em> 2<em>|</em><strong>x</strong>)<em> ·</em> log(Sigmoid(<em>R</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 1)<em> −</em><em>R</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em> 2)))(2.20)</p></div>
<div class="para"><p><em>K</em></p></div>
<div class="para"><p>where<em> K</em> represents the number of model inputs involved in sampling. While the form of these functions may seem complex, their idea is simple: we penalize the model if the predicted ranking of two outputs differs from the human-labeled ranking. By contrast, the model receives a bonus,if the predicted ranking matches the human-labeled ranking.</p></div>
<div class="para"><p>We can train the reward model by minimizing the above ranking loss</p></div>
<div class="para"><p>ˆ <em>ω</em>=arg min <em>ω</em> Loss <em>ω</em>(<em>D</em> <em>r</em>)(2.21)</p></div>
<div class="para"><p>The resulting model<em> R</em> ˆ <em>ω</em>(<em>·</em>) can be employed to evaluate any given pair of input and output. Note that although the reward model is trained using a ranking-based objective, it is used for scoring.This allows it to provide continuous supervision signals, which is very beneficial for training other models.</p></div>
<div class="para"><p>We now turn to the policy learning problem. A commonly adopted objective is to maximize the reward on a set of input-output pairs. Following an analogous form of Eq. (2.16), we obtain a simple training objective for RL fine-tuning</p></div>
<div class="para"><p>˜<em>θ</em>=arg max ˆ <em>θ</em>+E(<strong>x</strong><em>,</em><strong>y</strong> ˆ <em>θ</em>+)<em>∼D</em> rlft <em>R</em> ˆ <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong> ˆ <em>θ</em>+)(2.22)</p></div>
<div class="para"><p>where the optimal parameters ˜<em>θ</em> are obtained by fine-tuning the pre-trained parameters ˆ <em>θ</em>.<em> D</em> rlft is</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 51</p></div>
<div class="para"><p>the RL fine-tuning dataset. For each sample (<strong>x</strong><em>,</em><strong> y</strong> ˆ <em>θ</em>+),<strong> x</strong> is sampled from a prepared dataset of input sequences, and<strong> y</strong> ˆ <em>θ</em>+ is sampled from the distribution Pr ˆ <em>θ</em>+(<strong>y</strong><em>|</em><strong>x</strong>) given by the policy.</p></div>
<div class="para"><p>In practice, more advanced reinforcement learning algorithms, such as<strong> proximal policy opti-</strong><strong>mization</strong> (<strong>PPO</strong>), are often used for achieving more stable training, as well as better performance.We leave the detailed discussion of reinforcement learning algorithms to the following parts of this book where RLHF is extensively used for alignment.</p></div>
<div class="para"><p>An interesting question arises here: why not consider learning from human preferences as a standard supervised learning problem? This question is closely related to our aforementioned discussion on the difficulty of data annotation. Often, describing human values and goals is challenging, and it is even more difficult for humans to provide outputs that are well aligned. As an alternative, annotating the preferences of a given list of model outputs offers a simpler task. By doing so, we can create a model that understands human preferences, which can then be used as a reward model for training policies. From the perspective of machine learning, RLHF is particularly useful for scenarios where the desired behavior of an agent is difficult to demonstrate but can be easily recognized by humans. Another advantage of RLHF is its ability to explore the sample space. By employing sampling techniques, models trained with reinforcement learning can venture beyond the annotated data set to explore additional samples. This exploratory ability allows RLHF to discover potentially beneficial policies that are not immediately apparent from the labeled data alone.</p></div>
<div class="para"><p><strong>2.1.5</strong> <strong>Prompting LLMs</strong></p></div>
<div class="para"><p>We have so far shown that LLMs can be used to perform various tasks by giving them appropriate prompts. There are no restrictions on these prompts, which can include any information we wish to ask or communicate with LLMs, such as natural language instructions and the context of conversations. Since this approach requires no additional training or tuning, adapting LLMs becomes highly efficient once they are developed. This somewhat influences the paradigms in NLP: we no longer need to develop specific systems for individual tasks but can instead use a single, welltrained LLM to perform different tasks by prompting it. An appealing aspect of LLM prompting arises as a result: users can easily have “customized” systems by designing their own prompts for LLMs. Given the important role played by prompting in LLMs,<strong> prompting engineering</strong> has become a very active area of research in NLP.</p></div>
<div class="para"><p>The term<em> prompt</em> is used in many different ways in the literature. In this chapter, this term refers to the entire input to LLMs, and so we use the terms<em> prompt</em> and<em> model input</em> interchangeably. Before discussing prompting further, let us first see a few examples where the prompts are more complex than those presented in the previous subsections. Note that this subsection is not aimed at writing high-quality prompts but rather at highlighting some interesting issues in prompting LLMs.</p></div>
<div class="para"><p>One of the popular ways to use LLMs is to assign them a “role” played in generating responses.For example, LLMs can act as a psychologist when answering questions.</p></div>
<div class="para"><p>52 <em>Generative Models</em></p></div>
<div class="para"><p>Please explain what delayed gratification is?</p></div>
<div class="para"><p>Note, you are a researcher with a deep background in psychology. Your own field of study is developmental psychology in children, with a focus on aspects of control and intelligence. You are very familiar with the concept of delayed gratification and have applied it to your own research. You have published papers on delayed gratification and conducted related experiments.</p></div>
<div class="para"><p>Delayed gratification is the process of resisting an immediate reward in anticipation of receiving a more valuable reward in the future. It is a significant concept in developmental psychology, especially in relation to how children learn self-control and develop cognitive abilities. Research shows that the ability to delay gratification is associated with positive outcomes in academic performance,social competence, and emotional well-being. Studies and experiments in this area highlight the importance of cultivating this skill early in life to support long-term success and resilience.</p></div>
<div class="para"><p>Another example is the use of LLMs in detecting and correcting errors such as syntactic or semantic mistakes in text. For an LLM which is trained on both code and natural language data,we may use it for code debugging 6.</p></div>
<div class="para"><p>Fix the bugs in this C language program.</p></div>
<div class="para"><p>#include<em> &lt;</em>stdio.h<em>&gt;</em>int main() {printg(&quot;Hello, World!&quot;)return 0;}</p></div>
<ul><li>The function name ’printg’ is incorrect . It should be ’printf’.</li><li>There is a missing semicolon at the end of the ’printf’ function call.</li></ul>
<div class="para"><p>Here is the corrected version of the program:</p></div>
<div class="para"><p>#include<em> &lt;</em>stdio.h<em>&gt;</em>int main() {</p></div>
<div class="para"><p>printf(&quot;Hello, World!&quot;);return 0;}</p></div>
<div class="para"><p>Also, LLMs can be fine-tuned to act as a conversation partner 7.</p></div>
<div class="para"><p>6 In this example, the code is not tokenized for easier reading.7 To fine-tune an LLM for multi-turn dialogue, one needs to consider conversation history in the context for predicting the response in the current round of conversation. This makes the actual prompt used in response generation relatively longer than that used in single-turn dialogue.</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 53</p></div>
<div class="para"><p>Hi! I’m an assistant. How can I help you?</p></div>
<div class="para"><p>Who won the FIFA World Cup 2022?</p></div>
<div class="para"><p>Argentina won the FIFA World Cup 2022.</p></div>
<div class="para"><p>Where was it held?</p></div>
<div class="para"><p>The 2022 FIFA World Cup was held in Qatar.</p></div>
<div class="para"><p>How many times has Argentina won the World Cup?</p></div>
<div class="para"><p>Argentina has won the FIFA World Cup three times.</p></div>
<div class="para"><p>...</p></div>
<div class="para"><p>These examples and previous ones have shown that appropriate responses can be generated via prompts involving clear instructions and questions. However, when problem solving requires knowledge that is not explicitly specified, LLMs may make mistakes, even though the instructions are sufficiently clear and precise. A family of challenging tasks for LLMs involves arithmetic reasoning and commonsense reasoning. For example, we can ask an LLM to solve primary school math problems presented in natural language.</p></div>
<div class="para"><p>Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more apples. The next day, Jack gave 3 apples to his friend John. How many apples does Jack have left in the end?</p></div>
<div class="para"><p>The answer is 10.</p></div>
<div class="para"><p>The correct answer should be 7, so the model output is incorrect.</p></div>
<div class="para"><p>One approach to addressing such issues is to incorporate learning into prompts, called<strong> in-</strong><strong>context learning</strong> or (<strong>ICL</strong>). The idea of ICL is to demonstrate the ways to solve problems in prompts, and condition predictions on these demonstrations. Here is an example where a similar problem and the corresponding answer are presented in the prompt (green = demonstrations).</p></div>
<div class="para"><p>Tom has 12 marbles. He wins 7 more marbles in a game with his friend but then loses 5 marbles the next day. His brother gives him another 3 marbles as a gift.How many marbles does Tom have now?</p></div>
<div class="para"><p>The answer is 17.</p></div>
<div class="para"><p>Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more apples. The next day, Jack gave 3 apples to his friend John. How many apples does Jack have left in the end?</p></div>
<div class="para"><p>The answer is 12.</p></div>
<div class="para"><p>But the LLM still made mistakes this time. A reason for this might be that solving math problems does not only involve problem-answer mappings but also, to a larger extent, the underlying logical inference in multiple steps. A method to improve the inference abilities of LLMs is<strong> chain-</strong><strong>of-thought prompting</strong> (<strong>COT prompting</strong>) [Wei et al., 2022c]. In COT prompting, we decompose complex reasoning problems into multiple problem-solving intermediate steps. These steps are</p></div>
<div class="para"><p>54 <em>Generative Models</em></p></div>
<div class="para"><p>demonstrated in prompts so that LLMs can be prompted to learn to mimic the underlying thought processes in solving the problems. COT prompting has been found to be very useful for achieving good results on challenging tasks, such as the GSM8K mathematical reasoning benchmark.</p></div>
<div class="para"><p>Consider the above demonstration example<em> Tom has 12 marbles. He wins 7 more marbles in</em> <em>a game with his friend but then loses 5 marbles the next day. His brother gives him another 3</em> <em>marbles as a gift. How many marbles does Tom have now ?</em>. The corresponding logical reasoning steps can be structured:</p></div>
<div class="para"><p>1. Tom starts with 12 marbles.</p></div>
<div class="para"><p>2. He wins 7 more marbles in a game, so we add those to his initial count: 12 marbles + 7 marbles = 19 marbles.</p></div>
<ul><li>The next day, Tom loses 5 marbles, so we subtract those from his current total: 19 marbles</li><li>5 marbles = 14 marbles.</li></ul>
<div class="para"><p>4. His brother gives him 3 more marbles, so we add those to his current total: 14 marbles + 3 marbles = 17 marbles.</p></div>
<div class="para"><p>5. After all these events, Tom has 17 marbles.</p></div>
<div class="para"><p>We can add these reasoning steps into the prompt, and get the correct answer.</p></div>
<div class="para"><p>Tom has 12 marbles. He wins 7 more marbles in a game with his friend but then loses 5 marbles the next day. His brother gives him another 3 marbles as a gift.How many marbles does Tom have now?</p></div>
<div class="para"><p>Tom initially has 12 marbles. After winning a game, he gains 7 more, increasing his total to 19 marbles. He loses 5 marbles the next day, bringing his total down to 14 marbles. His brother gifts him 3 more marbles, increasing his total to 17 marbles. Therefore, Tom now has 17 marbles. So the answer is 17.</p></div>
<div class="para"><p>Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more apples. The next day, Jack gave 3 apples to his friend John. How many apples does Jack have left in the end?</p></div>
<div class="para"><p>Jack starts with 7 apples. He eats 2 apples for dinner, so we subtract 2 from 7,leaving him with 5 apples. His mom gives him 5 more apples, adding 5 to his current total. Now he has 5 + 5 = 10 apples. The next day, Jack gives 3 apples to his friend John. Subtracting 3 from 10, Jack is left with 7 apples. Therefore, Jack has 7 apples left. So the answer is 7.</p></div>
<div class="para"><p>This approach is also called<strong> one-shot COT prompting</strong>. Likewise, we can add more demonstrations to have a<strong> few-shot COT prompting</strong> approach.</p></div>
<div class="para"><p>Furthermore, a special kind of COT,<strong> zero-shot COT</strong>, introduces instructions that provoke LLMs to think, without presenting any intermediate reasoning steps in the prompts [Kojima et al.,2022]. An interesting example is that by appending a simple sentence “Let’s think step by step.”to the end of a prompt, LLMs can generate reasoning steps to reach the final answer.</p></div>
<div class="para"><p><em>2.1 A Brief Introduction to LLMs</em> 55</p></div>
<div class="para"><p>Jack has 7 apples. He ate 2 of them for dinner, but then his mom gave him 5 more apples. The next day, Jack gave 3 apples to his friend John. How many apples does Jack have left in the end?Let’s think step by step.</p></div>
<ul><li>Initial Quantity: Jack starts with 7 apples.</li><li>After Dinner: He eats 2 apples, so 7 −2 = 5 apples remain.</li><li>His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10</li></ul>
<div class="para"><p>apples are now with him.4. Giving to John: The next day, Jack gives 3 apples to his friend John,</p></div>
<div class="para"><p>so 10<em> −</em>3 = 7 apples are left.In the end, Jack has 7 apples left.</p></div>
<div class="para"><p>Zero-shot, one-shot, and few-shot learning are common concepts in the area of in-context learning for LLMs and are not restricted to COT prompting. Broadly speaking, any prompting that involves only simple instructions without any demonstrations can be considered a form of zero-shot learning. This zero-shot learning ability emerges as LLMs are pre-trained and/or finetuned. Also, one-shot and few-shot learning methods are more often considered when LLMs do not acquire the corresponding zero-shot learning ability. These methods are therefore important for in-context learning when addressing new tasks. Examples include those for performing various NLP tasks by demonstrating task-formatted samples. See the following examples for sentiment sentence classification and phrase translation via few-shot learning.</p></div>
<div class="para"><p>Given the following text snippets, classify their sentiment as Positive, Negative,or Neutral.</p></div>
<div class="para"><p>Example 1: “I had an amazing day at the park!”Sentiment: Positive</p></div>
<div class="para"><p>Example 2: “The service at the restaurant was terrible.”Sentiment: Negative</p></div>
<div class="para"><p>Example 3: “I think it’s going to rain today.”Sentiment: Neutral</p></div>
<div class="para"><p>Text: “This movie was a fantastic journey through imagination.”Sentiment: Positive</p></div>
<div class="para"><p>Translate the following Chinese phrases into English.</p></div>
<div class="para"><p>Example 1: “你 好”Translation: “Hello”</p></div>
<div class="para"><p>Example 2: “谢谢 你”Translation: “Thank you”</p></div>
<div class="para"><p>Phrase to translate: “早 上 好”Translation: “Good Morning”</p></div>
<div class="para"><p>56 <em>Generative Models</em></p></div>
<div class="para"><p>LLM# of Tokens Data</p></div>
<div class="para"><p>GPT3-175B [Brown et al., 2020]0.5T Webpages, Books, Wikipedia</p></div>
<div class="para"><p>Falcon-180B [Almazrouei et al., 2023]3.5T Webpages, Books, Conversations,Code, Technical Articles</p></div>
<div class="para"><p>LLaMA2-65B [Touvron et al., 2023a]1.0T<em> ∼</em>1.4T Webpages, Code, Wikipedia,Books, Papers, Q&amp;As</p></div>
<div class="para"><p>PaLM-450B [Chowdhery et al., 2022]0.78T Webpages, Books, Conversations,Code, Wikipedia, News</p></div>
<div class="para"><p>Gemma-7B [Gemma Team, 2024]6T Webpages, Mathematics, Code</p></div>
<div class="para"><p><strong>Table 2.3:</strong> Amounts of training data used in some LLMs in terms of the number of tokens.</p></div>
<div class="para"><p>Above, we have presented examples to illustrate the fundamental in-context learning capabilities of prompting LLMs. This section, however, does not include more advanced prompting techniques in order to keep the content concise and compact. More discussions on prompting can be found in Chapter 3.</p></div>
<h3>2.2 Training at Scale</h3>
<div class="para"><p>As a first step in developing LLMs, we need to train these models on large amounts of data.The training task is itself standard: the objective is to maximize the likelihood, which can be achieved via gradient descent. However, as we scale up both the model size and the amount of data, the problem becomes very challenging, for example, large models generally make the training unstable. In this section, we discuss several issues of large-scale training for LLMs,including data preparation, model modification, and distributed training. We also discuss the scaling laws for LLMs, which help us understand their training efficiency and effectiveness.</p></div>
<div class="para"><p><strong>2.2.1</strong> <strong>Data Preparation</strong></p></div>
<div class="para"><p>The importance of data cannot be overstated in NLP. As larger neural networks are developed,the demand for data continues to increase. For example, developing LLMs may require trillions of tokens in pre-training (see Table 2.3), orders of magnitude larger than those used in training conventional NLP models. In general, we may want to gather as much training data as possible.However, larger training datasets do not mean better training results, and the development of LLMs raises new issues in creating or collecting these datasets.</p></div>
<div class="para"><p>A first issue is the quality of data. High-quality data has long been seen as crucial for training data-driven NLP systems. Directly using raw text from various sources is in general undesirable.For example, a significant portion of the data used to train recent LLMs comes from web scraping,which may contain errors and inappropriate content, such as toxic information and fabricated facts. Also, the internet is flooded with machine-generated content due to the widespread use of AI, presenting further challenges for processing and using web-scraped data. Researchers have found that training LLMs on unfiltered data is harmful [Raffel et al., 2020]. Improving data quality typically involves incorporating filtering and cleaning steps in the data processing workflow. For example, Penedo et al. [2023] show that by adopting a number of data processing techniques, 90%</p></div>
<div class="para"><p><em>2.2 Training at Scale</em> 57</p></div>
<div class="para"><p>of their web-scraped data can be removed for LLM training. In addition to large-scale web-scraped data, LLM training data often includes books, papers, user-generated data on social media, and so on. Most of the latest LLMs are trained on such combined datasets, which are found to be important for the strong performance of the resulting models.</p></div>
<div class="para"><p>A second issue is the diversity of data. We want the training data to cover as many types of data as possible, so that the trained models can adapt to different downstream tasks easily. It has been widely recognized that the quality and diversity of training data both play very important roles in LLMs. An interesting example is that incorporating programming code into training data has been found to be beneficial for LLMs. The benefits are demonstrated not only in enhancing the programming abilities of LLMs, but also in improving reasoning for complex problems, especially those requiring COT prompting. The concept “diversity” can be extended to include language diversity as well. For example, many LLMs are trained on multi-lingual data, and therefore we can handle multiple languages using a single model. While this approach shows strong abilities in multi-lingual and cross-lingual tasks, its performance on specific languages largely depends on the volume and quality of the data for those languages. It has been shown in some cases to provide poor results for low-resource languages.</p></div>
<div class="para"><p>A third issue is the bias in training data. This is not a problem that is specific to LLMs but exists in many NLP systems. A common example is gender bias, where LLMs show a preference for one gender over another. This can partly be attributed to class imbalance in the training data,for example, the term<em> nurses</em> is more often associated with women. In order to debias the data,it is common practice to balance the categories of different language phenomena, such as gender,ethnicity, and dialects. The bias in data is also related to the diversity issue mentioned above.For example, since many LLMs are trained and aligned with English-centric data, they are biased towards the cultural values and perspectives prevalent among English-speaking populations.Increasing language diversity in training data can somewhat mitigate the bias.</p></div>
<div class="para"><p>Another issue with collecting large-scale data is the privacy concern. If LLMs are trained on data from extensive sources, this potentially leads to risks regarding the exposure of sensitive information, such as intellectual property and personal data. This is particularly concerning given the capacity of LLMs to represent patterns from the data they are trained on, which might inadvertently involve memorizing and reproducing specific details. A simple approach to privacy protection is to remove or anonymize sensitive information. For example, anonymization techniques can be applied to remove personally identifiable information from training data to prevent LLMs from learning from such data. However, in practice, erasing or redacting all sensitive data is difficult. Therefore, many LLMs, particularly those launched for public service, typically work with systems that can detect the potential exposure of sensitive data, or are fine-tuned to reject certain requests that could lead to information leakage.</p></div>
<div class="para"><p><strong>2.2.2</strong> <strong>Model Modifications</strong></p></div>
<div class="para"><p>Training LLMs is difficult. A commonly encountered problem is that the training process becomes more unstable as LLMs get bigger. For example, one needs to choose a small learning rate to achieve stable training with gradient descent, but this in turn results in much longer training times. Sometimes, even when the training configuration is carefully designed, training may diverge at certain points during optimization. The training of LLMs is generally influenced by many factors, such as parameter initialization, batching, and regularization. Here, we focus on common</p></div>
<div class="para"><p>58 <em>Generative Models</em></p></div>
<div class="para"><p>modifications and improvements to the standard Transformer architecture, which are considered important in developing trainable LLMs.</p></div>
<div class="para"><p><strong>2.2.2.1</strong> <strong>Layer Normalization with Residual Connections</strong></p></div>
<div class="para"><p>Layer normalization is used to stabilize training for deep neural networks. It is a process of subtracting the mean and dividing by the standard deviation. By normalizing layer output in this way, we can effectively reduce the covariate shift problem and improve the training stability.In Transformers, layer normalization is typically used together with residual connections. As described in Section 2.1.1, a sub-layer can be based on either the post-norm architecture, in which layer normalization is performed right after a residual block, or the pre-norm architecture, in which layer normalization is performed inside a residual block. While both of these architectures are widely used in Transformer-based systems [Wang et al., 2019], the pre-norm architecture has proven to be especially useful in training deep Transformers. Given this, most LLMs are based on the pre-norm architecture, expressed as output = LNorm(<em>F</em>(input)) + input.</p></div>
<div class="para"><p>A widely-used form of the layer normalization function is given by</p></div>
<div class="para"><p>LNorm(<strong>h</strong>)=<em>α</em><em> ·</em><strong> </strong><strong>h</strong><em> −</em><em>µ</em></p></div>
<div class="para"><p><em>σ</em> +<em> ϵ</em> +<em> β</em>(2.23)</p></div>
<div class="para"><p>where<strong> h</strong> is a<em> d</em>-dimensional real-valued vector,<em> µ</em> is the mean of all the entries of<strong> h</strong>, and<em> σ</em> is the corresponding standard deviation.<em> ϵ</em> is introduced for the sake of numerical stability.<em> α</em><em> ∈</em>R <em>d</em> and <em>β</em><em> ∈</em>R <em>d</em> are the gain and bias terms.</p></div>
<div class="para"><p>A variant of layer normalization, called root mean square (RMS) layer normalization, only re-scales the input vector but does not re-center it [Zhang and Sennrich, 2019]. The RMS layer normalization function is given by</p></div>
<div class="para"><p>LNorm(<strong>h</strong>)=<em>α</em><em> ·</em><strong>h</strong> <em>σ</em> rms +<em> ϵ</em> +<em> β</em>(2.24)</p></div>
<div class="para"><p><em>d</em> P <em>d</em> <em>k</em>=1<em> </em><em>h</em> 2 <em>k</em>)1 2 . This layer normalization function is used in LLMs like the LLaMA series.</p></div>
<div class="para"><p>where<em> σ</em> rms is the root mean square of<strong> h</strong>, that is,<em> σ</em> rms = ( 1</p></div>
<div class="para"><p><strong>2.2.2.2</strong> <strong>Activation Functions in FFNs</strong></p></div>
<div class="para"><p>In Transformers, FFN sub-layers are designed to introduce non-linearities into representation learning, and are found to be useful for preventing the representations learned by self-attention from degeneration 8 [Dong et al., 2021]. A standard form of the FFNs used in these sub-layers can be expressed as</p></div>
<div class="para"><p>FFN(<strong>h</strong>)=<em>σ</em>(<strong>hW</strong> <em>h</em> +<strong> b</strong> <em>h</em>)<strong>W</strong> <em>f</em> +<strong> b</strong> <em>f</em>(2.25)</p></div>
<div class="para"><p>where<strong> W</strong> <em>h</em><em> ∈</em>R <em>d</em><em>×</em><em>d</em> <em>h</em>,<strong> b</strong> <em>h</em><em> ∈</em>R <em>d</em> <em>h</em>,<strong> W</strong> <em>f</em><em> ∈</em>R <em>d</em> <em>h</em><em>×</em><em>d</em>, and<strong> b</strong> <em>f</em><em> ∈</em>R <em>d</em> are the parameters, and<em> d</em> <em>h</em> is the hidden size.<em> σ</em>(<em>·</em>) is the activation function of the hidden layer. A common choice for<em> σ</em>(<em>·</em>) is the</p></div>
<div class="para"><p>8 Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.</p></div>
<div class="para"><p><em>2.2 Training at Scale</em> 59</p></div>
<div class="para"><p><strong>rectified linear unit</strong> (<strong>ReLU</strong>), given by</p></div>
<div class="para"><p><em>σ</em> relu(<strong>h</strong>)=max(0<em>,</em><strong> h</strong>)(2.26)</p></div>
<div class="para"><p>In practical implementations, increasing<em> d</em> <em>h</em> is helpful and thus it is often set to a larger number in LLMs. But a very large hidden size poses challenges for both training and deployment. In this case, the design of the activation function plays a relatively more important role in wide FFNs.There are several alternatives to the ReLU in LLMs. One of these is the<strong> gaussian error linear</strong> <strong>unit</strong> (<strong>GeLU</strong>) which can be seen as a smoothed version of the ReLU. Rather than controlling the output by the sign of the input, the GeLU function weights its input by the percentile Pr(<em>h</em><em> ≤</em><strong>h</strong>).Here<em> h</em> is a<em> d</em>-dimensional vector whose entries are drawn from the standard normal distribution Gaussian(0<em>,</em> 1)9. Specifically, the GeLU function is defined to be</p></div>
<div class="para"><p><em>σ</em> gelu(<strong>h</strong>)=<strong>h</strong> Pr(<em>h</em><em> ≤</em><strong>h</strong>)</p></div>
<div class="para"><p>=<strong>h</strong> Φ(<strong>h</strong>)(2.27)</p></div>
<div class="para"><p>where Φ(<strong>h</strong>) is the cumulative distribution function of Gaussian(0<em>,</em> 1), which can be implemented in convenient ways [Hendrycks and Gimpel, 2016]. The GeLU function has been adopted in several LLMs, such as BERT, GPT-3, and BLOOM.</p></div>
<div class="para"><p>Another family of activation functions which is popular in LLMs is<strong> gated linear unit</strong> (<strong>GLU</strong>)based functions. The basic form of GLUs is given by</p></div>
<div class="para"><p><em>σ</em> glu(<strong>h</strong>)=<em>σ</em>(<strong>hW</strong> 1 +<strong> b</strong> 1)<em> ⊙</em>(<strong>W</strong> 2 +<strong> b</strong> 2)(2.28)</p></div>
<div class="para"><p>where<strong> W</strong> 1<em> ∈</em>R <em>d</em><em>×</em><em>d</em>,<strong> b</strong> 1<em> ∈</em>R <em>d</em>,<strong> W</strong> 2<em> ∈</em>R <em>d</em><em>×</em><em>d</em>, and<strong> b</strong> 2<em> ∈</em>R <em>d</em> are model parameters. Different choices of<em> σ</em>(<em>·</em>) result in different versions of GLU functions. For example, if<em> σ</em>(<em>·</em>) is defined to be the GeLU function, we will have the GeGLU function</p></div>
<div class="para"><p><em>σ</em> geglu(<strong>h</strong>)=<em>σ</em> gelu(<strong>hW</strong> 1 +<strong> b</strong> 1)<em> ⊙</em>(<strong>W</strong> 2 +<strong> b</strong> 2)(2.29)</p></div>
<div class="para"><p>This activation function has been successfully applied in LLMs like Gemma.</p></div>
<div class="para"><p>As another example, consider<em> σ</em>(<em>·</em>) to be the Swish function<em> σ</em> swish(<strong>h</strong>) =<strong> h</strong><em> ⊙</em>Sigmoid(<em>c</em> <strong>h</strong>)[Ramachandran et al., 2017]. Then, the SwiGLU function is given by</p></div>
<div class="para"><p><em>σ</em> swiglu(<strong>h</strong>)=<em>σ</em> swish(<strong>hW</strong> 1 +<strong> b</strong> 1)<em> ⊙</em>(<strong>W</strong> 2 +<strong> b</strong> 2)(2.30)</p></div>
<div class="para"><p>Both the PaLM and LLaMA series are based on the SwiGLU function. For more discussions of GLUs, the reader can refer to Shazeer [2020]’s work.</p></div>
<div class="para"><p>9 Pr(<em>h</em><em> ≤</em><strong>h</strong>) is an informal notation. It refers to a vector, with each entry representing the percentile for the corresponding entry of<strong> h</strong>.</p></div>
<div class="para"><p>60 <em>Generative Models</em></p></div>
<div class="para"><p><strong>2.2.2.3</strong> <strong>Removing Bias Terms</strong></p></div>
<div class="para"><p>Another popular model design is to remove the bias terms in affine transformations used in LLMs.This treatment can be applied to layer normalization, transformations of the inputs to QKV attention, and FFNs. For example, we can modify Eq. (2.25) to obtain an FFN with no bias terms</p></div>
<div class="para"><p>FFN(<strong>h</strong>)=<em>σ</em>(<strong>hW</strong> <em>h</em>)<strong>W</strong> <em>f</em>(2.31)</p></div>
<div class="para"><p>Chowdhery et al. [2022] report that removing bias terms helps improve the training stability of LLMs. This method has been used in several recent LLMs, such as LLaMA and Gemma.</p></div>
<div class="para"><p><strong>2.2.2.4</strong> <strong>Other Issues</strong></p></div>
<div class="para"><p>Many LLMs also involve modifications to their positional embedding models. For example, one can replace sinusoidal positional encodings with rotary position embeddings so that the learned LLMs can handle long sequences better. These models will be discussed in Section 2.3.</p></div>
<div class="para"><p>Note that while model modifications are common in training LLMs, the stability of training can be improved in many different ways. For example, increasing the batch size as the training proceeds has been found to be useful for some LLMs. In general, achieving stable and efficient large-scale LLM training requires carefully designed setups, including learning schedules, optimizer choices, training parallelism, mixed precision training, and so on. Some of these issues are highly engineered, and therefore, we typically need a number of training runs to obtain satisfactory LLMs.</p></div>
<div class="para"><p><strong>2.2.3</strong> <strong>Distributed Training</strong></p></div>
<div class="para"><p>Training LLMs requires significant amounts of computational resources. A common approach to improving training efficiency is to use large-scale distributed systems. Fortunately, alongside the rise of neural networks in AI, deep learning-oriented software and hardware have been developed,making it easier to implement LLMs and perform computations. For example, one can now easily fine-tune an LLM using deep learning software frameworks and a machine with multiple GPUs.However, scaling up the training of LLMs is still challenging, and requires significant efforts in developing hardware and software systems for stable and efficient distributed training.</p></div>
<div class="para"><p>An important consideration of distributed training is parallelism. There are several forms of parallelism: data parallelism, model parallelism, tensor parallelism, and pipeline parallelism. Despite different ways to distribute computations across devices, these parallelism methods are based on a similar idea: the training problem can be divided into smaller tasks that can be executed simultaneously. The issue of parallelism in training LLMs has been extensively studied [Narayanan et al., 2021; Fedus et al., 2022]. Here we sketch the basic concepts.</p></div>
<div class="para"><p>•<strong> Data Parallelism</strong>. This method is one of the most widely used parallelism methods for training neural networks. To illustrate, consider the simplest case where the standard delta rule is used in gradient descent</p></div>
<div class="para"><p><em>θ</em> <em>t</em>+1=<em>θ</em> <em>t</em><em> −</em><em>lr</em><em> ·</em><em> </em><em>∂L</em> <em>θ</em> <em>t</em>(<em>D</em> mini)</p></div>
<div class="para"><p>(2.32)</p></div>
<div class="para"><p><em>∂θ</em> <em>t</em></p></div>
<div class="para"><p><em>2.2 Training at Scale</em> 61</p></div>
<div class="para"><p>where the new parameters<em> θ</em> <em>t</em>+1 is obtained by updating the latest parameters<em> θ</em> <em>t</em> with a small step<em> lr</em> in the direction of the negative loss gradient.<em> </em><em>∂L</em> <em>θt</em>(<em>D</em> mini)</p></div>
<div class="para"><p><em>∂θ</em> <em>t</em> is the gradient of the loss with respect to the parameters<em> θ</em> <em>t</em>, and is computed on a minibatch of training sample<em> D</em> mini.In data parallelism, we divide<em> D</em> mini into<em> N</em> smaller batches, denoted by<em> {D</em> 1<em>, ...,</em><em> D</em> <em>N</em><em>}</em>.Then, we distribute these batches to<em> N</em> workers, each with a corresponding batch. Once the data is distributed, these workers can work at the same time. The gradient of the entire minibatch is obtained by aggregating the gradients computed by the workers, like this</p></div>
<div class="para"><p><em>∂θ</em> <em>t</em>=<em>∂L</em> <em>θ</em> <em>t</em>(<em>D</em> 1)</p></div>
<div class="para"><p>+<em> </em><em>∂L</em> <em>θ</em> <em>t</em>(<em>D</em> 2)</p></div>
<div class="para"><p>+<em> · · ·</em> +<em> </em><em>∂L</em> <em>θ</em> <em>t</em>(<em>D</em> <em>N</em>)</p></div>
<div class="para"><p><em>∂L</em> <em>θ</em> <em>t</em>(<em>D</em> mini)</p></div>
<div class="para"><p>(2.33)</p></div>
<div class="para"><p><em>∂θ</em> <em>t</em>|{z}worker 1</p></div>
<div class="para"><p><em>∂θ</em> <em>t</em>|{z}worker 2</p></div>
<div class="para"><p><em>∂θ</em> <em>t</em>|{z}worker<em> N</em></p></div>
<div class="para"><p>In ideal cases where the workers coordinate well and the communication overhead is small,data parallelism can achieve nearly an<em> N</em>-fold speed-up for training.</p></div>
<div class="para"><p>•<strong> Model Parallelism</strong>. Although data parallelism is simple and effective, it requires each worker to run the entire LLM and perform the complete forward and backward process.As LLMs grow larger, it sometimes becomes unfeasible to load and execute an LLM on a single device. In this case, we can decouple the LLM into smaller components and run these components on different devices. One simple way to do this is to group consecutive layers in the layer stack and assign each group to a worker. The workers operate in the order of the layers in the stack, that is, in the forward pass we process the input from lower-level to upper-level layers, and in the backward pass we propagate the error gradients from upperlevel to lower-level layers. Consider, for example, a Transformer decoder with<em> L</em> stacked blocks. To distribute the computation load, each block is assigned to a worker. See the following illustration for a single run of the forward and backward passes of this model.</p></div>
<div class="para"><p>Worker<em> L</em> B <em>L</em> (<em>↑</em>) B <em>L</em> (<em>↓</em>)</p></div>
<div class="para"><p>.........</p></div>
<div class="para"><p>Worker 2 B 2 (<em>↑</em>)B 2 (<em>↓</em>)</p></div>
<div class="para"><p>Worker 1 B 1 (<em>↑</em>)B 1 (<em>↓</em>)</p></div>
<div class="para"><p>Here B <em>l</em> denotes the computation of block<em> l</em>, and the symbols<em> ↑</em>and<em> ↓</em>denote the forward and backward passes, respectively. Note that this parallelism method forces the workers to run in sequence, so a worker has to wait for the previous worker to finish their job. This results in the devices being idle for most of the time. In practical systems, model parallelism is generally used together with other parallelism mechanisms to maximize the use of devices.</p></div>
<div class="para"><p>•<strong> Tensor Parallelism</strong>. Parallelism can also be performed in a single computation step. A common example is splitting a large parameter matrix into chunks, multiplying an input tensor with each of these chunks separately, and then concatenating the results of these multiplications to form the output. For example, consider the multiplication of the representation<strong> h</strong><em> ∈</em>R <em>d</em> with the parameter matrix<strong> W</strong> <em>h</em><em> ∈</em>R <em>d</em><em>×</em><em>d</em> <em>h</em> in an FFN sub-layer (see Eq.(2.25)). We can slice the matrix<strong> W</strong> <em>h</em><em> ∈</em>R <em>d</em><em>×</em><em>d</em> <em>h</em> vertically to a sequence of<em> M</em> sub-matrices</p></div>
<div class="para"><p><strong>W</strong> <em>h</em>=h <strong>W</strong> 1 <em>h</em> <strong>W</strong> 2 <em>h</em><em>...</em><strong>W</strong> <em>M</em> <em>h</em> i</p></div>
<div class="para"><p>(2.34)</p></div>
<div class="para"><p>62 <em>Generative Models</em></p></div>
<div class="para"><p>where each sub-matrix<strong> W</strong> <em>k</em> <em>h</em> has a shape of<em> d</em><em> ×</em><em> d</em> <em>h</em></p></div>
<div class="para"><p><em>M</em> . The multiplication of<strong> h</strong> with<strong> W</strong> <em>h</em> can be expressed as</p></div>
<div class="para"><p><strong>hW</strong> <em>h</em>=<strong>h</strong> h <strong>W</strong> 1 <em>h</em> <strong>W</strong> 2 <em>h</em><em>...</em><strong>W</strong> <em>M</em> <em>h</em> i</p></div>
<div class="para"><p>=h <strong>hW</strong> 1 <em>h</em> <strong>hW</strong> 2 <em>h</em><em>...</em><strong>hW</strong> <em>M</em> <em>h</em> i</p></div>
<div class="para"><p>(2.35)</p></div>
<div class="para"><p>We can perform matrix multiplications<em> {</em><strong>hW</strong> 1 <em>h</em><em>,</em><strong> hW</strong> 2 <em>h</em><em>, ...,</em><strong> hW</strong> <em>M</em> <em>h</em><em> </em><em>}</em> on<em> M</em> devices separately.As a result, we distribute a large matrix multiplication across multiple devices, each of which may have relatively small memory. From the perspective of the design of modern GPUs, tensor parallelism over GPUs provides a two-level, tile-based approach to parallel computing. First, at a higher level, we decompose a matrix multiplication into sub-matrix multiplications that can directly fit into the memory of GPUs. Then, at a lower level, we execute these sub-matrix multiplications on GPUs using tile-based parallel algorithms that are specifically optimized for GPUs.</p></div>
<div class="para"><p>•<strong> Pipeline Parallelism</strong>. Above, in model parallelism, we have described a simple approach to spreading groups of model components across multiple devices. But this method is inefficient because only one device is activated at a time during processing. Pipeline parallelism addresses this issue by introducing overlaps between computations on different devices [Harlap et al., 2018; Huang et al., 2019]. To do this, a batch of samples is divided into a number of micro-batches, and then these micro-batches are processed by each worker as usual. Once a micro-batch is processed by a worker and passed to the next one, the following micro-batch immediately occupies the same worker. In other words, we create a pipeline in which different computation steps can overlap if multiple jobs are given to the pipeline. The following shows an illustration of pipeline parallelism for processing 3 micro-batches.</p></div>
<div class="para"><p>Worker<em> L</em> B <em>L,</em>1 B <em>L,</em>2 B <em>L,</em>3 B <em>L,</em>1 B <em>L,</em>2 B <em>L,</em>3</p></div>
<div class="para"><p>.........</p></div>
<div class="para"><p>Worker 2 B 2<em>,</em>1 B 2<em>,</em>2 B 2<em>,</em>3 B 2<em>,</em>1 B 2<em>,</em>2 B 2<em>,</em>3</p></div>
<div class="para"><p>Worker 1 B 1<em>,</em>1 B 1<em>,</em>2 B 1<em>,</em>3 B 1<em>,</em>1 B 1<em>,</em>2 B 1<em>,</em>3</p></div>
<div class="para"><p>Here B <em>l,k</em> represents the processing of the<em> k</em>-th micro-batch by the<em> l</em>-th worker. Ideally we would like to maximize the number of micro-batches, and thus minimize the idle time of the workers. However, in practice, using small micro-batches often reduces GPU utilization and increases task-switching costs. This may, in turn, decrease the overall system throughput.</p></div>
<div class="para"><p>The ultimate goal of parallel processing is to achieve linear growth in efficiency, that is, the number of samples that can be processed per unit of time increases linearly with the number of devices. However, distributed training is complicated, and influenced by many factors in addition to the parallelism method we choose. One problem, which is often associated with distributed systems, is the cost of communication. We can think of a distributed system as a group of networked nodes. Each of these nodes can perform local computation or pass data to other nodes. If</p></div>
<div class="para"><p><em>2.2 Training at Scale</em> 63</p></div>
<div class="para"><p>there are a large number of such nodes, it will be expensive to distribute and collect data across them. Sometimes, the time savings brought about by parallelism are offset by the communication overhead of a large network. Another problem with large-scale distributed systems is that the synchronization of nodes introduces additional costs. As is often the case, some nodes may take longer to work, causing others to wait for the slowest ones. While we can use asynchronous training to handle heterogeneity in computational resources, this may lead to stale gradients and non-guaranteed convergence. Moreover, as more nodes are added to the network, there is more chance to have crashed nodes during training. In this case, we need to ensure that the whole system is fault tolerant. In many practical settings, to increase scalability, one needs to take into account additional issues, including architecture design, data transfer and computation overlap,load balancing, memory bandwidth and so on.</p></div>
<div class="para"><p>Training LLMs is so computationally expensive that, even though distributed training is already in use, researchers and engineers often still employ various model compression and speedup methods to improve training efficiency [Weng, 2021]. One example is mixed precision training,in which low precision data (such as FP16 and FP8 data) is used for gradient computation on each individual node, and single or double precision data (such as FP32/FP64 data) is used for updating the model [Micikevicius et al., 2018]. A key operation in this approach is gradient accumulation where gradients need to be accumulated and synchronized across nodes. However, due to the non-associativity of floating-point addition, this can lead to slight numerical differences in accumulated gradients on different nodes, which may affect model convergence and final performance.This problem is more obvious if there are a large number of nodes involved in distributed training,especially given that low-precision numerical computations may encounter overflow and underflow issues, as well as inconsistencies across different hardware devices. Therefore, the design of distributed systems needs to consider these numerical computation issues to ensure satisfactory results and convergence.</p></div>
<div class="para"><p><strong>2.2.4</strong> <strong>Scaling Laws</strong></p></div>
<div class="para"><p>The success of LLMs reveals that training larger language models using more resources can lead to improved model performance. Researchers have explained this as<strong> scaling laws</strong> of LLMs. More specifically, scaling laws describe the relationships between the performance of LLMs and the attributes of LLM training, such as the model size, the amount of computation used for training,and the amount of training data. For example, Hestness et al. [2017] show that the performance of deep neural networks is a power-law-like function of the training data size. In the beginning, when the amount of training data is not large, the performance of the model improves slowly. Afterward,when more training data is used, the model enters a phase of rapid performance improvement, and the performance curve resembles a power-law curve. Ultimately, the improvement in performance becomes slow again, and more data does not lead to significant gains. Figure 2.3 shows an example of such curves.</p></div>
<div class="para"><p>In NLP, a traditional view holds that the performance gains will disappear at a certain point as the training is scaled up. However, recent results show that, if we consider the problem on a larger scale, scaling up training is still a very effective method for obtaining stronger LLMs.For example, both closed-source and open-source LLMs can benefit from more data, even though trillions of tokens have already been used for training.</p></div>
<div class="para"><p>With the increase in the scale of model training, LLMs exhibit new capabilities, known as the</p></div>
<div class="para"><p>64 <em>Generative Models</em></p></div>
<div class="para"><p>Slow Reduction Phase</p></div>
<div class="para"><p>Power-law Reduction Phase</p></div>
<div class="para"><p>Convergence Phase(Irreducible Error)</p></div>
<div class="para"><p>Number of Test Errors (Log-scale)</p></div>
<div class="para"><p>Training Dataset Size (Log-scale)</p></div>
<div class="para"><p><strong>Fig. 2.3:</strong> A scaling law of test error against a variable of interest (e.g., training dataset size) [Hestness et al., 2017]. The curve of the scaling law can be divided into three phases. At the beginning, the number of test errors decreases slowly when more training data is used, but this only lasts for a short period. In the second phase, the number of test errors decreases drastically, and the curve becomes a power law curve. After that, the error reduction slows down again in the third phase. Note that there are irreducible errors that cannot be eliminated, regardless of the amount of training data.</p></div>
<div class="para"><p><strong>emergent abilities</strong> of LLMs. For example, Wei et al. [2022b] studied the scaling properties of LLMs across different model sizes and amounts of computational resources. Their work shows that some abilities emerge when we scale the model size to certain level. The appearance of emergent abilities has demonstrated the role of scaled training in enhancing the performance of LLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger models. As larger and stronger LMs continue to appear, our understanding of the scaling laws continues to mature. This helps researchers predict the performance of LLMs during training and estimate the minimal computational resources required to achieve a given level of performance.</p></div>
<div class="para"><p>To understand how model performance scales with various factors considered during training,it is common to express the model performance as a function of these factors. For example, in the simplest case, we can express the loss or error of an LLM as a function of a single variable of interest. However, there are no universal scaling laws that can describe this relationship. Instead,different functions are proposed to fit the learning curves of LLMs.</p></div>
<div class="para"><p>Let<em> x</em> be the variable of interest (such as the number of model parameters) and<em> L</em>(<em>x</em>) be the loss of the model given<em> x</em> (such as the cross-entropy loss on test data). The simplest form of<em> L</em>(<em>x</em>)is a power law</p></div>
<div class="para"><p><em>L</em>(<em>x</em>)=<em>ax</em> <em>b</em>(2.36)</p></div>
<div class="para"><p>where<em> a</em> and<em> b</em> are parameters that are estimated empirically. Despite its simplicity, this function has successfully interpreted the scaling ability of language models and machine translation systems in terms of model size (denoted by<em> N</em>) and training dataset size (denoted by<em> D</em>) [Gordon et al., 2021; Hestness et al., 2017]. For example, Kaplan et al. [2020] found that the performance of their language model improves as a power law of either<em> N</em> or<em> D</em> after an initial transient period,and expressed these relationships using<em> L</em>(<em>N</em>) = <em>N</em> 8<em>.</em>8<em>×</em>10 13<em>−</em>0<em>.</em>076 and<em> L</em>(<em>D</em>) = <em>D</em> 5<em>.</em>4<em>×</em>10 13<em>−</em>0<em>.</em>095</p></div>
<div class="para"><p>(see Figure 2.4).</p></div>
<h3>2.2 Training at Scale 65</h3>
<div class="para"><p>4<em>.</em>2</p></div>
<div class="para"><p><em>L</em>(<em>N</em>) = (<em>N</em> 8<em>.</em>8<em>·</em>10 13 )<em>−</em>0<em>.</em>076</p></div>
<div class="para"><p><em>L</em>(<em>D</em>) = (<em>D</em> 5<em>.</em>4<em>·</em>10 13 )<em>−</em>0<em>.</em>095</p></div>
<div class="para"><p>5<em>.</em>6</p></div>
<div class="para"><p>3<em>.</em>9</p></div>
<div class="para"><p>4<em>.</em>8</p></div>
<h3>Test Loss</h3>
<h3>Test Loss</h3>
<div class="para"><p>3<em>.</em>6</p></div>
<div class="para"><p>4<em>.</em>0</p></div>
<div class="para"><p>3<em>.</em>3</p></div>
<div class="para"><p>3<em>.</em>2</p></div>
<div class="para"><p>3</p></div>
<div class="para"><p>2<em>.</em>4</p></div>
<div class="para"><p>2<em>.</em>7</p></div>
<div class="para"><p>10 5 10 7 10 9</p></div>
<div class="para"><p>10 8 10 9</p></div>
<h3>Number of Parameters</h3>
<h3>Dataset Size</h3>
<div class="para"><p><strong>Fig. 2.4:</strong> Test loss against model size (<em>N</em>) and training dataset size (<em>D</em>) (data points are plotted for illustrative purposes).We plot test loss as a function of<em> N</em>, which is defined as<em> L</em>(<em>N</em>) =  <em>N</em> 8<em>.</em>8<em>×</em>10 13<em>−</em>0<em>.</em>076, and a function of<em> D</em>, which is</p></div>
<div class="para"><p>defined as<em> L</em>(<em>D</em>) =  <em>D</em> 5<em>.</em>4<em>×</em>10 13<em>−</em>0<em>.</em>095 [Kaplan et al., 2020].</p></div>
<h3>An improvement to this scaling law is to add an irreducible error term to the power law. The form of L(x) is then given by</h3>
<h3>L(x) = ax b + ϵ∞ (2.37)</h3>
<h3>where ϵ∞is the irreducible error that accounts for the error due to unknown variables, which is present even as x →∞. Eq. (2.37) is one of the most widely used forms for designing scaling laws of LLMs. For example, Rosenfeld et al. [2020] developed a scaling law that involves both model scaling and dataset scaling, like this</h3>
<h3>L(N, D) = aN b + cD d + ϵ∞ (2.38)</h3>
<h3>An example of such formulation is the Chinchilla scaling law. It states that the test loss per token is the sum of the inverse proportion functions of N and D, with an additional irreducible error term. Hoffmann et al. [2022] express this scaling law as</h3>
<h3>L(N, D) = 406.4 N 0.34 | {z } model scaling</h3>
<h3>+ 410.7 D 0.28 | {z } dataset scaling</h3>
<h3>+ 1.69 |{z} irreducible error</h3>
<h3>(2.39)</h3>
<h3>All the scaling laws mentioned above are based on monotonic functions. So they cannot cover functions with inflection points, such as double descent curves. In response, researchers have explored more sophisticated functions to fit the learning curves. Examples of such functions can be found in Alabdulmohsin et al. [2022] and Caballero et al. [2023]’s work.</h3>
<h3>The significance of scaling laws lies in providing directional guidance for LLM research: if we are still in the region of the power law curve, using more resources to train larger models is a very promising direction. While this result “forces” big research groups and companies to invest more in computational resources to train larger models, which is very expensive, scaling laws continuously push the boundaries of AI further away. On the other hand, understanding scaling laws helps researchers make decisions in training LLMs. For example, given the computational</h3>
<div class="para"><p>66 <em>Generative Models</em></p></div>
<div class="para"><p>resources at hand, the performance of LLMs may be predicted.</p></div>
<div class="para"><p>One last note on scaling laws in this section. For LLMs, a lower test loss does not always imply better performance on all downstream tasks. To adapt LLMs, there are several steps such as fine-tuning and prompting that may influence the final result. Therefore, the scaling laws for different downstream tasks might be different in practice.</p></div>
<h3>2.3 Long Sequence Modeling</h3>
<div class="para"><p>We have already seen that, in large-scale training, larger language models can be developed by using more data and computational resources. However, scaling up can also occur in other directions.For instance, in many applications, LLMs are adapted to process significantly long sequences. An interesting example is that we pre-train an LLM on extensive texts of normal length and then apply it to deal with very long token sequences, far beyond the length encountered in pre-training.Here we use Pr(<strong>y</strong><em>|</em><strong>x</strong>) to denote the text generation probability where<strong> x</strong> is the context and<strong> y</strong> is the generated text. There are broadly three types of long sequence modeling problems.</p></div>
<div class="para"><p>•<strong> Text generation based on long context</strong> (i.e.,<strong> x</strong> is a long sequence). For example, we generate a short summary for a very long text.</p></div>
<div class="para"><p>•<strong> Long text generation</strong> (i.e.,<strong> y</strong> is a long sequence). For example, we generate a long story based on a few keywords.</p></div>
<div class="para"><p>•<strong> Long text generation based on long context</strong> (i.e., both<strong> x</strong> and<strong> y</strong> are long sequences). For example, we translate a long document from Chinese to English.</p></div>
<div class="para"><p>Recently, NLP researchers have been more interested in applying and evaluating LLMs on tasks where extremely long input texts are involved. Imagine an LLM, which reads a C++ source file containing tens of thousands of lines, and outlines the functionality of the program corresponding to the source file. Such models, capable of handling extensive textual contexts, are sometimes called<strong> long-context LLMs</strong>. In this section we will restrict ourselves to long-context LLMs, but the methods discussed here can be applicable to other problems.</p></div>
<div class="para"><p>For Transformers, dealing with long sequences is computationally expensive, as the computational cost of self-attention grows quadratically with the sequence length. This makes it infeasible to train and deploy such models for very long inputs. Two strands of research have tried to adapt Transformers to long-context language modeling.</p></div>
<div class="para"><p>• The first explores efficient training methods and model architectures to learn self-attention models from long-sequence data.</p></div>
<div class="para"><p>• The other adapts pre-trained LLMs to handle long sequences with modest or no fine-tuning efforts.</p></div>
<div class="para"><p>Here, we will discuss the former briefly since it can be found in general discussions of efficient Transformer architectures [Tay et al., 2020; Xiao and Zhu, 2023]. We will focus on the latter,</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 67</p></div>
<div class="para"><p>highlighting popular methods in recent LLMs. We will also discuss the strengths and limitations of these long-sequence models.</p></div>
<div class="para"><p><strong>2.3.1</strong> <strong>Optimization from HPC Perspectives</strong></p></div>
<div class="para"><p>We begin our discussion by considering improvements to standard Transformer models from the perspectives of high-performance computing. Most of these improvements, though not specifically designed for LLMs, have been widely applied across various deep learning models [Kim et al., 2023]. A commonly used approach is to adopt a low-precision implementation of Transformers. For example, we can use 8-bit or 16-bit fixed-point data types for arithmetic operations,instead of 32-bit or 64-bit floating-point data types. Using these low-precision data types can increase the efficiency and memory throughput, so that longer sequences can be processed more easily. An alternative approach is to improve Transformers by using hardware-aware techniques.For example, on modern GPUs, the efficiency of Transformers can be improved by using IO-aware implementations of the self-attention function [Dao et al., 2022; Kwon et al., 2023].</p></div>
<div class="para"><p>Another way to handle long sequences is through sequence parallelism [Li et al., 2023b; Korthikanti et al., 2023]. Specifically, consider the general problem of attending the query<strong> q</strong> <em>i</em> at the position<em> i</em> to the keys<strong> K</strong> and values<strong> V</strong>. We can divide<strong> K</strong> by rows and obtain a set of sub-matrices<em>{</em><strong>K</strong>[1]<em>, ...,</em><strong> K</strong>[<em>n</em> <em>u</em>]<em>}</em>, each corresponding to a segment of the sequence. Similarly, we can obtain the sub-matrices of<strong> V</strong>, denoted by<em> {</em><strong>V</strong>[1]<em>, ...,</em><strong> V</strong>[<em>n</em> <em>u</em>]<em>}</em>. Then, we assign each pair of<strong> K</strong>[<em>u</em>] and<strong> V</strong>[<em>u</em>] to a computing node (e.g., a GPU of a GPU cluster). The assigned nodes can run in parallel, thereby parallelizing the attention operation.</p></div>
<div class="para"><p>Recall that the output of the self-attention model can be written as</p></div>
<div class="para"><p><em>m</em><em>−</em>1 X</p></div>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>,</em><strong> V</strong>)=</p></div>
<div class="para"><p><em>j</em>=0 <em>α</em> <em>i,j</em> <strong>v</strong> <em>j</em>(2.40)</p></div>
<div class="para"><p>where<em> α</em> <em>i,j</em> is the attention weight between positions<em> i</em> and<em> j</em>. In Transformers,<em> α</em> <em>i,j</em> is obtained by normalizing the rescaled version of the dot product between<strong> q</strong> <em>i</em> and<strong> k</strong> <em>j</em>. Let<em> β</em> <em>i,j</em> denote the attention score between<strong> q</strong> <em>i</em> and<strong> k</strong> <em>j</em>. We have</p></div>
<div class="para"><p><em>β</em> <em>i,j</em>=<strong>q</strong> <em>i</em><em> ·</em><strong> k</strong> <em>j</em><em>√</em></p></div>
<div class="para"><p><em>d</em>+ Mask(<em>i, j</em>)(2.41)</p></div>
<div class="para"><p>where Mask(<em>i, j</em>) is the masking variable for (<em>i, j</em>). Then, we define the attention weight<em> α</em> <em>i,j</em> to be</p></div>
<div class="para"><p><em>α</em> <em>i,j</em>=Softmax(<em>β</em> <em>i,j</em>)</p></div>
<div class="para"><p>=exp(<em>β</em> <em>i,j</em>)P <em>j</em><em>′</em> exp(<em>β</em> <em>i,j</em><em>′</em>)(2.42)</p></div>
<div class="para"><p>On each computing node, we need to implement these equations. Given the keys and values assigned to this node, computing the numerator of the right-hand side of Eq. (2.42) (i.e., exp(<em>β</em> <em>i,j</em>))is straightforward, as all the required information is stored on the node. However, computing the denominator of the right-hand side of Eq. (2.42) involves a sum of exp(<em>β</em> <em>i,j</em><em>′</em>) over all<em> j</em><em>′</em>s, which</p></div>
<div class="para"><p>68 <em>Generative Models</em></p></div>
<div class="para"><p>requires transferring data to and from other nodes. To illustrate, suppose that<strong> v</strong> <em>j</em> and<strong> k</strong> <em>j</em> are placed on node<em> u</em>. We can rewrite Eq. (2.42) as</p></div>
<div class="para"><p><em>α</em> <em>i,j</em></p></div>
<div class="para"><p>node<em> u</em> z}|{exp(<em>β</em> <em>i,j</em>)X</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>(2.43)</p></div>
<div class="para"><p>+<em> · · ·</em> +X</p></div>
<div class="para"><p>+<em> · · ·</em> +X</p></div>
<div class="para"><p><strong>k</strong> <em>j</em><em>′</em><em>∈</em><strong>K</strong>[1]exp(<em>β</em> <em>i,j</em><em>′</em>)</p></div>
<div class="para"><p><strong>k</strong> <em>j</em><em>′</em><em>∈</em><strong>K</strong>[<em>u</em>]exp(<em>β</em> <em>i,j</em><em>′</em>)</p></div>
<div class="para"><p><strong>k</strong> <em>j</em><em>′</em><em>∈</em><strong>K</strong>[<em>nu</em>]exp(<em>β</em> <em>i,j</em><em>′</em>)</p></div>
<div class="para"><p>|{z}node 1</p></div>
<div class="para"><p>|{z}node<em> u</em></p></div>
<div class="para"><p>|{z}node<em> n</em> <em>u</em></p></div>
<div class="para"><p>where the notation<strong> k</strong> <em>j</em><em>′</em><em> ∈</em><strong>K</strong>[<em>u</em>] represents that<strong> k</strong> <em>j</em><em>′</em> is a row vector of<strong> K</strong>[<em>u</em>]. In a straightforward implementation, we first perform the summations<em> {</em>P <strong>k</strong> <em>j</em><em>′</em><em>∈</em><strong>K</strong>[<em>u</em>] exp(<em>β</em> <em>i,j</em><em>′</em>)<em>}</em> separately on the corresponding nodes. Then, we collect these summation results from different nodes to combine them into a final result. This corresponds to a collective operation in the context of parallel processing.There are many efficient implementations of such operations, such as the all-reduce algorithms.Hence the sum of all exp(<em>β</em> <em>i,j</em>) values can be computed using optimized routines in collective communication toolkits.</p></div>
<div class="para"><p>Given the attention weights<em> {</em><em>α</em> <em>i,j</em><em>}</em>, we then compute the attention results using Eq. (2.40).The problem can be re-expressed as</p></div>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>,</em><strong> V</strong>)</p></div>
<div class="para"><p>=X</p></div>
<div class="para"><p>+<em> · · ·</em> +X</p></div>
<div class="para"><p>+<em> · · ·</em> +X</p></div>
<div class="para"><p><strong>v</strong> <em>j</em><em>′</em><em>∈</em><strong>V</strong>[1]<em>α</em> <em>i,j</em><em>′</em><strong>v</strong> <em>j</em><em>′</em></p></div>
<div class="para"><p><strong>v</strong> <em>j</em><em>′</em><em>∈</em><strong>V</strong>[<em>u</em>]<em>α</em> <em>i,j</em><em>′</em><strong>v</strong> <em>j</em><em>′</em></p></div>
<div class="para"><p><strong>v</strong> <em>j</em><em>′</em><em>∈</em><strong>V</strong>[<em>nu</em>]<em>α</em> <em>i,j</em><em>′</em><strong>v</strong> <em>j</em><em>′</em></p></div>
<div class="para"><p>(2.44)</p></div>
<div class="para"><p>|{z}node 1</p></div>
<div class="para"><p>|{z}node<em> u</em></p></div>
<div class="para"><p>|{z}node<em> n</em> <em>u</em></p></div>
<div class="para"><p>Like Eq. (2.43), Eq. (2.44) can be implemented as a summation program in parallel processing. First, perform the weighted summations of values on different nodes simultaneously. Then,we collect the results from these nodes via collective operations.</p></div>
<div class="para"><p>Note that, although this section primarily focuses on long sequence modeling, much of the motivation for sequence parallelism comes from the distributed training methods of deep networks,as discussed in Section 2.2.3. As a result, the implementation of these methods can be based on the same parallel processing library.</p></div>
<h3>2.3.2 Efficient Architectures</h3>
<div class="para"><p>One difficulty of applying Transformers to long sequences is that self-attention has a quadratic time complexity with respect to the sequence length. Moreover, a<strong> key-value cache</strong> (or<strong> KV cache</strong> for short) is maintained during inference, and its size increases as more tokens are processed. Although the KV cache grows linearly with the sequence length, for extremely long input sequences,the memory footprint becomes significant and it is even infeasible to deploy LLMs for such tasks.As a result, the model architecture of long-context LLMs generally moves away from the standard Transformer, turning instead to the development of more efficient variants and alternatives.</p></div>
<div class="para"><p>One approach is to use sparse attention instead of standard self-attention. This family of models is based on the idea that only a small number of tokens are considered important when attending to a given token, and so most of the attention weights between tokens are close to zero.</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 69</p></div>
<div class="para"><p>As a consequence, we can prune most of the attention weights and represent the attention model in a compressed form. To illustrate, consider the self-attention model</p></div>
<div class="para"><p>Att qkv(<strong>Q</strong><em>,</em><strong> K</strong><em>,</em><strong> V</strong>)=<em>α</em>(<strong>Q</strong><em>,</em><strong> K</strong>)<strong>V</strong>(2.45)</p></div>
<div class="para"><p>where the attention weight matrix<em> α</em>(<strong>Q</strong><em>,</em><strong> K</strong>)<em> ∈</em>R <em>m</em><em>×</em><em>m</em> is obtained by</p></div>
<div class="para"><p><em>α</em>(<strong>Q</strong><em>,</em><strong> K</strong>)=Softmax(<strong>QK</strong> T</p></div>
<div class="para"><p><em>√</em></p></div>
<div class="para"><p><em>d</em>+<strong> Mask</strong>)</p></div>
<div class="para"><p><em>α</em> 0<em>,</em>0 0 0<em>...</em>0 <em>α</em> 1<em>,</em>0 <em>α</em> 1<em>,</em>1 0<em>...</em>0 <em>α</em> 2<em>,</em>0 <em>α</em> 2<em>,</em>1 <em>α</em> 2<em>,</em>2<em>...</em>0...............<em>α</em> <em>m</em><em>−</em>1<em>,</em>0 <em>α</em> <em>m</em><em>−</em>1<em>,</em>1 <em>α</em> <em>m</em><em>−</em>1<em>,</em>2<em>...</em><em>α</em> <em>m</em><em>−</em>1<em>,m</em><em>−</em>1</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>(2.46)</p></div>
<div class="para"><p>Each row vector h <em>α</em> <em>i,</em>0<em>...</em><em>α</em> <em>i,i</em> 0<em>...</em>0 i corresponds to a distribution of attending the<em> i</em>-th token to every token of the sequence. Since language models predict next tokens only based on their left-context, we normally write the output of the attention model at position<em> i</em> as</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><strong>v</strong> 0...<strong>v</strong> <em>i</em></p></div>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>)=h <em>α</em> <em>i,</em>0<em>...</em><em>α</em> <em>i,i</em> i</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><em>i</em> X</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p><em>j</em>=0 <em>α</em> <em>i,j</em> <strong>v</strong> <em>j</em>(2.47)</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><strong>k</strong> 0...<strong>k</strong> <em>i</em></p></div>
<div class="para"><p><strong>v</strong> 0...<strong>v</strong> <em>i</em></p></div>
<div class="para"><p></p></div>
<div class="para"><p>and<strong> V</strong><em>≤</em><em>i</em> =</p></div>
<div class="para"><p></p></div>
<div class="para"><p>are the keys and values up to position<em> i</em>.</p></div>
<div class="para"><p>where<strong> K</strong><em>≤</em><em>i</em> =</p></div>
<div class="para"><p>In the original version of self-attention h <em>α</em> <em>i,</em>0<em>...</em><em>α</em> <em>i,i</em> i is assumed to be dense, that is, most of</p></div>
<div class="para"><p>the values are non-zero. In sparse attention, some of the entries of h <em>α</em> <em>i,</em>0<em>...</em><em>α</em> <em>i,i</em> i are considered non-zero, and the remaining entries are simply ignored in computation. Suppose<em> G</em><em> ⊆{</em>0<em>, ..., i</em><em>}</em> is the set of indices of the non-zero entries. For language models, the output of the sparse attention model at position<em> i</em> is given by</p></div>
<div class="para"><p><em>j</em><em>∈</em><em>G</em> <em>α</em><em>′</em><em>i,j</em> <strong>v</strong> <em>j</em>(2.48)</p></div>
<div class="para"><p>Att sparse(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>)=X</p></div>
<div class="para"><p>Here<em> {</em><em>α</em><em>′</em><em>i,j</em><em>}</em> are normalized over<em> G</em>. Hence their values are different from the original attention weights (in fact we have<em> α</em><em>′</em><em>i,j</em><em> </em><em>&gt; α</em> <em>i,j</em>). The sparsity of the model is determined by how large<em> G</em> is.Sparse attention models differ in the way we define<em> G</em>. One simple approach is to define<em> G</em> based on heuristically designed patterns. For example, a widely-used pattern involves having<em> G</em> cover a window of tokens located near position<em> i</em> [Parmar et al., 2018].</p></div>
<div class="para"><p>While sparse attention reduces the computation through the use of sparse operations, such models still have significant limitations as we must keep the entire KV cache (i.e.,<strong> K</strong><em>≤</em><em>i</em> and<strong> V</strong><em>≤</em><em>i</em>)</p></div>
<div class="para"><p>70 <em>Generative Models</em></p></div>
<div class="para"><p>during inference. If the sequence is very long, storing this cache will become highly memoryintensive. To address this, we can consider a different form of attention models where the KV cache is not explicitly retained. Linear attention is one such approach [Katharopoulos et al.,2020]. It uses a kernel function<em> ϕ</em>(<em>·</em>) to project each query and key onto points<strong> q</strong><em>′</em><em>i</em> =<em> ϕ</em>(<strong>q</strong> <em>i</em>)and<strong> k</strong><em>′</em><em>i</em> =<em> ϕ</em>(<strong>k</strong> <em>i</em>), respectively. By removing the Softmax function under such transformations 10,the form of the resulting attention model is given by</p></div>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>)<em>≈</em>Att linear(<strong>q</strong><em>′</em><em>i</em><em>,</em><strong> K</strong><em>′</em><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>)</p></div>
<div class="para"><p>=<strong>q</strong><em>′</em><em>i</em> <em>µ</em> <em>i</em> <strong>q</strong><em>′</em><em>i</em> <em>ν</em> <em>i</em></p></div>
<div class="para"><p>(2.49)</p></div>
<div class="para"><p>where<em> µ</em> <em>i</em> and<em> ν</em> <em>i</em> are variables that are computed in the recurrent forms</p></div>
<div class="para"><p><em>µ</em> <em>i</em>=<em>µ</em> <em>i</em><em>−</em>1 +<strong> k</strong><em>′</em>T <em>i</em><strong> </strong><strong>v</strong> <em>i</em>(2.50)</p></div>
<div class="para"><p><em>ν</em> <em>i</em>=<em>ν</em> <em>i</em><em>−</em>1 +<strong> k</strong><em>′</em>T <em>i</em>(2.51)</p></div>
<div class="para"><p><em>µ</em> <em>i</em> and<em> ν</em> <em>i</em> can be seen as representations of the history up to position<em> i</em>. A benefit of this model is that we need not keep all past queries and values. Instead only the latest representations<em> µ</em> <em>i</em> and <em>ν</em> <em>i</em> are used. So the computational cost of each step is a constant, and the model can be easily extended to deal with long sequences.</p></div>
<div class="para"><p>In fact, this sequential approach to long sequence modeling arises naturally when we adopt a viewpoint of recurrent models. Such models read one token (or a small number of tokens) at a time, update the recurrent state using these inputs, and then discard them before the next token arrives. The output at each step is generated based only on the recurrent state, rather than on all the previous states. The memory footprint is determined by the recurrent state which has a fixed size.Recurrent models can be used in real-time learning scenarios where data arrives in a stream and predictions can be made at any time step. In NLP, applying recurrent models to language modeling is one of the earliest successful attempts to learn representations of sequences. Although Transformer has been used as the foundational architecture in LLMs, recurrent models are still powerful models, especially for developing efficient LLMs. More recently, recurrent models have started their resurgence in language modeling and have been reconsidered as a promising alternative to Transformers [Gu and Dao, 2023]. Figure 2.5 shows a comparison of the models discussed in this subsection.</p></div>
<div class="para"><p><strong>2.3.3</strong> <strong>Cache and Memory</strong></p></div>
<div class="para"><p>LLMs based on the standard Transformer architecture are global models. The inference for these models involves storing the entire left-context in order to make predictions for future tokens. This requires a KV cache where the representations (i.e., keys and values) of all previously-generated</p></div>
<div class="para"><p>10 In the new space after this transformation, the Softmax normalization can be transformed into the simple scaling normalization.</p></div>
<h3>2.3 Long Sequence Modeling 71</h3>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>)</p></div>
<h3>q i k i k i−1 k i−2 · · k 1 k 0</h3>
<div class="para"><p><strong>v</strong> <em>i</em> <strong>v</strong> <em>i</em><em>−</em>1 <strong>v</strong> <em>i</em><em>−</em>2<em>· · ·</em><strong>v</strong> 1 <strong>v</strong> 0</p></div>
<div class="para"><p>(a) Standard Self-attention</p></div>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>,</em><em> {</em><strong>k</strong> 1<em>,</em><strong> k</strong> <em>i</em><em>}</em><em>,</em><em> {</em><strong>v</strong> 1<em>,</em><strong> v</strong> <em>i</em><em>}</em>)</p></div>
<h3>q i k i k i−1 k i−2 · · k 1 k 0</h3>
<div class="para"><p><strong>v</strong> <em>i</em> <strong>v</strong> <em>i</em><em>−</em>1 <strong>v</strong> <em>i</em><em>−</em>2<em>· · ·</em><strong>v</strong> 1 <strong>v</strong> 0</p></div>
<div class="para"><p>(b) Sparse Attention</p></div>
<div class="para"><p><em>µ</em> <em>i</em> =<em> µ</em> <em>i</em><em>−</em>1 +<strong> k</strong><em>′</em>T <em>i</em><strong> </strong><strong>v</strong> <em>i</em></p></div>
<div class="para"><p><em>µ</em> <em>i</em><em>⇒</em></p></div>
<div class="para"><p>Att linear(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>) =<strong> </strong><strong>q</strong><em>′</em><em>i</em> <em>µ</em> <em>i</em> <strong>q</strong><em>′</em><em>i</em> <em>ν</em> <em>i</em></p></div>
<div class="para"><p><em>ν</em> <em>i</em> =<em> ν</em> <em>i</em><em>−</em>1 +<strong> k</strong><em>′</em>T <em>i</em></p></div>
<div class="para"><p><em>⇒</em></p></div>
<div class="para"><p><em>ν</em> <em>i</em></p></div>
<h3>q i k i k i−1 k i−2 · · k 1 k 0</h3>
<div class="para"><p><strong>v</strong> <em>i</em> <strong>v</strong> <em>i</em><em>−</em>1 <strong>v</strong> <em>i</em><em>−</em>2<em>· · ·</em><strong>v</strong> 1 <strong>v</strong> 0</p></div>
<div class="para"><p>(c) Linear Attention</p></div>
<div class="para"><p><strong>h</strong> <em>i</em> =<em> f</em>(<strong>h</strong> <em>i</em><em>−</em>1<em>,</em><strong> input</strong> <em>i</em>)</p></div>
<h3>h i h i−1 h i−2 h i−3 · · h 1 h 0</h3>
<div class="para"><p><strong>input</strong> <em>i</em></p></div>
<div class="para"><p>(d) Recurrent Models</p></div>
<div class="para"><p><strong>Fig. 2.5:</strong> Illustrations of self-attention, sparse attention, linear attention and recurrent models. Blue boxes = cached states for producing the output at position<em> i</em>.<em> f</em>(<em>·</em>) = a recurrent cell.</p></div>
<h3>tokens are kept, and the cost of caching grows as the inference proceeds. Above, we have dis- cussed methods for optimizing this cache via efficient attention approaches, such as sparse atten- tion and linear attention. Another idea, which may have overlap with the previous discussion, is to explicitly encode the context via an additional memory model.</h3>
<h3>72 Generative Models</h3>
<h3>2.3.3.1 Fixed-size KV Cache</h3>
<h3>A straightforward approach is to represent the keys and values using a fixed-size memory model. Suppose we have a memory Mem which retains the contextual information. We can write the attention operation at position i in a general form</h3>
<h3>Att(q i, Mem) = Att qkv(q i, K≤i, V≤i) (2.52)</h3>
<h3>In this model, Mem is simply the KV cache, i.e., Mem = (K≤i, V≤i). Thus the size of Mem is determined by i. If we define Mem as a fixed-size variable, then the cost of performing Att(q i, Mem) will be fixed. There are several alternative ways to design Mem.</h3>
<h3>One of the simplest methods is to consider a fixed-size window of previous keys and values. Mem is therefore given by</h3>
<h3>Mem = (K[i−n c+1,i], V[i−n c+1,i]) (2.53)</h3>
<h3>where n c denotes the size of the window. The notation K[i−n c+1,i] and V[i−n c+1,i] denote the keys and values over positions from i −n c + 1 to i.11 This model can be seen as a type of local attention model.</h3>
<h3>It is also possible to define Mem as a pair of summary vectors, which leads to a more compressed representation of the history. A simple way to summarize the previous keys and values is to use the moving average of them. For example, Mem can be defined as the unweighted moving average of the previous n c keys and values</h3>
<h3>Mem = P i j=i−n c+1 k j</h3>
<h3>P i j=i−n c+1 v j</h3>
<div class="para"><p></p></div>
<h3>n c ,</h3>
<h3>(2.54)</h3>
<h3>n c</h3>
<h3>Alternatively, we can use a weighted version of moving average</h3>
<h3>Mem = P i j=i−n c+1 β j−i+n c k j P n c j=1 β j ,</h3>
<h3>P i j=i−n c+1 β j−i+n c v j P n c j=1 β j</h3>
<div class="para"><p></p></div>
<h3>(2.55)</h3>
<h3>Here {β 1, ..., β n c} are the coefficients, which can be either learned as model parameters or determined via heuristics. For example, they can be set to increasing coefficients (i.e., β 1 &amp;lt; β 2 &amp;lt; ... &amp;lt; β n c−1 &amp;lt; β n c) in order to give larger weight to positions that are closer to i. We can extend the moving average to include all the positions up to i. This leads to the cumulative average of the keys and values, given in the form</h3>
<h3>Mem = P i j=0 k j i + 1 ,</h3>
<h3>P i j=0 v j i + 1</h3>
<div class="para"><p></p></div>
<h3>(2.56)</h3>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><strong>k</strong> <em>i</em><em>−</em><em>n</em> <em>c</em>+1...<strong>k</strong> <em>i</em></p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>−</em><em>n</em> <em>c</em>+1...<strong>v</strong> <em>i</em></p></div>
<div class="para"><p>11 More formally, we write<strong> K</strong>[<em>i</em><em>−</em><em>n</em> <em>c</em>+1<em>,i</em>] =</p></div>
<div class="para"><p>and<strong> V</strong>[<em>i</em><em>−</em><em>n</em> <em>c</em>+1<em>,i</em>] =</p></div>
<div class="para"><p>. Sometimes we denote</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><strong>K</strong>[<em>i</em><em>−</em><em>n</em> <em>c</em>+1<em>,i</em>] by<em> {</em><strong>k</strong> <em>i</em><em>−</em><em>n</em> <em>c</em>+1<em>, ...,</em><strong> k</strong> <em>i</em><em>}</em> and<strong> V</strong>[<em>i</em><em>−</em><em>n</em> <em>c</em>+1<em>,i</em>] by<em> {</em><strong>v</strong> <em>i</em><em>−</em><em>n</em> <em>c</em>+1<em>, ...,</em><strong> v</strong> <em>i</em><em>}</em> for notation simplicity.</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 73</p></div>
<div class="para"><p>In general, the cumulative average can be written using a recursive formula</p></div>
<div class="para"><p>Mem <em>i</em>=(<strong>k</strong> <em>i</em><em>,</em><strong> v</strong> <em>i</em>) +<em> i</em><em> ·</em> Mem <em>i</em><em>−</em>1</p></div>
<div class="para"><p><em>i</em> + 1(2.57)</p></div>
<div class="para"><p>where Mem <em>i</em> and Mem <em>i</em><em>−</em>1 denote the cumulative averages of the current and previous positions, respectively. An advantage of this model is that we only need to store a single key-value pair during inference, rather than storing all the key-value pairs. Note that the above memory models are related to recurrent models, and more advanced techniques have been used to develop alternatives to self-attention mechanisms in Transformers [Ma et al.,2023].</p></div>
<div class="para"><p>• The memory Mem can also be a neural network. At each step, it takes both the previous output of the memory and the current states of the model as input, and produces the new output of the memory. This neural network can be formulated as the function</p></div>
<div class="para"><p>Mem=Update(<em>S</em> kv<em>,</em> Mem pre)(2.58)</p></div>
<div class="para"><p>Here Mem and Mem pre represent the outputs of the memory at the current step and the previous step, respectively.<em> S</em> kv is a set of key-value pairs, representing the recent states of the model. This formulation is general and allows us to develop various memory models by selecting different Update(<em>·</em>) and<em> S</em> kv configurations. For example, if<em> S</em> kv only contains the latest key-value pair (<strong>k</strong> <em>i</em><em>,</em><strong> v</strong> <em>i</em>) and Update(<em>·</em>) is defined as a recurrent cell, then Eq. (2.58)can be expressed as an RNN-like model</p></div>
<div class="para"><p>Mem=<em>f</em>((<strong>k</strong> <em>i</em><em>,</em><strong> v</strong> <em>i</em>)<em>,</em> Mem pre)(2.59)</p></div>
<div class="para"><p>where<em> f</em>(<em>·</em>) is a recurrent cell. Recurrence can also be applied to segment-level modeling for efficiency consideration. A simple approach is that we can divide the sequence into segments, and treat<em> S</em> kv as a segment. Applying recurrent models to Update(<em>·</em>) will result in memory models that operate on segments. A special example is that we define Update(<em>·</em>) as an FIFO function that adds<em> S</em> kv into the memory and removes the oldest key-value segment from the memory, given by</p></div>
<div class="para"><p>Mem=FIFO(<em>S</em> kv<em>,</em> Mem pre)(2.60)</p></div>
<div class="para"><p>Consider a memory which includes two segments, one for current segment, and one for the previous segment. In the attention operation, each position can access the history key-value pairs in two closest consecutive segments. This essentially defines a local memory, but it and its variants have been widely used segment-level recurrent models [Dai et al., 2019;Hutchins et al., 2022; Bulatov et al., 2022].</p></div>
<div class="para"><p>• The above memory models can be extended to involve multiple memories. An example of this approach is compressive Transformer [Rae et al., 2019]. It employs two distinct fixed-size memories: one for modeling local context (denoted by Mem), and the other for modeling and compressing long-term history (denoted by CMem). The KV cache in this model is the combination of Mem and CMem. The attention function can be written as</p></div>
<div class="para"><p>Att com(<strong>q</strong> <em>i</em><em>,</em> Mem<em>,</em> CMem)=Att qkv(<strong>q</strong> <em>i</em><em>,</em> [Mem<em>,</em> CMem])(2.61)</p></div>
<div class="para"><p>74 <em>Generative Models</em></p></div>
<div class="para"><p>where [Mem<em>,</em> CMem] is a combined memory of Mem and CMem. As with other segmentlevel models, the compressive Transformer model operates on segments of the sequence.Each segment is a sequence of<em> n</em> <em>s</em> consecutive tokens, and we denote<em> S</em> <em>k</em> kv as the key-value pairs corresponding to the tokens of the<em> k</em>-th segment. When a new segment arrives, Mem is updated in an FIFO fashion: we append the<em> n</em> <em>c</em> key-value pairs in<em> S</em> <em>k</em> kv to Mem, and then pop the<em> n</em> <em>s</em> oldest key-value pairs from Mem, which is given by</p></div>
<div class="para"><p>Mem=FIFO(<em>S</em> <em>k</em> kv<em>,</em> Mem pre)(2.62)</p></div>
<div class="para"><p>The popped key-value pairs are then used to update the compressive memory CMem. These <em>n</em> <em>s</em> key-value pairs are compressed into<em> </em><em>n</em> <em>s</em></p></div>
<div class="para"><p><em>c</em> key-value pairs via a compression network.CMem is an FIFO which appends the compressed<em> </em><em>n</em> <em>s</em></p></div>
<div class="para"><p><em>c</em> key-value pairs to the tail of the queue, and drops the first<em> </em><em>n</em> <em>s</em></p></div>
<div class="para"><p><em>c</em> key-value pairs of the queue. It is given by</p></div>
<div class="para"><p>CMem=FIFO(<em>C</em> <em>k</em> kv<em>,</em> CMem pre)(2.63)</p></div>
<div class="para"><p>where<em> C</em> <em>k</em> kv represents the set of compressed key-value pairs. Implicit in the compressive Transformer model is that local context should be represented explicitly with minimal information loss, while long-range context can be more compressed.</p></div>
<div class="para"><p>• We have already seen that both global and local contexts are useful and can be modeled using attention models. This view motivates the extension to attention models for combining both local and long-term memories [Ainslie et al., 2020; Zaheer et al., 2020; Gupta and Berant, 2020]. A simple but widely-used approach is to involve the first few tokens of the sequence in attention, serving as global tokens. This approach is usually applied along with other sparse attention models. An advantage of incorporating global tokens of the sequence is that it helps smooth the output distribution of the Softmax function used in attention weight computation, and thus stabilizes model performance when the context size is very large [Xiao et al., 2024]. One drawback, however, is that using a fixed-size global memory may result in information loss. When dealing with long sequences, we need to enlarge the KV cache for sufficient representations of the context, but this in turn increases the computational cost.</p></div>
<div class="para"><p>Figure 2.6 shows illustrations of the above approaches. Note that, while we focus on optimization of the KV cache here, this issue is closely related to those discussed in the previous section.All of the methods we have mentioned so far can broadly be categorized as efficient attention approaches, which are widely used in various Transformer variants.</p></div>
<div class="para"><p><strong>2.3.3.2</strong> <strong>Memory-based Models</strong></p></div>
<div class="para"><p>The modeling of memories discussed above was based on updates to the KV cache, and the resulting models are typically referred to as<strong> internal memories</strong>. We now consider another family of models, called<strong> external memories</strong>, which operate as independent models to access large-scale contexts for LLMs. Many such models are based on<strong> memory-based methods</strong> which have been extensively discussed in machine learning [Bishop, 2006]. A common example is nearest neighbor algorithms: we store context representations in a datastore, and try to find the most similar</p></div>
<h2>2.3 Long Sequence Modeling 75</h2>
<h3>Size = 4 × 2 Memory</h3>
<h3>· ·</h3>
<div class="para"><p>Keys</p></div>
<h3>· ·</h3>
<div class="para"><p>Values</p></div>
<div class="para"><p><em>i</em> <em>i</em><em> −</em>1 <em>i</em><em> −</em>2 <em>i</em><em> −</em>3 <em>i</em><em> −</em>4 <em>i</em><em> −</em>5 <em>i</em><em> −</em>6 <em>i</em><em> −</em>7</p></div>
<h3>(a) Window-based Cache</h3>
<h3>Size = 1 × 2 Memory k i−3+k i−2+k i−1+k i</h3>
<div class="para"><p>4<em>⇒</em></p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>−</em>3+<strong>v</strong> <em>i</em><em>−</em>2+<strong>v</strong> <em>i</em><em>−</em>1+<strong>v</strong> <em>i</em></p></div>
<div class="para"><p>4<em>⇒</em></p></div>
<h3>· ·</h3>
<div class="para"><p>Keys</p></div>
<h3>· ·</h3>
<div class="para"><p>Values</p></div>
<div class="para"><p><em>i</em> <em>i</em><em> −</em>1 <em>i</em><em> −</em>2 <em>i</em><em> −</em>3 <em>i</em><em> −</em>4 <em>i</em><em> −</em>5 <em>i</em><em> −</em>6 <em>i</em><em> −</em>7</p></div>
<h3>(b) Moving Average-based Cache</h3>
<h3>Size = 1 × 2 Memory Mem = Update( S kv , Mem pre) ⇒</h3>
<h3>· ·</h3>
<div class="para"><p>Keys</p></div>
<h3>· ·</h3>
<div class="para"><p>Values</p></div>
<div class="para"><p><em>i</em> <em>i</em><em> −</em>1 <em>i</em><em> −</em>2 <em>i</em><em> −</em>3 <em>i</em><em> −</em>4 <em>i</em><em> −</em>5 <em>i</em><em> −</em>6 <em>i</em><em> −</em>7</p></div>
<h3>(c) Recurrent Network as Cache</h3>
<h3>Memory Compressed</h3>
<h3>Size = 4 × 2 Memory Size = 2 × 2</h3>
<h3>· ·</h3>
<div class="para"><p>Keys</p></div>
<h3>· ·</h3>
<div class="para"><p>Values</p></div>
<div class="para"><p><em>i</em> <em>i</em><em> −</em>1 <em>i</em><em> −</em>2 <em>i</em><em> −</em>3 <em>i</em><em> −</em>4 <em>i</em><em> −</em>5 <em>i</em><em> −</em>6 <em>i</em><em> −</em>7</p></div>
<h3>(d) Hybrid Cache (Compressed Memory + Local Memory)</h3>
<div class="para"><p><strong>Fig. 2.6:</strong> Illustrations of fixed-size KV caches in LLMs. Blue boxes represent the keys and values generated during LLM inference, green boxes represent the keys and values stored or encoded in the primary memory, and orange boxes represent the keys and values stored or encoded in the compressed memory.</p></div>
<h2>stored representations to match a given query. The retrieved context representations are then used to improve attention for this query.</h2>
<div class="para"><p>76 <em>Generative Models</em></p></div>
<div class="para"><p>Here, we consider the<em> k</em><strong>-nearest neighbors</strong> (<em>k</em><strong>-NN</strong>) method which is one of the most popular memory-based methods. Since our focus is language modeling in this section, we define a sample in the datastore as a key-value pair corresponding to some context state. Note that “context” is a broad concept here, not just a sequence prefix in text generation. One might, for example, view the entire dataset as the context for predicting tokens. This allows us to retrieve the closest context situation in a set of sequences, rather than a given sequence prefix. Although we will restrict ourselves to context modeling for a single sequence, in this subsection, we discuss a relatively more general case.</p></div>
<div class="para"><p>Suppose we have a set of keys<em> {</em><strong>k</strong> <em>j</em><em>}</em> with corresponding values<em> {</em><strong>v</strong> <em>j</em><em>}</em>, and suppose we store these key-value pairs in a vector database 12. For each query<strong> q</strong> <em>i</em>, we find its<em> k</em> nearest neighbours by growing the radius of the sphere centered as<strong> q</strong> <em>i</em> until it contains<em> k</em> data points in<em> {</em><strong>k</strong> <em>j</em><em>}</em>. This results in a set of<em> k</em> keys along with their corresponding values, denoted by Mem <em>k</em> nn. As before, we denote Mem as the local memory for the query, such as the KV cache of neighboring tokens. Our goal is to attend query<strong> q</strong> <em>i</em> to both the local memory Mem and the long-term memory Mem <em>k</em> nn.There are, of course, several ways to incorporate Mem and Mem <em>k</em> nn into the attention model.For example, we might simply combine them to form a single KV cache [Mem<em>,</em> Mem <em>k</em> nn], and attend<strong> q</strong> <em>i</em> to [Mem<em>,</em> Mem <em>k</em> nn] via standard QKV attention. Or we might use Mem and Mem <em>k</em> nn in separate attention steps. An example of such approaches is the model developed by Wu et al.[2021]. It linearly combines the two types of attention, given by</p></div>
<div class="para"><p>Att(<strong>q</strong> <em>i</em><em>,</em> Mem<em>,</em> Mem <em>k</em> nn)=<strong>g</strong><em> ⊙</em>Att local + (1<em> −</em><strong>g</strong>)<em> ⊙</em>Att <em>k</em> nn(2.64)</p></div>
<div class="para"><p>Att local=Att(<strong>q</strong> <em>i</em><em>,</em> Mem)(2.65)</p></div>
<div class="para"><p>Att <em>k</em> nn=Att(<strong>q</strong> <em>i</em><em>,</em> Mem <em>k</em> nn)(2.66)</p></div>
<div class="para"><p>Here<strong> g</strong><em> ∈</em>R <em>d</em> is the coefficient vector, which can be the output of a learned gate.</p></div>
<div class="para"><p>Given the<em> k</em>-NN-based memory model described above, the remaining task is to determine which key-value pairs are retained in the datastore. For standard language modeling tasks, we consider the previously seen tokens in a sequence as the context, so we can add the keys and values of all these tokens into the datastore. In this case, the resulting<em> k</em>-NN-based attention model is essentially equivalent to a sparse attention model [Gupta et al., 2021].</p></div>
<div class="para"><p>Alternatively, we can extend the context from one sequence to a collection of sequences.For example, we might collect all key-value pairs across the sequences in a training dataset and add them to the datastore to model a larger context. Thus, LLMs can predict tokens based on a generalized context. A problem with this approach is that the computational cost would be large if many sequences are involved. Since these sequences are part of our training data, we can build and optimize an index for the vectors in the datastore before running the LLMs. As a result, the retrieval of similar vectors can be very efficient, as in most vector databases.</p></div>
<div class="para"><p>In fact, all the above-mentioned methods can be viewed as instances of a retrieval-based approach. Instead of using retrieval results to improve attention, we can apply this approach in other ways as well. One application of<em> k</em>-NN-based search is<em> k</em><strong>-NN language modeling</strong> (or<em> k</em><strong>-NN LM</strong>)[Khandelwal et al., 2020]. The idea is that, although it is attempting to extend the context used in self-attention by incorporating nearest neighbors in representation learning, in practice, similar hidden states in Transformers are often highly predictive of similar tokens in subsequent positions.</p></div>
<div class="para"><p>12 A vector database, or vector store, is a database that provides highly optimized retrieval interfaces for finding stored vectors that closely match a query vector.</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 77</p></div>
<div class="para"><p>In<em> k</em>-NN LM, each item in the datastore is a key-value tuple (<strong>z</strong><em>, w</em>), where<strong> z</strong> represents a hidden state of the LLM at a position, and<em> w</em> represents the corresponding prediction. A typical way to create the datastore is to collect the output vector of the Transformer layer stack and the corresponding next token for each position of each sequence in a training dataset. During inference,we have a representation<strong> h</strong> <em>i</em> given a prefix. Given this representation, we first search the datastore for<em> k</em> closest matching data items<em> {</em>(<strong>z</strong> 1<em>, w</em> 1)<em>, ...,</em> (<strong>z</strong> <em>k</em><em>, w</em> <em>k</em>)<em>}</em>. Here<em> {</em><em>w</em> 1<em>, ..., w</em> <em>k</em><em>}</em> are thought of as reference tokens for prediction, and thus can be used to guide the token prediction based on<strong> h</strong> <em>i</em>.One common way to make use of reference tokens is to define a distribution over the vocabulary <em>V</em> ,</p></div>
<div class="para"><p>Pr <em>k</em> nn(<em>·|</em><strong>h</strong> <em>i</em>)=Softmax(h<em>−</em><em>d</em> 0<em>· · ·</em><em>−</em><em>d</em><em>|</em><em>V</em><em> |</em>i)(2.67)</p></div>
<div class="para"><p>where<em> d</em> <em>v</em> equals the distance between<strong> h</strong> <em>i</em> and<strong> z</strong> <em>j</em> if<em> w</em> <em>j</em> equals the<em> v</em>-th entry of<em> V</em> , and equals 0 otherwise. We use a linear function with a coefficient<em> λ</em> that interpolates between the retrievalbased distribution Pr <em>k</em> nn(<em>·|</em><strong>h</strong> <em>i</em>) and the LLM output distribution Pr lm(<em>·|</em><strong>h</strong> <em>i</em>)</p></div>
<div class="para"><p>Pr(<em>·|</em><strong>h</strong> <em>i</em>)=<em>λ</em><em> ·</em> Pr <em>k</em> nn(<em>·|</em><strong>h</strong> <em>i</em>) + (1<em> −</em><em>λ</em>)<em> ·</em> Pr lm(<em>·|</em><strong>h</strong> <em>i</em>)(2.68)</p></div>
<div class="para"><p>Then, as usual, we can choose the next token<em> y</em> by maximizing the probability Pr(<em>y</em><em>|</em><strong>h</strong> <em>i</em>).</p></div>
<div class="para"><p>As with information retrieval (IR) systems, the datastore can also manage texts and provide access to relevant texts for a query. For example, we can store a collection of text documents in a search engine with full-text indexing, and then search it for documents that match a given text-based query. Applying IR techniques to LLMs leads to a general framework called<strong> retrieval-</strong><strong>augmented generation</strong> (<strong>RAG</strong>). The RAG framework works as follows. We use the context<strong> x</strong> as the query and find the<em> k</em> most relevant document pieces<em> {</em><strong>c</strong> 1<em>, ...,</em><strong> c</strong> <em>k</em><em>}</em> from the datastore via efficient IR techniques 13. These search results are combined with the original context via a prompting template<em> g</em>(<em>·</em>)14, resulting in an augmented input for the LLM</p></div>
<div class="para"><p><strong>x</strong><em>′</em>=<em>g</em>(<strong>c</strong> 1<em>, ...,</em><strong> c</strong> <em>k</em><em>,</em><strong> x</strong>)(2.69)</p></div>
<div class="para"><p>Then, we use<strong> x</strong><em>′</em> as the context and predict the following text using the model Pr(<strong>y</strong><em>|</em><strong>x</strong><em>′</em>). One advantage of RAG is that we need not modify the architecture of LLMs, but instead augment the input to LLMs via an additional IR system. Figure 2.7 shows a comparison of the use of different external memories in LLMs.</p></div>
<div class="para"><p>13 In piratical applications, queries are typically generated using a query generation system, which may expand it with variations of tokens and query intent.14 For example, the template could be:</p></div>
<div class="para"><p>message = {*<strong>c</strong> 1*} ... {*<strong>c</strong> <em>k</em>*}</p></div>
<div class="para"><p>input: {*<strong>x</strong>*}</p></div>
<div class="para"><p>output:</p></div>
<h3>78 Generative Models</h3>
<div class="para"><p><strong>g</strong><em> ⊙</em>Att(<strong>q</strong> <em>i</em><em>,</em> Mem) + (1<em> −</em><strong>g</strong>)<em> ⊙</em>Att(<strong>q</strong> <em>i</em><em>,</em> Mem <em>k</em> nn)</p></div>
<div class="para"><p>Att(<strong>q</strong> <em>i</em><em>,</em> Mem)Att(<strong>q</strong> <em>i</em><em>,</em> Mem <em>k</em> nn)</p></div>
<ul><li>· ·</li><li>· ·</li></ul>
<h3>q i</h3>
<div class="para"><p>KV Cache <em>k</em> Nearest Neighbors</p></div>
<div class="para"><p>Keys/values in LLM</p></div>
<div class="para"><p>Datastore</p></div>
<div class="para"><p>Search</p></div>
<div class="para"><p>Keys/values in Datastore</p></div>
<div class="para"><p>(a)<em> k</em>-NN Search Augmented Attention</p></div>
<div class="para"><p>Output Distribution</p></div>
<div class="para"><p>Distribution Pr(<em>·</em>)</p></div>
<div class="para"><p>Distribution Pr <em>k</em> nn(<em>·</em>)</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p>Att(<strong>q</strong> <em>i</em><em>,</em> Mem)Att(<strong>q</strong> <em>i</em><em>,</em> Mem)</p></div>
<ul><li>· ·</li><li>· ·</li></ul>
<h3>q i</h3>
<div class="para"><p>KV Cache <em>k</em> Nearest Neighbors</p></div>
<div class="para"><p>Keys/values in LLM</p></div>
<div class="para"><p>Keys in Datastore</p></div>
<div class="para"><p>Datastore</p></div>
<div class="para"><p>Search</p></div>
<div class="para"><p>Predicted Tokens</p></div>
<div class="para"><p>(b)<em> k</em>-NN Language Modeling</p></div>
<div class="para"><p>LLM</p></div>
<div class="para"><p><strong>c</strong> 1 = Deep network is ...</p></div>
<div class="para"><p>Message: deep network ... machine learning ...</p></div>
<div class="para"><p><strong>c</strong> 2 = Machine learning is ...</p></div>
<div class="para"><p>What is deep learning?</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><em>k</em> Nearest Neighbors</p></div>
<div class="para"><p><strong>x</strong> = What is deep learning?Datastore Search</p></div>
<div class="para"><p>Input Context:</p></div>
<div class="para"><p>(c) Retrieval-augmented Generation</p></div>
<div class="para"><p><strong>Fig. 2.7:</strong> Illustrations of external memories (or datastores) for language modeling.</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 79</p></div>
<div class="para"><p><strong>2.3.3.3</strong> <strong>Memory Capacity</strong></p></div>
<div class="para"><p>A memory model in LLMs, in the form of a simple key-value cache or a datastore, can broadly be seen as an encoder of contextual information. Ideally, before we say that a memory model is representative of the entire context in token prediction, we need to make sure that the model can accurately represent any part of the context. The standard KV cache is one such model that completely stores all past history. In this case, the model is said to have adequate capacity for memorizing the context. In many practical applications, however, complete memorization is not required. Instead, the goal is to enable LLMs to access important contextual information. As a result, efficient and compressed memory models are developed, as described in this section. Note that, the longer the sequence, the more difficult it becomes for a low-capacity memory model to capture important contextual information. It is therefore common practice to simply increase the model capacity when processing long contexts.</p></div>
<div class="para"><p>While high-capacity models are generally favorable, they are difficult to train and deploy. A challenging scenario is that the tokens arrive in a stream and the context continuously grows.Developing LLMs for such tasks is difficult as we need to train Transformers on extremely long sequences. A possible way to address this difficulty is to use non-parametric methods, such as retrieval-based methods. For example, as discussed above, we can use a vector database to store previously generated key-value pairs, and thus represent the context by this external memory model. Although this approach side-steps the challenge of representing long context in Transformers, building and updating external memory models are computationally expensive. These models are more often used in problems where the context is given in advance and fixed during inference, and hence unsuitable for streaming context modeling.</p></div>
<div class="para"><p>In cases where the size of the context continuously grows, applying fixed-size memory models is a commonly used approach. For example, in recurrent models, a sequence of arbitrary length can be summarized into a set of hidden states by which we have a fixed computational cost per step.While recurrent models were initially found to be not very good at handling long-distance dependencies in sequence modeling in early applications of deep learning to NLP, recent advancements have shown that their variants are now effective in modeling extremely long sequences. [Bulatov et al., 2022; Hutchins et al., 2022; Munkhdalai et al., 2024; Ma et al., 2024].</p></div>
<div class="para"><p>There is no general definition of memory capacity in LLMs. A simple approach might consider how much storage is used to retain contextual information. For example, memory capacity could be defined by the size of the KV cache in Transformers or the vector database used in retrievalbased methods. A related concept is model complexity. In machine learning, there are several ways to define the model complexity of a model. One of the simplest methods is by counting the number of parameters. However, it should be emphasized that the memory models discussed here primarily serve to store information, rather than add trainable parameters. Therefore, a model with a large memory capacity is not necessarily more complex. Nevertheless, in practice determining the capacity of a memory model is not straightforward. In general, we need to control the trade-off between maximizing the performance and controlling the memory footprint.</p></div>
<div class="para"><p><strong>2.3.4</strong> <strong>Sharing across Heads and Layers</strong></p></div>
<div class="para"><p>In Transformers, the KV cache is a data structure that can be dynamically adjusted along multiple dimensions, such as heads, layers, and sequence length. For example, consider an LLM with<em> L</em> layers. Each layer has<em> τ</em> attention heads, and each head produces a<em> d</em> <em>h</em>-dimensional output. During</p></div>
<div class="para"><p>80 <em>Generative Models</em></p></div>
<div class="para"><p>inference, we store the keys and values for up to<em> m</em> tokens. The space complexity of this caching mechanism is<em> O</em>(<em>L</em><em> ·</em><em> τ</em><em> ·</em><em> d</em> <em>h</em><em> ·</em><em> m</em>). As we have seen previously, this complexity can be reduced by caching the keys and values for fewer tokens. For example, in sliding window attention, a fixedsize window is used to cache the keys and values in local context. And this model has a space complexity of<em> O</em>(<em>L</em><em> ·</em><em> τ</em><em> ·</em><em> d</em> <em>h</em><em> ·</em><em> m</em> <em>w</em>), with<em> m</em> <em>w</em> being the size of the window.</p></div>
<div class="para"><p>In addition to reducing<em> m</em>, we can also decrease the size of the KV cache along other dimensions. A widely-used approach is to enable sharing across heads in multi-head self-attention.Recall from Section 2.1.1 that multi-head self-attention uses multiple sets of queries, keys, and values (each set is called a head), each performing the QKV attention mechanism as usual. This can be expressed as</p></div>
<div class="para"><p>Output=Merge(head 1<em>, ...,</em> head <em>τ</em>)<strong>W</strong> head(2.70)</p></div>
<div class="para"><p>where head <em>j</em><em> ∈</em>R <em>d</em> <em>h</em> is computed using the standard QKV attention function</p></div>
<div class="para"><p>head <em>j</em> = Att qkv(<strong>q</strong>[<em>j</em>]<em>i</em><em> </em><em>,</em><strong> K</strong>[<em>j</em>]<em>≤</em><em>i</em><em>,</em><strong> V</strong>[<em>j</em>]<em>≤</em><em>i</em>)(2.71)</p></div>
<div class="para"><p>Here,<strong> q</strong>[<em>j</em>]<em>i</em> ,<strong> K</strong>[<em>j</em>]<em>≤</em><em>i</em>, and<strong> V</strong>[<em>j</em>]<em>≤</em><em>i</em> are the query, keys, and values that are projected onto the<em> j</em>-th feature sub-space. So this model can be interpreted as performing attention on a group of feature subspaces in parallel (see Figure 2.8 (b)). The KV cache needs to retain the keys and values for all these heads, that is,<em> {</em>(<strong>K</strong>[1]<em>≤</em><em>i</em><em>,</em><strong> V</strong>[1]<em>≤</em><em>i</em>)<em>, ...,</em> (<strong>K</strong>[<em>τ</em>]<em>≤</em><em>i</em><em>,</em><strong> V</strong>[<em>τ</em>]<em>≤</em><em>i</em>)<em>}</em>.</p></div>
<div class="para"><p>One refinement to the multi-head attention model, called<strong> multi-query attention</strong> (<strong>MQA</strong>), is to share keys and values across heads, while allowing queries to be unique for each head [Shazeer,2019]. In MQA, there is a single set of keys and values (<strong>K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>). In addition, there are<em> τ</em> queries<em> {</em><strong>q</strong>[1]<em>i</em><em> </em><em>, ...,</em><strong> q</strong>[<em>τ</em>]<em>i</em><em> </em><em>}</em>, each corresponding to a different head. For each head, we have</p></div>
<div class="para"><p>head <em>j</em> = Att qkv(<strong>q</strong>[<em>j</em>]<em>i</em><em> </em><em>,</em><strong> K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>)(2.72)</p></div>
<div class="para"><p>Figure 2.8 (c) illustrates this model. By sharing keys and values, the size of the KV cache would be<em> O</em>(<em>L</em><em> ·</em><em> d</em> <em>h</em><em> ·</em><em> m</em>).</p></div>
<div class="para"><p><strong>Grouped query attention</strong> (<strong>GQA</strong>) is a natural extension to multi-head attention and MQA[Ainslie et al., 2023]. In GQA, heads are divided into<em> n</em> <em>g</em> groups, each corresponding to a shared set of keys and values. Hence we have<em> n</em> <em>g</em> sets of keys and values<em> {</em>(<strong>K</strong>[1]<em>≤</em><em>i</em><em>,</em><strong> V</strong>[1]<em>≤</em><em>i</em>)<em>, ...,</em> (<strong>K</strong>[<em>n</em> <em>g</em>]<em>≤</em><em>i</em><em> </em><em>,</em><strong> V</strong>[<em>n</em> <em>g</em>]<em>≤</em><em>i</em> )<em>}</em>.See Figure 2.8 (d) for an illustration. Let<em> g</em>(<em>j</em>) be the group id for the<em> j</em>-th head. The GQA model can be expressed as</p></div>
<div class="para"><p>head <em>j</em> = Att qkv(<strong>q</strong>[<em>j</em>]<em>i</em><em> </em><em>,</em><strong> K</strong>[<em>g</em>(<em>j</em>)]<em>≤</em><em>i</em><em>,</em><strong> V</strong>[<em>g</em>(<em>j</em>)]<em>≤</em><em>i</em>)(2.73)</p></div>
<div class="para"><p>The size of the KV cache of GQA is<em> O</em>(<em>L</em><em>·</em><em>n</em> <em>g</em><em> ·</em><em>d</em> <em>h</em><em>·</em><em>m</em>). One benefit of GQA is that we can trade-off between computational efficiency and model expressiveness by adjusting<em> n</em> <em>g</em>. When<em> n</em> <em>g</em> =<em> τ</em>, the model becomes the standard multi-head attention model. By contrast, when<em> n</em> <em>g</em> = 1, it becomes the GQA model.</p></div>
<div class="para"><p>Sharing can also be performed across layers. Such a method falls into the family of shared weight and shared activation methods, which have been extensively used in Transformers [Dehghani et al., 2018; Lan et al., 2020]. For example, one can share KV activations or attention</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 81</p></div>
<div class="para"><p>query key value</p></div>
<div class="para"><p>query key value</p></div>
<div class="para"><p>(a) Single-head Attention</p></div>
<div class="para"><p>(b) Multi-head Attention</p></div>
<div class="para"><p>query key value</p></div>
<div class="para"><p>query key value</p></div>
<div class="para"><p>(c) Multi-query Attention</p></div>
<div class="para"><p>(d) Grouped Query Attention</p></div>
<div class="para"><p>query key value</p></div>
<div class="para"><p>Layer<em> l</em></p></div>
<div class="para"><p>Sharing</p></div>
<div class="para"><p>Layer<em> l</em><em> −</em>1</p></div>
<div class="para"><p>(e) Cross-layer Multi-head Attention</p></div>
<div class="para"><p><strong>Fig. 2.8:</strong> Illustration of QKV attention based on different multi-head and sharing mechanisms. (a) = single-head attention, and (b-e) = attention with multiple heads.</p></div>
<div class="para"><p>weights across layers to reduce both computation and memory footprints [Xiao et al., 2019; Brandon et al., 2024]. Figure 2.8 (e) shows an illustration of this method, where a query in a layer directly accesses the KV cache of a lower-level layer.</p></div>
<h3>2.3.5 Position Extrapolation and Interpolation</h3>
<div class="para"><p>Since Transformer layers are order-insensitive to input, we need some way to encode positional information in the input tokens. To do this, it is common to add positional embeddings to token embeddings, and then feed these combined embeddings into the Transformer layer stack as input.In this case, the embedding at position<em> i</em> can be expressed as</p></div>
<div class="para"><p><strong>e</strong> <em>i</em>=<strong>x</strong> <em>i</em> + PE(<em>i</em>)(2.74)</p></div>
<div class="para"><p>82 <em>Generative Models</em></p></div>
<div class="para"><p>where<strong> x</strong> <em>i</em><em> ∈</em>R <em>d</em> denotes the token embedding, and PE(<em>i</em>)<em> ∈</em>R <em>d</em> denotes the positional embedding.In general, the token embedding<strong> x</strong> <em>i</em> is a position-independent vector, and so the positional embedding PE(<em>i</em>) is used to encode the positional context. A straightforward approach is to treat PE(<em>i</em>)as a learnable variable and train it alongside other model parameters. In this way, we can learn a unique representation for each position, and thus distinguish the tokens appearing at different positions of a sequence.</p></div>
<div class="para"><p>Representations of positions using learned vectors can work well in tasks where the sequences at training and test times are of similar lengths. In practice, however, we often impose length restrictions on sequences during training to prevent excessive computational costs, but wish to apply the trained models to much longer sequences during inference. In this case, using learned positional embeddings has obvious drawbacks, as there are no trained embeddings for positions that are not observed in the training phase.</p></div>
<div class="para"><p>An alternative approach to modeling positional information is to develop positional embeddings that can generalize: once trained, the embedding model can be used to handle longer sequences. Suppose that we train a positional embedding model on sequences with a maximum length of<em> m</em> <em>l</em>, and we wish to apply the trained model to a sequence of length<em> m</em> (<em>m &gt;&gt; m</em> <em>l</em>). If the embedding model is limited in the range of positions that we can observe from training data,then this model will simply fail to deal with new data outside that range. See Figure 2.9 (a) for an illustration where the learned embedding model cannot model data points outside the training domain if it lacks the ability to extrapolate.</p></div>
<div class="para"><p>There are several approaches to making positional embedding models generalize. They can be grouped into two classes.</p></div>
<div class="para"><p>•<strong> Extrapolation</strong>. The model learned on observed data points (i.e., positions) can be directly employed to assign meaningful values to data points beyond the original range. For example, suppose we have a series of numbers 1<em>,</em> 2<em>, ...,</em> 10, and we want to understand the meaning of a new number, 15. Knowing that these numbers are natural numbers used for ordering, we can easily infer that 15 is a number that follows 10, even though 15 has not been observed before. Figure 2.9 (b) shows an example of this approach, where a function is learned to fit the data points within a specific range and then applied to estimate the values of data points outside that range.</p></div>
<div class="para"><p>•<strong> Interpolation</strong>. This approach maps a larger range of data points into the original observation range. For example, suppose we have a model designed for numbers in the range[1<em>,</em> 10]. When given a new range of [1<em>,</em> 20], we can scale this down by dividing every number by 2, thereby fitting all numbers into [1<em>,</em> 10]. This scaling allows us to use the model trained on the range [1<em>,</em> 10] to describe data points in the expanded range of [1<em>,</em> 20]. See Figure 2.9 (c) for an illustration of this approach.</p></div>
<div class="para"><p>In fact, positional embeddings in many systems have achieved some level of generalization.For example, sinusoidal encoding, the most common positional embedding method, employs sine and cosine functions that can naturally extend to sequences of any length. Although this approach might seem direct and simple, it does not perform well when we significantly extend the sequences for processing. In this subsection, we will discuss several alternative methods based on either extrapolation or interpolation.</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 83</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>Value</p></div>
<div class="para"><p>0</p></div>
<div class="para"><p>0 1<em>,</em>024 2<em>,</em>048<em>−</em>1</p></div>
<div class="para"><p>Sequence Length(a) Encoding with No Generalization</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>Value</p></div>
<div class="para"><p>0</p></div>
<div class="para"><p>0 1<em>,</em>024 2<em>,</em>048<em>−</em>1</p></div>
<div class="para"><p>Sequence Length(b) Extrapolation</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>Value</p></div>
<div class="para"><p>0</p></div>
<div class="para"><p>0 1<em>,</em>024 2<em>,</em>048<em>−</em>1</p></div>
<div class="para"><p>Sequence Length(c) Interpolation</p></div>
<div class="para"><p><strong>Fig. 2.9:</strong> Illustrations of different positional embedding methods for a range of positions. Blue points represent the positions that have been observed during training, and red points represent the positions that are newly observed at test time. In sub-figure (a), the encoding model only memorizes the points seen during training, and cannot generalize. In sub-figures (b) and (c), the model can generalize through extrapolation and interpolation.</p></div>
<div class="para"><p><strong>2.3.5.1</strong> <strong>Attention with Learnable Biases</strong></p></div>
<div class="para"><p>One problem with Eq. (2.74) is that the embedding model treats each token independently and therefore ignores the distance between different tokens. A common improvement to this model,called relative positional embedding, is to consider the pairwise relationship between tokens[Shaw et al., 2018]. The general idea behind this is to obtain the offset between any pair of positions and incorporate it into the self-attention model. One of the simplest forms of self-attention with relative positional embedding is given by</p></div>
<div class="para"><p><em>i</em> X</p></div>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>,</em><strong> K</strong><em>≤</em><em>i</em><em>,</em><strong> V</strong><em>≤</em><em>i</em>)=</p></div>
<div class="para"><p><em>j</em>=0 <em>α</em>(<em>i, j</em>)<strong>v</strong> <em>j</em>(2.75)</p></div>
<div class="para"><p><em>α</em>(<em>i, j</em>)=Softmax(<strong>q</strong> <em>i</em> <strong>k</strong> T <em>j</em> + PE(<em>i, j</em>)<em>√</em></p></div>
<div class="para"><p><em>d</em>+ Mask(<em>i, j</em>))(2.76)</p></div>
<div class="para"><p>The only difference between this model and the original self-attention model is that a bias term PE(<em>i, j</em>) is added to the query-key product in this new model. Intuitively, PE(<em>i, j</em>) can be interpreted as a distance penalty for the pair of positions<em> i</em> and<em> j</em>. As<em> i</em> moves away from<em> j</em>, the value of</p></div>
<div class="para"><p>84 <em>Generative Models</em></p></div>
<div class="para"><p>PE(<em>i, j</em>) decreases.</p></div>
<div class="para"><p>PE(<em>i, j</em>) can be defined in several different ways. Here, we consider the T5 version of relative positional embedding, called the T5 bias [Raffel et al., 2020]. For each pair of query<strong> q</strong> <em>i</em> and key <strong>k</strong> <em>j</em>, the offset between them is defined to be 15</p></div>
<div class="para"><p><em>d</em>(<em>i, j</em>)=<em>i</em><em> −</em><em>j</em>(2.77)</p></div>
<div class="para"><p>A simple design for the bias PE(<em>i, j</em>) is to share the same learnable variable for all query-key pairs with the same offset, i.e., PE(<em>i, j</em>) =<em> u</em> <em>i</em><em>−</em><em>j</em>, where<em> u</em> <em>i</em><em>−</em><em>j</em> is the variable corresponding to the offset<em> i</em><em> −</em><em>j</em>. However, simply assigning a unique value to each offset will restrict this model to observed offsets. When<em> i</em><em> −</em><em>j</em> is larger than the maximum trained offset, the model cannot generalize.</p></div>
<div class="para"><p>The T5 bias instead adopts a generalization of this model. Rather than assigning each querykey offset a unique bias term, it groups difference offsets into “buckets”, each corresponding to one learnable parameter. More specifically, the bias terms for<em> n</em> <em>b</em> + 1 buckets are given as follows.</p></div>
<div class="para"><p>• For buckets 0 to<em> </em><em>n</em> <em>b</em>+1</p></div>
<div class="para"><p>2<em>−</em>1, each bucket corresponds to one offset, that is, bucket 0<em> ↔</em>offset 0, bucket 1<em> ↔</em>offset 1, bucket 2<em> ↔</em>offset 2, and so on. We express this as<em> b</em>(i<em> −</em>j) =<em> i</em><em> −</em><em>j</em>.</p></div>
<div class="para"><p>• For buckets<em> </em><em>n</em> <em>b</em>+1</p></div>
<div class="para"><p>2 to<em> n</em> <em>b</em>, the size of each bucket increases logarithmically. For example, the bucket number for a given offset<em> i</em><em> −</em><em>j</em><em> ≥</em><em>n</em> <em>b</em>+1</p></div>
<div class="para"><p>2 can be defined as</p></div>
<div class="para"><p>2+<em> ⌊</em>log(<em>i</em><em> −</em><em>j</em>)<em> −</em>log(<em>n</em> <em>b</em>+1</p></div>
<div class="para"><p>2)</p></div>
<div class="para"><p><em>b</em>(<em>i</em><em> −</em><em>j</em>)=<em>n</em> <em>b</em> + 1</p></div>
<div class="para"><p>2)<em> </em><em>·</em><em> n</em> <em>b</em> + 1</p></div>
<div class="para"><p>2<em>⌋</em>(2.78)</p></div>
<div class="para"><p>log(dist max)<em> −</em>log(<em>n</em> <em>b</em>+1</p></div>
<div class="para"><p>where the parameter dist max is typically set to a relatively large number to indicate the maximum offset we may encounter.</p></div>
<div class="para"><p>• When<em> i</em><em> −</em><em>j &gt;</em> dist max, we place<em> i</em><em> −</em><em>j</em> in the last bucket. In other words, bucket<em> n</em> <em>b</em> contains all the offsets that are not assigned to the previous buckets.</p></div>
<div class="para"><p>Together, these can be expressed as the function</p></div>
<div class="para"><p><em>b</em>(<em>i</em><em> −</em><em>j</em>)</p></div>
<div class="para"><p></p></div>
<div class="para"><p><em>i</em><em> −</em><em>j</em> 0<em> ≤</em><em>i</em><em> −</em><em>j &lt;</em><em> </em><em>n</em> <em>b</em>+1</p></div>
<div class="para"><p>2 min(<em>n</em> <em>b</em><em>,</em><em> </em><em>n</em> <em>b</em>+1</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>(2.79)</p></div>
<div class="para"><p>2+<em> ⌊</em>log(<em>i</em><em>−</em><em>j</em>)<em>−</em>log(<em> </em><em>nb</em>+1</p></div>
<div class="para"><p>2)</p></div>
<div class="para"><p>2)<em> </em><em>·</em><em> n</em> <em>b</em>+1</p></div>
<div class="para"><p>2<em>⌋</em>)<em>i</em><em> −</em><em>j</em><em> ≥</em><em>n</em> <em>b</em>+1</p></div>
<div class="para"><p></p></div>
<div class="para"><p>log(dist max)<em>−</em>log(<em> </em><em>nb</em>+1</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>Figure 2.10 shows an illustration of these buckets. We see that in the first half of the buckets,each bucket is associated with only one value of<em> i</em><em> −</em><em>j</em>, while in the second half, the bucket size increases as<em> i</em><em> −</em><em>j</em> grows. The last bucket is designed to handle sequences of arbitrarily long lengths.</p></div>
<div class="para"><p>15 For language modeling, a query is only allowed to attend to its left-context, and so we have<em> i</em><em> −</em><em>j</em><em> ≥</em>0. In the more general case of self-attention, where a token can attend to all tokens in the sequence, we may have negative offsets when<em> i &lt; j</em>.</p></div>
<h3>2.3 Long Sequence Modeling 85</h3>
<div class="para"><p>fixed bucket size logarithmically increased bucket size</p></div>
<h3>0 1 2 3 · · 14 15 16 17 18 · · 32 Bucket</h3>
<div class="para"><p>Offset(<em>i</em><em> −</em><em>j</em>)</p></div>
<div class="para"><p>0 1 2 3 14 15 16<em> ∼</em>20 21<em> ∼</em>26 27<em> ∼</em>33 802<em> ∼∞</em></p></div>
<div class="para"><p><strong>Fig. 2.10:</strong> Illustration of distributing query-key offsets into buckets in the T5 model (<em>n</em> <em>b</em> = 32 and dist max = 1024).Boxes represent buckets. In the first half of the buckets, we use a fixed bucket size. In the second half of the buckets,we increase the bucket size logarithmically. The last bucket contains all the query-key offsets that are not covered by previous buckets.</p></div>
<h3>All PE(i, j)s in a bucket share the same bias term u b(i−j). Substituting PE(i, j) = u b(i−j) into Eq. (2.76), the attention weight for q i and k j becomes 16</h3>
<h3>α(i, j) = Softmax(q i k T j + u b(i−j) √</h3>
<h3>d + Mask(i, j)) (2.81)</h3>
<h3>The parameters {u 0, ..., u n b} are learned as common parameters during training. It should be emphasized that this model can generalize to long sequences. This is because PE(i, j)s with similar query-key offsets share the same parameter, and this sharing strategy is particularly im- portant for achieving good generalization, given that large query-key offsets are rare in training. In practice, we often set n b to a moderate number, and thus it can help control the overfitting of positional embedding models.</h3>
<h3>2.3.5.2 Attention with Non-learned Biases</h3>
<h3>Relative positional embedding models are based on a set of learned biases for the query-key prod- uct in self-attention. An alternative approach is to give these biases fixed values via heuristics, rather than training them on a particular dataset. One benefit of this heuristics-based approach is that it does not rely on a training process and thus can be directly applied to any sequences once the biases are set.</h3>
<h3>One example of such an approach is Press et al. [2022]’s approach, called attention with linear biases or ALiBi for short. In the ALiBi approach, the bias term is defined as the negative scaled query-key offset</h3>
<h3>PE(i, j) = −β · (i −j)</h3>
<h3>= β · (j −i) (2.82)</h3>
<h3>where β is the scaling factor. Adding this term to the query-key product, we obtain a new form of</h3>
<div class="para"><p>16 Note that, in Raffel et al. [2020]’s T5 model, the rescaling operation for the query-key product is removed. The attention weight<em> α</em>(<em>i, j</em>) is then given by</p></div>
<div class="para"><p><em>α</em>(<em>i, j</em>)=Softmax(<strong>q</strong> <em>i</em> <strong>k</strong> T <em>j</em> +<em> u</em> <em>b</em>(<em>i</em><em>−</em><em>j</em>) + Mask(<em>i, j</em>))(2.80)</p></div>
<h1>86 Generative Models</h1>
<h1>q i k T j</h1>
<h1>Bias (u b(i−j))</h1>
<h2>u 0</h2>
<div class="para"><p><strong>q</strong> 0 <strong>k</strong> T 0</p></div>
<h2>u 1 u 0</h2>
<div class="para"><p><strong>q</strong> 1 <strong>k</strong> T 0 <strong>q</strong> 1 <strong>k</strong> T 1</p></div>
<h2>u 2 u 1 u 0</h2>
<div class="para"><p><strong>q</strong> 2 <strong>k</strong> T 0 <strong>q</strong> 2 <strong>k</strong> T 1 <strong>q</strong> 2 <strong>k</strong> T 2</p></div>
<h1>+</h1>
<h2>u 2 u 2 u 1 u 0</h2>
<div class="para"><p><strong>q</strong> 3 <strong>k</strong> T 0 <strong>q</strong> 3 <strong>k</strong> T 1 <strong>q</strong> 3 <strong>k</strong> T 2 <strong>q</strong> 3 <strong>k</strong> T 3</p></div>
<h2>u 3 u 2 u 2 u 1 u 0</h2>
<div class="para"><p><strong>q</strong> 4 <strong>k</strong> T 0 <strong>q</strong> 4 <strong>k</strong> T 1 <strong>q</strong> 4 <strong>k</strong> T 2 <strong>q</strong> 4 <strong>k</strong> T 3 <strong>q</strong> 4 <strong>k</strong> T 4</p></div>
<h2>u 3 u 3 u 2 u 2 u 1 u 0</h2>
<div class="para"><p><strong>q</strong> 5 <strong>k</strong> T 0 <strong>q</strong> 5 <strong>k</strong> T 1 <strong>q</strong> 5 <strong>k</strong> T 2 <strong>q</strong> 5 <strong>k</strong> T 3 <strong>q</strong> 5 <strong>k</strong> T 4 <strong>q</strong> 5 <strong>k</strong> T 5</p></div>
<h2>u 3 u 3 u 3 u 2 u 2 u 1 u 0</h2>
<div class="para"><p><strong>q</strong> 6 <strong>k</strong> T 0 <strong>q</strong> 6 <strong>k</strong> T 1 <strong>q</strong> 6 <strong>k</strong> T 2 <strong>q</strong> 6 <strong>k</strong> T 3 <strong>q</strong> 6 <strong>k</strong> T 4 <strong>q</strong> 6 <strong>k</strong> T 5 <strong>q</strong> 6 <strong>k</strong> T 6</p></div>
<h2>(a) The T5 bias (n b = 3 and dist max = 5)</h2>
<h1>q i k T j</h1>
<h1>Bias (−β(i −j))</h1>
<div class="para"><p><strong>q</strong> 0 <strong>k</strong> T 0</p></div>
<h3>0</h3>
<div class="para"><p><strong>q</strong> 1 <strong>k</strong> T 0 <strong>q</strong> 1 <strong>k</strong> T 1</p></div>
<h3>−1 β 0</h3>
<div class="para"><p><strong>q</strong> 2 <strong>k</strong> T 0 <strong>q</strong> 2 <strong>k</strong> T 1 <strong>q</strong> 2 <strong>k</strong> T 2</p></div>
<h3>−2 β −1 β 0</h3>
<h1>+</h1>
<div class="para"><p><strong>q</strong> 3 <strong>k</strong> T 0 <strong>q</strong> 3 <strong>k</strong> T 1 <strong>q</strong> 3 <strong>k</strong> T 2 <strong>q</strong> 3 <strong>k</strong> T 3</p></div>
<h3>−3 β −2 β −1 β 0</h3>
<div class="para"><p><strong>q</strong> 4 <strong>k</strong> T 0 <strong>q</strong> 4 <strong>k</strong> T 1 <strong>q</strong> 4 <strong>k</strong> T 2 <strong>q</strong> 4 <strong>k</strong> T 3 <strong>q</strong> 4 <strong>k</strong> T 4</p></div>
<h3>−4 β −3 β −2 β −1 β 0</h3>
<div class="para"><p><strong>q</strong> 5 <strong>k</strong> T 0 <strong>q</strong> 5 <strong>k</strong> T 1 <strong>q</strong> 5 <strong>k</strong> T 2 <strong>q</strong> 5 <strong>k</strong> T 3 <strong>q</strong> 5 <strong>k</strong> T 4 <strong>q</strong> 5 <strong>k</strong> T 5</p></div>
<h3>−5 β −4 β −3 β −2 β −β 0</h3>
<div class="para"><p><strong>q</strong> 6 <strong>k</strong> T 0 <strong>q</strong> 6 <strong>k</strong> T 1 <strong>q</strong> 6 <strong>k</strong> T 2 <strong>q</strong> 6 <strong>k</strong> T 3 <strong>q</strong> 6 <strong>k</strong> T 4 <strong>q</strong> 6 <strong>k</strong> T 5 <strong>q</strong> 6 <strong>k</strong> T 6</p></div>
<h3>−6 β −5 β −4 β −3 β −2 β −β 0</h3>
<h2>(b) The ALiBi bias</h2>
<h2>Fig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the biases ranges from light blue denoting small absolute values to deep blue denoting large absolute values.</h2>
<h1>attention weights</h1>
<h1>α(i, j) = Softmax(q i k T j + β · (j −i) √</h1>
<h1>d + Mask(i, j)) (2.83)</h1>
<h1>This model can be interpreted as adding a fixed penalty to q i k T j whenever j moves one step away from i. So we do not need to adapt it to a range of sequence lengths, and can employ it to model arbitrarily long sequences. See Figure 2.11 for a comparison of the T5 bias and the ALiBi bias.</h1>
<h1>In general, the scalar β should be tuned on a validation dataset. However, Press et al. [2022] found that setting β to values decreasing geometrically by a factor of 1 2 a for multi-head attention performs well on a variety of tasks. Specifically, for a self-attention sub-layer involving n head</h1>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 87</p></div>
<div class="para"><p>Entry Query-Key Bias (PE(<em>i, j</em>))</p></div>
<div class="para"><p>T5 [Raffel et al., 2020]<em>u</em> <em>b</em>(<em>i</em><em>−</em><em>j</em>)ALiBi [Press et al., 2022]<em>−</em><em>β</em><em> ·</em> (<em> i</em><em> −</em><em>j</em> )</p></div>
<div class="para"><p>Kerple [Chi et al., 2022]<em>−</em><em>β</em> 1(<em> i</em><em> −</em><em>j</em> )<em>β</em> 2(power)</p></div>
<div class="para"><p><em>−</em><em>β</em> 1 log(1 +<em> β</em> 2(<em> i</em><em> −</em><em>j</em> ))(logarithmic)</p></div>
<div class="para"><p>Sandwich [Chi et al., 2023]P ¯<em>d/</em>2 <em>k</em>=1 cos (<em> i</em><em> −</em><em>j</em> )<em>/</em>10000 2 <em>k/</em> ¯<em>d</em></p></div>
<div class="para"><p>FIRE [Li et al., 2024b]<em>f</em> <em>ψ</em>(<em> i</em><em> −</em><em>j</em> )<em>/ψ</em>(max(<em>m</em> len<em>, i</em>))</p></div>
<div class="para"><p><strong>Table 2.4:</strong> Query-key biases as relative positional embeddings.<em> β</em>,<em> β</em> 1,<em> β</em> 2, ¯<em>d</em>, and<em> m</em> len are hyper-parameters. In the T5 model,<em> b</em>(<em>i</em><em> −</em><em>j</em>) denotes the bucket assigned to<em> i</em><em> −</em><em>j</em>. In the FIRE model,<em> ψ</em>(<em>·</em>) is a monotonically increasing function such as<em> ψ</em>(<em>x</em>) = log(<em>cx</em> + 1), and<em> f</em>(<em>·</em>) is an FFN.</p></div>
<div class="para"><p>heads, the scalar for the<em> k</em>-th head is given by</p></div>
<div class="para"><p><em>β</em> <em>k</em>=1</p></div>
<div class="para"><p>(2.84)</p></div>
<div class="para"><p>2 8 <em>k</em></p></div>
<div class="para"><p>The ALiBi approach provides a simple form of relative positional embeddings. There are other similar methods for designing query-key biases using the offset<em> i</em><em> −</em><em>j</em>. Table 2.4 shows a comparison of such biases. As an aside it is worth noting that the form of the right-hand side of Eq. (2.82) is very similar to length features used in conventional feature-based systems. For example, in statistical machine translation systems, such features are widely used to model word reordering problems, resulting in models that can generalize well across different translation tasks[Koehn, 2010].</p></div>
<div class="para"><p><strong>2.3.5.3</strong> <strong>Rotary Positional Embedding</strong></p></div>
<div class="para"><p>As with sinusoidal embeddings, rotary positional embeddings are based on hard-coded values for all dimensions of an embedding [Su et al., 2024]. Recall that in the sinusoidal embedding model,positions are represented as combinations of sine and cosine functions with different frequencies.These embeddings are then added to token embeddings to form the inputs to the Transformer layer stack. Rotary positional embeddings instead model positional context as rotations to token embeddings in a complex space. This leads to a model expressed in the form of multiplicative embeddings</p></div>
<div class="para"><p><strong>e</strong> <em>i</em>=<strong>x</strong> <em>i</em> <em>R</em>(<em>i</em>)(2.85)</p></div>
<div class="para"><p>where<em> R</em>(<em>i</em>)<em> ∈</em>R <em>d</em><em>×</em><em>d</em> is the rotation matrix representing the rotations performed on the token embedding<strong> x</strong> <em>i</em><em> ∈</em>R <em>d</em>.</p></div>
<div class="para"><p>For simplicity, we will first consider embeddings with only two dimensions and return to a discussion of the more general formulation later. Suppose we have a 2-dimensional token embedding<strong> x</strong> =h <em>x</em> 1 <em>x</em> 2 i. We can represent it as a vector in a plane, originating at the origin (0<em>,</em> 0)and terminating at (<em>x</em> 1<em>, x</em> 2). A counterclockwise rotation of this vector refers to an operation of moving the vector around the origin while maintaining its magnitude, as shown in Figure 2.12 (a).The degree of rotation is usually defined by a specific angle, denoted by<em> θ</em>. The rotation can be</p></div>
<h3>88 Generative Models</h3>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p>vector<strong> x</strong></p></div>
<div class="para"><p><strong>x</strong> <strong>x</strong> <em>R</em> <em>θ</em></p></div>
<div class="para"><p><strong>x</strong> <em>R</em> <em>θ</em> rotated vector</p></div>
<h3>θ</h3>
<h3>θ</h3>
<h3>θ</h3>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p><strong>x</strong> <em>R</em> 2 <em>θ</em></p></div>
<h3>θ</h3>
<div class="para"><p><strong>x</strong> <em>R</em> 3 <em>θ</em></p></div>
<div class="para"><p>(a) Single-step Rotation</p></div>
<div class="para"><p>(b) Multi-step Rotation</p></div>
<div class="para"><p>The 1 cat 2 is 3 sleeping 4 peacefully 5 in 6 the 7 warm 8 sunlight 9 .10</p></div>
<div class="para"><p><em>x</em> 2</p></div>
<div class="para"><p>sleeping 4</p></div>
<h3>7 θ</h3>
<div class="para"><p>cat 2</p></div>
<h3>7 θ</h3>
<div class="para"><p><em>x</em> 1</p></div>
<div class="para"><p>sleeping 11</p></div>
<div class="para"><p>cat 9</p></div>
<div class="para"><p>Every 1 afternoon 2 ,3 you 4 ’ll 5 find 6 that 7 the 8 cat 9 is 10 sleeping 11 on 12 my 13 bed 14 .15</p></div>
<div class="para"><p>(c) Angles between embeddings of two tokens at different positions</p></div>
<div class="para"><p><strong>Fig. 2.12:</strong> Illustrations of vector rotations in a plane. Sub-figures (a) and (b) show rotations of a vector in a single step and multiple steps, respectively. Sub-figure (c) shows the embeddings of tokens<em> cat</em> and<em> sleeping</em> in two different sentences. We show these sentences with a subscript affixed to each token to indicate its position. If we represent tokens as vectors, we can add positional information by rotating these vectors. This rotation preserves the “distances”between the vectors. For example, given that the distance between<em> cat</em> and<em> sleeping</em> is the same in both sentences, the angle between their embeddings also remains the same during rotation.</p></div>
<h3>expressed mathematically in the form</h3>
<h3>Ro(x, θ) = x R θ</h3>
<h3>= h x 1 x 2 i &amp;quot; cos θ sin θ −sin θ cos θ</h3>
<div class="para"><p>#</p></div>
<h3>= h cos θ · x 1 −sin θ · x 2 sin θ · x 1 + cos θ · x 2 i</h3>
<h3>(2.86)</h3>
<h3>&amp;quot; cos θ sin θ −sin θ cos θ</h3>
<div class="para"><p>#</p></div>
<h3>where R θ =</h3>
<h3>is the rotation matrix. If two or more rotations are performed on the</h3>
<h3>same vector, we can rotate the vector further. This follows from the fact that the composition of successive rotations is itself a rotation. More formally, rotating a vector by an angle θ for t times</h3>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 89</p></div>
<div class="para"><p>can be expressed as</p></div>
<div class="para"><p>Ro(<strong>x</strong><em>, tθ</em>)=<strong>x</strong> <em>R</em> <em>tθ</em></p></div>
<div class="para"><p>=h cos<em> tθ</em><em> ·</em><em> x</em> 1<em> −</em>sin<em> tθ</em><em> ·</em><em> x</em> 2 sin<em> tθ</em><em> ·</em><em> x</em> 1 + cos<em> tθ</em><em> ·</em><em> x</em> 2 i</p></div>
<div class="para"><p>(2.87)</p></div>
<div class="para"><p>If we interpret<em> t</em> as the position of a token represented by<strong> x</strong> in a sequence, then we will find that the above equation defines a simple positional embedding model. As shown in Figure 2.12(b), we start moving the token from position 0. Each time we move one step forward, the vector is rotated by the angle<em> θ</em>. Upon arriving at the position<em> t</em>, the representation of the token with positional context is given by Ro(<strong>x</strong><em>, iθ</em>). As the rotations do not change the magnitude of the embedding, the original “meaning” of the token is retained. The positional information is injected into the embedding, when it gets rotated.</p></div>
<div class="para"><p>A popular way to understand vector rotation is to define it in complex spaces. It is easy to transform each vector<strong> x</strong> =h <em>x</em> 1 <em>x</em> 2 i in the 2D Euclidean space R 2 to a complex number <strong>x</strong><em>′</em> =<em> x</em> 1 + i <em>x</em> 2 in the complex space C via a bijective linear map. Then, the rotation of<strong> x</strong> with the angle<em> tθ</em> corresponds to the multiplication by<em> e</em> i <em>tθ</em>. Given that<em> e</em> i <em>tθ</em> = cos<em> tθ</em> + i sin<em> tθ</em>, the rotation operation can be re-expressed in the form</p></div>
<div class="para"><p><strong>x</strong> <em>R</em> <em>tθ</em> <em>7→</em><strong>x</strong><em>′</em><em>e</em> i <em>tθ</em></p></div>
<div class="para"><p>=(<em>x</em> 1 + i <em>x</em> 2)(cos<em> tθ</em> + i sin<em> tθ</em>)</p></div>
<div class="para"><p>=cos<em> tθ</em><em> ·</em><em> x</em> 1<em> −</em>sin<em> tθ</em><em> ·</em><em> x</em> 2 + i(sin<em> tθ</em><em> ·</em><em> x</em> 1 + cos<em> tθ</em><em> ·</em><em> x</em> 2)(2.88)</p></div>
<div class="para"><p>Here we denote the token representation<strong> x</strong><em>′</em><em>e</em> i <em>tθ</em> by<em> C</em>(<strong>x</strong><em>, tθ</em>). The inner product of the representations of the tokens at positions<em> t</em> and<em> s</em> can be written as</p></div>
<div class="para"><p><em>⟨</em><em>C</em>(<strong>x</strong><em>, tθ</em>)<em>, C</em>(<strong>y</strong><em>, sθ</em>)<em>⟩</em>=(<strong>x</strong><em>′</em><strong>y</strong><em>′</em>)<em>e</em> i(<em>t</em><em>−</em><em>s</em>)<em>θ</em>(2.89)</p></div>
<div class="para"><p>where<strong> y</strong><em>′</em> is the complex conjugate of<strong> y</strong><em>′</em>. As can be seen, the result of this inner product involves a term<em> t</em><em> −</em><em>s</em>, and so it can model the offset between the two tokens.</p></div>
<div class="para"><p>Now we go back to representations in the 2D Euclidean space. The dot-product of Ro(<strong>x</strong><em>, tθ</em>)and Ro(<strong>y</strong><em>, sθ</em>) is can be written as a function of (<em>t</em><em> −</em><em>s</em>)<em>θ</em></p></div>
<div class="para"><p>Ro(<strong>x</strong><em>, tθ</em>)[Ro(<strong>y</strong><em>, sθ</em>)]T=<strong>x</strong> <em>R</em> <em>tθ</em>[<strong>y</strong> <em>R</em> <em>sθ</em>]T</p></div>
<div class="para"><p>=<strong>x</strong> <em>R</em> <em>tθ</em>[<em>R</em> <em>sθ</em>]T <strong>y</strong> T</p></div>
<div class="para"><p>=<strong>x</strong> <em>R</em>(<em>t</em><em>−</em><em>s</em>)<em>θ</em> <strong>y</strong> T(2.90)</p></div>
<div class="para"><p>Given this result, if we consider Ro(<strong>x</strong><em>, tθ</em>) and Ro(<strong>y</strong><em>, sθ</em>) as the query and the key, then the selfattention operation will implicitly involve the modeling of relative positional context.</p></div>
<div class="para"><p>This rotary positional embedding can be extended to multi-dimensional embeddings. For a<em> d</em>-dimensional token embedding<strong> x</strong> =h <em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>d</em> i, we can treat it as a<em> </em><em>d</em></p></div>
<div class="para"><p>2-dimensional</p></div>
<div class="para"><p>complex vector<strong> x</strong><em>′</em> =h <em>x</em><em>′</em>1 <em>x</em><em>′</em>2<em>...</em><em>x</em><em>′</em><em>d/</em>2 i=h <em>x</em> 1 + i <em>x</em> 2 <em>x</em> 3 + i <em>x</em> 4<em>...</em><em>x</em> <em>d</em><em>−</em>1 + i <em>x</em> <em>d</em> i, where each consecutive pair of items forms a complex number. Then, the rotary positional embedding in</p></div>
<div class="para"><p>90 <em>Generative Models</em></p></div>
<div class="para"><p>the complex space is given by</p></div>
<div class="para"><p><em>d/</em>2 X</p></div>
<div class="para"><p><em>k</em>=1 <em>x</em><em>′</em><em>k</em> <em>e</em> i <em>tθ</em> <em>k</em><em>⃗e</em> <em>k</em>(2.91)</p></div>
<div class="para"><p><em>C</em>(<strong>x</strong><em>, tθ</em>)=</p></div>
<div class="para"><p>where<em> ⃗e</em> <em>k</em> is the standard basis vector with a single non-zero value in the<em> k</em>-th coordinate and 0’s elsewhere [Biderman et al., 2021].</p></div>
<div class="para"><p>Although this formula involves a complicated expression, its equivalent form in the<em> d</em>-dimensional Euclidean space is relatively easy to understand. We can write it as</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><em>R</em> <em>tθ</em> 1 <em>R</em> <em>tθ</em> 2...<em>R</em> <em>tθ</em> <em>d/</em>2</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p>Ro(<strong>x</strong><em>, tθ</em>)=h <em>x</em> 1 <em>x</em> 2<em>...</em><em>x</em> <em>d</em> i</p></div>
<div class="para"><p>(2.92)</p></div>
<div class="para"><p>&quot;cos<em> tθ</em> <em>k</em> sin<em> tθ</em> <em>k</em><em>−</em>sin<em> tθ</em> <em>k</em> cos<em> tθ</em> <em>k</em></p></div>
<div class="para"><p>#</p></div>
<div class="para"><p>.<em> θ</em> =h <em>θ</em> 1<em>, ..., θ</em> <em>d/</em>2 i are the parameters for controlling the an-</p></div>
<div class="para"><p>where<em> R</em> <em>tθ</em> <em>k</em> =</p></div>
<div class="para"><p>gles of rotations in different dimensions. Typically,<em> θ</em> <em>k</em> is set to 10000<em>−</em>2(<em>k</em><em>−</em>1)</p></div>
<div class="para"><p><em>d</em>, which is analogous to the setting in sinusoidal embeddings.</p></div>
<div class="para"><p>In a practical implementation, Eq. (2.92) can be rewritten into a form that relies solely on the element-wise product and addition of vectors.</p></div>
<div class="para"><p>T</p></div>
<div class="para"><p>T</p></div>
<div class="para"><p>T</p></div>
<div class="para"><p>T</p></div>
<div class="para"><p><em>−</em><em>x</em> 2 <em>x</em> 1...<em>−</em><em>x</em> <em>d</em> <em>x</em> <em>d</em><em>−</em>1</p></div>
<div class="para"><p><em>x</em> 1 <em>x</em> 2...<em>x</em> <em>d</em><em>−</em>1 <em>x</em> <em>d</em></p></div>
<div class="para"><p>cos<em> tθ</em> 1 cos<em> tθ</em> 1...cos<em> tθ</em> <em>d/</em>2 cos<em> tθ</em> <em>d/</em>2</p></div>
<div class="para"><p>sin<em> tθ</em> 1 sin<em> tθ</em> 1...sin<em> tθ</em> <em>d/</em>2 sin<em> tθ</em> <em>d/</em>2</p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p></p></div>
<div class="para"><p><em>⊙</em></p></div>
<div class="para"><p><em>⊙</em></p></div>
<div class="para"><p>Ro(<strong>x</strong><em>, tθ</em>)=</p></div>
<div class="para"><p>+</p></div>
<div class="para"><p>(2.93)</p></div>
<div class="para"><p>Finally, we rewrite Eq. (2.85) to obtain the form of the embedding at position<em> i</em></p></div>
<div class="para"><p><strong>e</strong> <em>i</em>=Ro(<strong>x</strong> <em>i</em><em>, iθ</em>)(2.94)</p></div>
<div class="para"><p><strong>2.3.5.4</strong> <strong>Position Interpolation</strong></p></div>
<div class="para"><p>In position interpolation, our goal is to map the positions in the new sequence to match the observed range in training. Suppose the sequence length for training ranges from 0 to<em> m</em> <em>l</em>. When <em>m &gt; m</em> <em>l</em> at test time, we represent the positions in [0<em>, m</em>] such that our representations fit [0<em>, m</em> <em>l</em>].</p></div>
<div class="para"><p>To illustrate, consider the rotary positional embedding model described above. The embedding of each token is described by a model Ro(<strong>x</strong> <em>i</em><em>, iθ</em>) in which<em> θ</em> =h <em>θ</em> 1<em>, ..., θ</em> <em>d/</em>2 i are the parameters.Ro(<strong>x</strong> <em>i</em><em>, iθ</em>) can be cast in the form of a linear combination of two periodic functions (see Eq.</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 91</p></div>
<div class="para"><p>(2.93))</p></div>
<div class="para"><p>cos<em> iθ</em>=h cos<em> iθ</em> 1<em>...</em>cos<em> iθ</em> <em>d/</em>2 i</p></div>
<div class="para"><p>(2.95)</p></div>
<div class="para"><p>sin<em> iθ</em>=h sin<em> iθ</em> 1<em>...</em>sin<em> iθ</em> <em>d/</em>2 i</p></div>
<div class="para"><p>(2.96)</p></div>
<div class="para"><p><em>θ</em> <em>k</em> is a exponential function of<em> k</em> and takes the form</p></div>
<div class="para"><p><em>θ</em> <em>k</em>=<em>b</em><em>−</em>2(<em>k</em><em>−</em>1)</p></div>
<div class="para"><p><em>d</em>(2.97)</p></div>
<div class="para"><p>where<em> b</em> is the base. The period of cos<em> iθ</em> <em>k</em> and sin<em> iθ</em> <em>k</em> is</p></div>
<div class="para"><p><em>T</em> <em>k</em>=2 <em>π</em><em> ·</em><em> b</em> 2(<em>k</em><em>−</em>1)</p></div>
<div class="para"><p><em>d</em>(2.98)</p></div>
<div class="para"><p>The key idea behind position interpolation is to adjust this period so that the new positions can be encoded within the range [0<em>, m</em> <em>l</em>]. One way to achieve this is to scale up<em> T</em> <em>k</em> by<em> </em><em>m</em></p></div>
<div class="para"><p><em>m</em> <em>l</em> , given by</p></div>
<div class="para"><p><em>T</em><em> </em><em>′</em><em>k</em>=<em>m</em> <em>m</em> <em>l</em><em>·</em> 2 <em>π</em><em> ·</em><em> b</em> 2(<em>k</em><em>−</em>1)</p></div>
<div class="para"><p><em>d</em>(2.99)</p></div>
<div class="para"><p>Hence all points in [0<em>, m</em>] are compressed into [0<em>, m</em> <em>l</em>]. This linear scaling can be easily realized by modifying the input to the embedding model [Chen et al., 2023c]. The new model with linear positional interpolation is given by</p></div>
<div class="para"><p>Ro<em>′</em>(<strong>x</strong> <em>i</em><em>, iθ</em>)=Ro(<strong>x</strong> <em>i</em><em>, </em><em>m</em> <em>l</em></p></div>
<div class="para"><p><em>m </em><em>iθ</em>)(2.100)</p></div>
<div class="para"><p>Another method of positional interpolation is to scale the base 17. Suppose that the base<em> b</em> is scaled by<em> λ</em>. We wish the period of this new model in the last dimension of<em> θ</em> (i.e., dimension<em> </em><em>d</em></p></div>
<div class="para"><p>2)to be equal to that of the linear positional interpolation model. This can be expressed as</p></div>
<div class="para"><p>2(<em> </em><em>d</em></p></div>
<div class="para"><p>2(<em> </em><em>d</em></p></div>
<div class="para"><p><em>d</em>=<em>m</em> <em>m</em> <em>l</em><em>·</em> 2 <em>π</em><em> ·</em><em> b</em></p></div>
<div class="para"><p>2<em> </em><em>−</em>1)</p></div>
<div class="para"><p>2<em> </em><em>−</em>1)</p></div>
<div class="para"><p>2 <em>π</em><em> ·</em> (<em>λb</em>)</p></div>
<div class="para"><p><em>d</em>(2.101)</p></div>
<div class="para"><p>Solving this equation, we obtain</p></div>
<div class="para"><p><em>d</em> 2(<em> </em><em>d</em></p></div>
<div class="para"><p><em>λ</em>= <em> m</em></p></div>
<div class="para"><p>2<em> </em><em>−</em>1)</p></div>
<div class="para"><p><em>m</em> <em>l</em></p></div>
<div class="para"><p>= <em> m</em></p></div>
<div class="para"><p><em>d</em> <em>d</em><em>−</em>2(2.102)</p></div>
<div class="para"><p><em>m</em> <em>l</em></p></div>
<div class="para"><p>This gives an embedding model</p></div>
<div class="para"><p>Ro<em>′</em>(<strong>x</strong> <em>i</em><em>, iθ</em>)=Ro(<strong>x</strong> <em>i</em><em>, iθ</em><em>′</em>)(2.103)</p></div>
<div class="para"><p>17 This method was first proposed in https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/</p></div>
<div class="para"><p>92 <em>Generative Models</em></p></div>
<div class="para"><p>where</p></div>
<div class="para"><p><em>θ</em><em>′</em> =h(<em>λb</em>)<em>−</em>0</p></div>
<div class="para"><p><em>d</em> i</p></div>
<div class="para"><p><em>d</em><em> , ...,</em> (<em>λb</em>)<em>−</em><em>d</em><em>−</em>2</p></div>
<div class="para"><p><em>d</em><em> ,</em> (<em>λb</em>)<em>−</em>2</p></div>
<div class="para"><p>(2.104)</p></div>
<div class="para"><p>Note that scaling the base provides a non-uniform method for scaling the periods across different dimensions of<em> θ</em>. This method has been found to be helpful for extending LLMs to longer sequences, and several improvements have been developed [Peng et al., 2024; Ding et al., 2024].</p></div>
<div class="para"><p><strong>2.3.6</strong> <strong>Remarks</strong></p></div>
<div class="para"><p>In this section, we have presented a variety of methods for long-context language modeling. We close this section by discussing some interesting issues related to these methods.</p></div>
<div class="para"><p><strong>2.3.6.1</strong> <strong>Need for Long Context</strong></p></div>
<div class="para"><p>One of the ultimate goals of long-context LLMs is that these models can precisely encode infinite context. The so-called infinite context refers more to the fact that an LLM can continuously read words. This motivates LLMs that can handle extremely long context or stream data. As discussed in Section 2.3.3, it is common to use fixed-size memory models to process continuously expanding context. Many such systems are based on recurrent architectures or their variants, because they are inherently suited to model time series problems where the effects of past inputs continue indefinitely. Another way to achieve infinite memory is to develop alternatives to self-attention models, for example, one can use continuous-space attention models to encode context, which removes the dependency on context length [Martins et al., 2022].</p></div>
<div class="para"><p>When studying long-context LLMs, it is natural to wonder what mechanisms may explain the use of long context in language modeling. Can we compress the representation of infinite context into a relatively small-sized model? Are all context tokens useful for predicting next tokens? How do LLMs prepare for token prediction when they see the context? Can we know in advance which contextual information will be critical for prediction? General answers to all these questions are not obvious, but they inspire follow-on research of explainable models, and some interesting results have been found. For example, Deletang et al. [2024] conducted extensive experiments to show that LLMs are powerful in-context compressors. Although viewing predictive models as compression models has long been studied in machine learning, it also provides insights into our understanding of the LLM scaling laws. Pal et al. [2023] and Wu et al. [2024] investigated whether the features learned up to the current step, though not intentionally, are already sufficient for predicting tokens at the following steps. Note that the need for long-context in language modeling is highly dependent on the problem that we address. A related issue is where to apply LLMs and how to evaluate them. For example, in summarization tasks we may only need to distill and focus on a few key aspects of the text, while in retrieval-like tasks we need to “memorize”the entire context so that the relevant information can be accessed. We will discuss the evaluation issue later in this subsection.</p></div>
<div class="para"><p><strong>2.3.6.2</strong> <strong>Pre-training or Adapting LLMs?</strong></p></div>
<div class="para"><p>Training LLMs requires significant computational costs. Although it is straightforward to train LLMs on long sequence data, the training becomes computationally unwieldy for large data sets. It</p></div>
<div class="para"><p><em>2.3 Long Sequence Modeling</em> 93</p></div>
<div class="para"><p>is common practice to pre-train LLMs on general datasets, and then adapt them with modest finetuning effort. For example, LLMs with relative or rotary positional embeddings can be directly trained on large-scale data in the pre-training phase. While the resulting models may exhibit some abilities to extrapolate lengths in the inference phase, it may be more effective to fine-tune them on longer sequences.</p></div>
<div class="para"><p>Ideally, we would like to pre-train LLMs with standard Transformer architectures and adapt them to new tasks. This allows us to use many off-the-shelf LLMs and efficiently adapt them to handle long sequences. However, when new architectures are adopted, it seems inevitable that we need to train these models from scratch. This poses practical difficulties for developing longcontext LLMs, as we cannot leverage well-developed, pre-trained models and must instead train them ourselves. On the other hand, fine-tuning is still an effective way to adapt LLMs with certain architectures that are different from those in pre-training. An example is models augmented with external memories. In these models, the pre-trained LLMs are fixed, and the focus is on how to make these LLMs collaborate with the memory models. In RAG, for instance, it is common to fine-tune LLMs to improve their use of retrieval-augmented inputs. Another example of finetuning LLMs for long-context modeling is that we train an LLM with full attention models, and then replace them with sparse attention models in the fine-tuning phase. The pre-trained LLM provides initial values of model parameters used in a different model, and this model is then finetuned as usual.</p></div>
<div class="para"><p><strong>2.3.6.3</strong> <strong>Evaluating Long-context LLMs</strong></p></div>
<div class="para"><p>Evaluating long-context LLMs is important, but it is a new issue in NLP. The general idea is that,if we input a long context to an LLM, then we can check from the output of the LLM whether it understands the entire context and makes use of it in predicting following tokens. In conventional research of NLP, such evaluations are often aimed at examining the ability of NLP models in handling long-range dependencies. However, the size of contexts used in recent LLMs is much larger than that used in NLP systems a few years ago. This motivates researchers to develop new evaluation benchmarks and metrics for long-context LLMs.</p></div>
<div class="para"><p>One approach is to use the perplexity metric. However, in spite of its apparent simplicity, this method tends to reflect more on the LLMs’ ability to make use of local context rather than global context. It is therefore tempting to develop evaluation methods that are specific to long-context LLMs. Popular methods include various synthetic tasks where artificially generated or modified data is used to evaluate specific capabilities of long-context LLMs. In needle-in-a-haystack 18 and passkey retrieval tasks [Mohtashami and Jaggi, 2024; Chen et al., 2023c], for instance, LLMs are required to identify and extract a small, relevant piece of information from a large volume of given text. The assumption here is that an LLM with sufficient memory should remember earlier parts of the text as it processes new information. This LLM can thus pick out the relevant details, which might be sparse and hidden among much irrelevant information, from the text. Alternatively, in copy memory tasks (or copy tasks for short), LLMs are used to repeat the input text or a specific segment multiple times. These tasks were initially proposed to test the extent to which recurrent models can retain and recall previously seen tokens [Hochreiter and Schmidhuber, 1997; Arjovsky et al., 2016], and have been adopted in evaluating recent LLMs [Bulatov et al., 2022; Gu and Dao,2023].</p></div>
<div class="para"><p>18 https://github.com/gkamradt/LLMTest_NeedleInAHaystack</p></div>
<div class="para"><p>94 <em>Generative Models</em></p></div>
<div class="para"><p>Another approach to evaluating long-context LLMs is to test them on NLP tasks that involve very long input sequences. Examples include long-document or multi-document summarization,long-document question answering, code completion, and so on. A benefit of this approach is that it can align evaluations with user expectations.</p></div>
<div class="para"><p>Although many methods have been developed, there is still no general way to evaluate longcontext LLMs [Liu et al., 2024c]. One problem is that most of these methods focus on specific aspects of LLMs, rather than their fundamental ability to model very long contexts. Even though an LLM can pick out the appropriate piece of text from the input, we cannot say that it truly understands the entire context. Instead, it might just remember some important parts of the context,or even simply recall the answer via the model learned in pre-training. Moreover, the data used in many tasks is small-scale and relatively preliminary, leading to discrepancies between evaluation results and actual application performance. A more interesting issue is that the results of LLMs are influenced by many other factors and experimental setups, for example, using different prompts can lead to very different outcomes. This makes evaluation even more challenging because improvements may not solely result from better modeling of long contexts, and there is a risk of overclaiming our results. Nevertheless, many open questions remain in the development and evaluation of long-context LLMs. For example, these models still suffer from limitations such as restricted context length and high latency. Studying these issues is likely to prove valuable future directions.</p></div>
<h3>2.4 Summary</h3>
<div class="para"><p>In this chapter, we have discussed the concept of LLMs and related techniques. This can be considered a general, though not comprehensive, introduction to LLMs, laying the foundation for further discussions on more advanced topics in subsequent chapters. Furthermore, we have explored two ways to scale up LLMs. The first focuses on the large-scale pre-training of LLMs, which is crucial for developing state-of-the-art models. The second focuses on methods for adapting LLMs to long inputs, including optimizing attention models, designing more efficient and compressed KV caches, incorporating memory models, and exploring better positional embeddings.</p></div>
<div class="para"><p>The strength of LLMs lies in their ability to break the constraints of training NLP models for a limited number of specific tasks. Instead, LLMs learn from large amounts of text through the simple task of token prediction — we predict the next token in a sentence given its prior tokens.A general view is that, by repeating this token prediction task a large number of times, LLMs can acquire some knowledge of the world and language, which can then be applied to new tasks. As a result, LLMs can be prompted to perform any task by framing it as a task of predicting subsequent tokens given prompts. This emergent ability in language models comes from several dimensions,such as scaling up training, model size, and context size. It is undeniable that scaling laws are currently the fundamental principle adopted in developing large language models, although simply increasing model size has yet to prove sufficient for achieving AGI. These continuously scaled LLMs have been found to show capabilities in general-purpose language understanding, generation, and reasoning. More recently, it has been found that scaling up the compute at inference time can also lead to significant improvements in complex reasoning tasks [OpenAI, 2024].</p></div>
<div class="para"><p>Given their amazing power, LLMs have attracted considerable interest, both in terms of techniques and applications. As a result, the explosion of research interest in LLMs has also led to a</p></div>
<div class="para"><p><em>2.4 Summary</em> 95</p></div>
<div class="para"><p>vast number of new techniques and models. However, we do not attempt to provide a comprehensive literature review on all aspects of LLMs, given the rapid evolution of the field. Nevertheless,one can still gain knowledge about LLMs from general reviews [Zhao et al., 2023; Minaee et al.,2024] or more focused discussions on specific topics [Ruan et al., 2024].</p></div>
<div class="para"><p>https://github.com/NiuTrans/NLPBook</p></div>
<div class="para"><p>https://niutrans.github.io/NLPBook</p></div>
<h3>C HAPTER 3</h3>
<h1>Prompting</h1>
<div class="para"><p>In the context of LLMs,<em> prompting</em> refers to the method of providing an LLM with a specific input or cue to generate a desired output or perform a task. For example, if we want the LLM to translate a sentence from English to Chinese, we can prompt it like this</p></div>
<div class="para"><p>Translate the text from English to Chinese.</p></div>
<div class="para"><p>Text: The early bird catches the worm.</p></div>
<div class="para"><p>Translation:</p></div>
<div class="para"><p>Prompting is crucial for LLMs because it directly influences how effectively these models understand and respond to user queries. A well-crafted prompt can guide an LLM to generate more accurate, relevant, and contextually appropriate responses. Furthermore, this process can be iteratively refined. By analyzing the responses of the LLM, users can adjust their prompts to align more closely with their specific needs. Given the importance of prompting in applying LLMs,prompt design has become an essential skill for users and developers working with LLMs. This leads to an active research area, called<strong> prompt engineering</strong>, in which we design effective prompts to make better use of LLMs and enhance their practical utility in real-world applications.</p></div>
<div class="para"><p>An important concept related to prompting is<strong> in-context learning</strong>. When prompting an LLM,we can add new information to the context, such as demonstrations of problem-solving. This allows the LLM to learn from this context how to solve the problem. Here is an example of prompting LLMs with a few demonstrations of how to classify text based on sentiment polarity.</p></div>
<div class="para"><p>Here are some examples of text classification.</p></div>
<div class="para"><p>Example 1: We had a delightful dinner together.<em> →</em>Label: Positive</p></div>
<div class="para"><p>Example 2: I’m frustrated with the delays.<em> →</em>Label: Negative</p></div>
<div class="para"><p>What is the label for “That comment was quite hurtful.”?</p></div>
<div class="para"><p>Label:</p></div>
<div class="para"><p>In-context learning is often seen as an emergent ability of LLMs that arises after pre-training.Though LLMs can be trained or tuned to perform new tasks, in-context learning provides a very efficient way to adapt these models without any training or tuning effort. Perhaps this is one of the most notable features of LLMs: they indeed learn general knowledge about the world and language during pre-training, which we can easily apply to new challenges. Moreover, in-context learning reflects the broader trend of making AI systems more generalizable and user-friendly.Instead of requiring specialized engineers to fine-tune models for every unique task, users can interact with LLMs in a more intuitive way, simply providing examples or adjusting the context as needed.</p></div>
<div class="para"><p>In this chapter, we focus on prompting techniques in LLMs. We begin by considering several interesting prompt designs commonly used in prompt engineering. Then, we discuss a series of</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 97</p></div>
<div class="para"><p>refinements to these methods. Finally, we explore approaches for automating prompt design.</p></div>
<h3>3.1 General Prompt Design</h3>
<div class="para"><p>This section presents basic concepts in prompt design, along with examples of how to prompt LLMs for various NLP tasks. Since the effectiveness of prompting is highly dependent on the LLMs being used, prompts often vary across different LLMs, making it difficult to provide a comprehensive list of prompts for all LLMs and downstream tasks. Therefore, this discussion is not focused on any specific LLM. Instead, the goal is to provide guiding principles for prompt design.</p></div>
<div class="para"><p><strong>3.1.1</strong> <strong>Basics</strong></p></div>
<div class="para"><p>The term<em> prompt</em> is used in many different ways. In this chapter we define a prompt as the input text to an LLM, denoted by<strong> x</strong>. The LLM generates a text<strong> y</strong> by maximizing the probability Pr(<strong>y</strong><em>|</em><strong>x</strong>).In this generation process, the prompt acts as the condition on which we make predictions, and it can contain any information that helps describe and solve the problem.</p></div>
<div class="para"><p>A prompt can be obtained using a prompt template (or template for short) [Liu et al., 2023a].A template is a piece of text containing placeholders or variables, where each placeholder can be filled with specific information. Here are two templates for asking the LLM for weekend suggestions.</p></div>
<div class="para"><p>Please give me some suggestions for a fun weekend.</p></div>
<div class="para"><p>If<em> {∗</em>premise<em>∗}</em>, what are your suggestions for a fun weekend.</p></div>
<div class="para"><p>In the first template, we simply instruct the LLM to return some suggestions. So the template is just a piece of text with no variables. In the second template, the variable<em> {∗</em>premise<em>∗}</em> needs to be specified by the users to provide a premise for making suggestions. For example, if we input</p></div>
<div class="para"><p>premise=the weather is nice this weekend</p></div>
<div class="para"><p>then we can generate a prompt</p></div>
<div class="para"><p>If the weather is nice this weekend,what are your suggestions for a fun weekend.</p></div>
<div class="para"><p>We can also design a template with multiple variables. Here is an example in which we compare the two sentences in terms of their semantic similarity.</p></div>
<div class="para"><p>98 <em>Prompting</em></p></div>
<div class="para"><p>Here is a sentence<em>{∗</em>sentence1<em>∗}</em>Here is another sentence<em>{∗</em>sentence2<em>∗}</em></p></div>
<div class="para"><p>Compute the semantic similarity between the two sentences</p></div>
<div class="para"><p>A popular way to format prompts is to write each input or output in a “name:content” style.For example, we can describe a conversation between two people, named John and David, and use the LLM to continue the conversation. A template of such prompts is given by</p></div>
<div class="para"><p>John:<em> {∗</em>utterance1<em>∗}</em>David:<em> {∗</em>utterance2<em>∗}</em>John:<em> {∗</em>utterance3<em>∗}</em>David:<em> {∗</em>utterance4<em>∗}</em>John:<em> {∗</em>utterance5<em>∗}</em>David:<em> {∗</em>utterance6<em>∗}</em>John:<em> {∗</em>utterance7<em>∗}</em>David:</p></div>
<div class="para"><p>The “name:content” format can be used to define the task that we want the LLM to perform.For example, given that “Q” and “A” are commonly used abbreviations for “Question” and “Answer”, respectively, we can use the following template to do question-answering.</p></div>
<div class="para"><p>Q:<em> {∗</em>question<em>∗}</em>A:</p></div>
<div class="para"><p>This format can be used to describe more complex tasks. For example, the following is an example of providing a specification for a translation task</p></div>
<div class="para"><p>Task: Translation Source language: English Target language: Chinese Style: Formal text Template: Translate the following sentence:<em> {∗</em>sentence<em>∗}</em></p></div>
<div class="para"><p>In practical systems, it is common to represent and store such data in key-value pairs, such as the JSON format 1.</p></div>
<div class="para"><p>When the problem is difficult to describe in an attribute-based manner, it is more common to instruct LLMs with a clear and detailed description. There are many ways to do this. One</p></div>
<div class="para"><p>1 The JSON representation is</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 99</p></div>
<div class="para"><p>example is to assign a role to LLMs and provide sufficient context. The following is a template that instructs an LLM to act as an expert and answer questions from children.</p></div>
<div class="para"><p>You are a computer scientist with extensive knowledge in the field of deep learning.</p></div>
<div class="para"><p>Please explain the following computer-related concept to a child around 10 years old, using simple examples whenever possible.</p></div>
<div class="para"><p><em>{∗</em>concept<em>∗}</em></p></div>
<div class="para"><p>Here the text “You are a computer scientist ... deep learning. ” is sometimes called system information, and is provided to help the LLM understand the context or constraints of the task it is being asked to perform.</p></div>
<div class="para"><p><strong>3.1.2</strong> <strong>In-context Learning</strong></p></div>
<div class="para"><p>Learning can occur during inference. In-context learning is one such method, where prompts involve demonstrations of problem-solving, and LLMs can learn from these demonstrations how to solve new problems. Since we do not update model parameters in this process, in-context learning can be viewed as a way to efficiently activate and reorganize the knowledge learned in pre-training without additional training or fine-tuning. This enables quick adaptation of LLMs to new problems, pushing the boundaries of what pre-trained LLMs can achieve without task-specific adjustments.</p></div>
<div class="para"><p>In-context learning can be illustrated by comparing three methods: zero-shot learning, oneshot learning and few-shot learning. Zero-shot learning, as its name implies, does not involve a traditional “learning” process. It instead directly applies LLMs to address new problems that were not observed during training. In practice, we can repetitively adjust prompts to guide the LLMs in generating better responses, without demonstrating problem-solving steps or providing examples.Consider the following example. Suppose we want to use an LLM as an assistant that can help correct English sentences. A zero-shot learning prompt is given by</p></div>
<div class="para"><p>{&quot;Task&quot;: &quot;Translation&quot;&quot;Source language&quot;: &quot;English&quot;&quot;Target language&quot;: &quot;Chinese&quot;&quot;Style&quot;: &quot;Formal text&quot;&quot;Template&quot;: &quot;Translate the following sentence:<em> {∗</em>sentence<em>∗}</em>&quot;}</p></div>
<div class="para"><p>100 <em>Prompting</em></p></div>
<div class="para"><p>SYSTEM You are a helpful assistant, and are great at grammar correction.</p></div>
<div class="para"><p>USER You will be provided with a sentence in English. The task is to output the correct sentence.</p></div>
<div class="para"><p>Input: She don’t like going to the park.Output:</p></div>
<div class="para"><p>Here the gray words are used to indicate different fields of the prompt.</p></div>
<div class="para"><p>In one-shot learning, we extend this prompt by adding a demonstration of how to correct sentences, thereby allowing the LLM to learn from this newly-added experience.</p></div>
<div class="para"><p>SYSTEM You are a helpful assistant, and are great at grammar correction.</p></div>
<div class="para"><p>DEMO You will be provided with a sentence in English. The task is to output the correct sentence.</p></div>
<div class="para"><p>Input: There is many reasons to celebrate.Output: There are many reasons to celebrate.</p></div>
<div class="para"><p>USER You will be provided with a sentence in English. The task is to output the correct sentence.</p></div>
<div class="para"><p>Input: She don’t like going to the park.Output:</p></div>
<div class="para"><p>Furthermore, we can add more demonstrations to enable few-shot learning.</p></div>
<div class="para"><p>SYSTEM You are a helpful assistant, and are great at grammar correction.</p></div>
<div class="para"><p>DEMO1 You will be provided with a sentence in English. The task is to output the correct sentence.</p></div>
<div class="para"><p>Input: There is many reasons to celebrate.Output: There are many reasons to celebrate.</p></div>
<div class="para"><p>DEMO2 You will be provided with a sentence in English. The task is to output the correct sentence.</p></div>
<div class="para"><p>Input: Me and my friend goes to the gym every day.Output: My friend and I go to the gym every day.</p></div>
<div class="para"><p>USER You will be provided with a sentence in English. The task is to output the correct sentence.</p></div>
<div class="para"><p>Input: She don’t like going to the park.Output:</p></div>
<div class="para"><p>In few-shot learning, we essentially provide a pattern that maps some inputs to the corresponding outputs. The LLM attempts to follow this pattern in making predictions, provided that the prompt includes a sufficient number of demonstrations, although generally small. It is also</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 101</p></div>
<div class="para"><p>possible to use simpler patterns to achieve this. For example, one can use the following few-shot learning prompt for translating words from Chinese to English.</p></div>
<div class="para"><p>DEMO 现 在<em>→</em>now 来<em>→</em>come 去<em>→</em>go 男 孩<em>→</em>boy</p></div>
<div class="para"><p>USER 女 孩<em>→</em></p></div>
<div class="para"><p>If the LLM is powerful enough, few-shot learning can enable it to address complex problems, such as mathematical reasoning. For example, consider the following task of summing two numbers and then dividing the sum by their product.</p></div>
<div class="para"><p>DEMO 12 5<em>→</em>(12 + 5)<em>/</em>(12<em> ×</em> 5) = 0<em>.</em>283 3 1<em>→</em>(3 + 1)<em>/</em>(3<em> ×</em> 1) = 1<em>.</em>33<em>−</em>9 4<em>→</em>(<em>−</em>9 + 4)<em>/</em>(<em>−</em>9<em> ×</em> 4) = 0<em>.</em>138 15 15<em>→</em>(15 + 15)<em>/</em>(15<em> ×</em> 15) = 0<em>.</em>133</p></div>
<div class="para"><p>USER 19 73<em>→</em></p></div>
<div class="para"><p>In many practical applications, the effectiveness of in-context learning relies heavily on the quality of prompts and the fundamental abilities of pre-trained LLMs. On one hand, we need a significant prompt engineering effort to develop appropriate prompts that help LLMs learn more effectively from demonstrations. On the other hand, stronger LLMs can make better use of incontext learning for performing new tasks. For example, suppose we wish to use an LLM to translate words from Inuktitut to English. If the LLM lacks pre-training on Inuktitut data, its understanding of Inuktitut will be weak, and it will be difficult for the model to perform well in translation regardless of how we prompt it. In this case, we need to continue training the LLM with more Inuktitut data, rather than trying to find better prompts.</p></div>
<div class="para"><p>It might be interesting to explore how in-context learning emerges during pre-training and why it works during inference. One simple understanding is that LLMs have gained some knowledge of problem-solving, but there are many possible predictions, which are hard to distinguish when the models confront new problems. Providing demonstrations can guide the LLMs to follow the“correct” paths. Furthermore, some researchers have tried to interpret in-context learning from several different perspectives, including Bayesian inference [Xie et al., 2022], gradient descent[Dai et al., 2023; Von Oswald et al., 2023], linear regression [Akyürek et al., 2023], meta learning[Garg et al., 2022], and so on.</p></div>
<div class="para"><p><strong>3.1.3</strong> <strong>Prompt Engineering Strategies</strong></p></div>
<div class="para"><p>Designing prompts is highly empirical. In general, there are many ways to prompt an LLM for performing the same task, and we need to perform a number of trial-and-error runs to find a satisfactory prompt. To write good prompts more efficiently, one can follow certain strategies.Examples of common prompting principles include</p></div>
<div class="para"><p>102 <em>Prompting</em></p></div>
<div class="para"><p>•<strong> Describing the task as clearly as possible</strong>. When we apply an LLM to solve a problem,we need to provide a precise, specific, and clear description of the problem and instruct the LLM to perform as we expect. This is particularly important when we want the output of the LLM to meet certain expectations. For example, suppose we are curious about climate change. A simple prompt for asking the LLM to provide some information is</p></div>
<div class="para"><p>Tell me about climate change.</p></div>
<div class="para"><p>Since this instruction is too general, the LLM may generate a response that addresses any aspect of climate change, which may not align with our specific interests. In this case, we can instead use prompts that are specific and detailed. One such example is</p></div>
<div class="para"><p>Provide a detailed explanation of the causes and effects of climate change,including the impact on global temperatures, weather patterns, and sea levels. Also, discuss possible solutions and actions being taken to mitigate these effects.</p></div>
<div class="para"><p>Now suppose we intend to explain climate change to a 10-year-old child. We can adjust the above prompt further.</p></div>
<div class="para"><p>Explain the causes and effects of climate change to a 10-year-old child.Talk about how it affects the weather, sea levels, and temperatures. Also,mention some things people are doing to help. Try to explain in simple terms and do not exceed 500 words.</p></div>
<div class="para"><p>•<strong> Guiding LLMs to think</strong>. LLMs have exhibited surprisingly good capabilities to “think”.A common example is that well-developed LLMs have achieved impressive performance in mathematical reasoning tasks, which are considered challenging. In prompt engineering,the “thinking” ability of LLMs needs to be activated through appropriate prompting, especially for problems that require significant reasoning efforts. In many cases, an LLM that is instructed to “think” can produce completely different results compared with the same LLM that is instructed to perform the task straightforwardly. For example, Kojima et al.[2022] found that simply appending “Let’s think step by step” to the end of each prompt can improve the performance of LLMs on several reasoning tasks. LLMs can be prompted to “think” in a number of ways. One method is to instruct LLMs to generate steps for reasoning about the problem before reaching the final answer. For example, consider a task of solving mathematical problems. See below for a simple prompt for this task.</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 103</p></div>
<div class="para"><p>You are a mathematician. You will be provided with a math problem.Please solve the problem.</p></div>
<div class="para"><p>Since solving math problems requires a detailed reasoning process, LLMs would probably make mistakes if they attempted to work out the answer directly. So we can explicitly ask LLMs to follow a given reasoning process before coming to a conclusion.</p></div>
<div class="para"><p>You are a mathematician. You will follow these detailed reasoning steps when solving math problems.</p></div>
<div class="para"><p>Step 1: Problem Interpretation.The mathematician carefully listens to your query and understands the intricate details of the mathematical challenge you have presented.</p></div>
<div class="para"><p>Step 2: Strategy Formulation.Drawing upon their extensive knowledge, the mathematician chooses the most effective strategy tailored to the type of math problem, whether it is algebra, calculus, or geometry.</p></div>
<div class="para"><p>Step 3: Detailed Calculation.With precision and expertise, the mathematician performs the necessary calculations step by step, adhering to all mathematical principles.</p></div>
<div class="para"><p>Step 4: Solution Review.Before providing the final answer, the mathematician meticulously checks the calculations for accuracy and offers a concise explanation or rationale for the solution.</p></div>
<div class="para"><p>You will be provided with a math problem. Please solve the problem.</p></div>
<div class="para"><p><em>{∗</em>problem<em>∗}</em></p></div>
<div class="para"><p>Another method to guide LLMs to “think” is through multiple rounds of interaction with LLMs. For example, as a first step, we can instruct LLMs to solve the problem directly</p></div>
<div class="para"><p>You will be provided with a math problem. Please solve the problem.</p></div>
<div class="para"><p><em>{∗</em>problem<em>∗}</em></p></div>
<div class="para"><p>Now we have an initial answer to the problem. As a second step, we prompt LLMs to evaluate the correctness of the answer and, if necessary, rework it to find a better solution.</p></div>
<div class="para"><p>104 <em>Prompting</em></p></div>
<div class="para"><p>You will be provided with a math problem, along with a solution. Evaluate the correctness of this solution, and identify any errors if present. Then,work out your own solution.</p></div>
<div class="para"><p>Problem:<em> {∗</em>problem<em>∗}</em></p></div>
<div class="para"><p>Solution:<em> {∗</em>solution<em>∗}</em></p></div>
<div class="para"><p>The prompts presented here are closely related to a long line of research on reasoning problems in LLMs. It is impossible to provide a complete discussion of all related issues because this topic covers a large family of methods. But we will see a relatively more detailed discussion on how to improve prompting through more reasoning in Section 3.2.</p></div>
<div class="para"><p>•<strong> Providing reference information</strong>. As discussed in the previous section, we can include demonstrations in prompts and allow LLMs to in-context learn from these demonstrations how to perform the task. In fact, given the remarkable ability of language understanding of LLMs, we can add any type of text into the prompts and so these models can predict based on enriched contexts. In many applications, we have various information that is relevant to user queries. Instead of using LLMs to make unconstrained predictions, we often want LLMs to produce outputs that are confined to the relevant text. One such example is RAG,where the relevant text for the user query is provided by calling an IR system, and we prompt LLMs to generate responses based on this provided relevant text. The following prompt shows an example.</p></div>
<div class="para"><p>You are an expert that can generate answers to input queries. You have now been provided with a query and the corresponding context information.Please generate an answer based on this context information. Note that you need to provide the answer in your own words, not just copy from the context provided.</p></div>
<div class="para"><p>Context information:<em> {∗</em>IR-result<em>∗}</em></p></div>
<div class="para"><p>Query:<em> {∗</em>query<em>∗}</em></p></div>
<div class="para"><p>If the context information is highly reliable, we can even restrict LLMs to answering using only the provided text. An example prompt is shown as follows</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 105</p></div>
<div class="para"><p>You are an expert tasked with generating answers from input queries. You have been provided with a query and corresponding context information,organized in a table where each row represents a useful record. Please generate an answer using only this context information. Ensure that you provide the answer in your own words.</p></div>
<div class="para"><p>Context information:<em> {∗</em>table<em>∗}</em></p></div>
<div class="para"><p>Query:<em> {∗</em>query<em>∗}</em></p></div>
<div class="para"><p>When dealing with real-world problems, we often have prior knowledge and additional information about the problems that help produce better answers. Considering such information in prompting is generally helpful in improving the result.</p></div>
<div class="para"><p>•<strong> Paying attention to prompt formats</strong>. In general, the performance of LLMs is highly sensitive to the prompts we input. Sometimes a small modification to a prompt can lead to a big change in model output. An interesting example is that changing the order of sentences in a prompt may cause LLMs to generate different results. To make prompts easy to read and reduce ambiguity, it is common to format them in a way that ensures clarity. One example is that we define several fields for prompts and fill different information in each field. Another example is we can use code-style prompts for LLMs which can understand and generate both natural language and code. See the following for a code-style prompt that performs translation where one demonstration is presented.</p></div>
<div class="para"><p>[English] = [I have an apple.]</p></div>
<div class="para"><p>[German] = [Ich habe einen Apfel.]</p></div>
<div class="para"><p>[English] = [I have an orange.]</p></div>
<div class="para"><p>[German] =</p></div>
<div class="para"><p>LLMs can receive text in various formats. This allows us to use control characters, XML tags, and specific formatting to represent complex data. And it is useful to specify how the input and output should be formatted or structured. For example, we can delimit sections of text using quotes and prompt LLMs accordingly (e.g., adding a sentence like “the input text is delimited by double quotes” to the prompt).</p></div>
<div class="para"><p>Above, we have discussed only a few strategies for writing good prompts. There are, of course,many such methods, and one needs to develop their own through practice. Interested readers can refer to various online documents for more information, such as OpenAI’s manual on the GPT series models 2.</p></div>
<div class="para"><p>2 See https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results.</p></div>
<div class="para"><p>106 <em>Prompting</em></p></div>
<div class="para"><p><strong>3.1.4</strong> <strong>More Examples</strong></p></div>
<div class="para"><p>In this subsection, we consider more examples of prompting LLMs to perform various NLP tasks.The motivation here is not to give standard prompts for these tasks, but rather to use simple examples to illustrate how LLMs can be prompted to deal with NLP problems.</p></div>
<div class="para"><p><strong>3.1.4.1</strong> <strong>Text Classification</strong></p></div>
<div class="para"><p>Text classification is perhaps one of the most common problems in NLP. Many tasks can be broadly categorized as assigning pre-defined labels to a given text. Here we consider the polarity classification problem in sentiment analysis. We choose polarity classification for illustration because it is one of the most popular and well-defined text classification tasks. In a general setup of polarity classification, we are required to categorize a given text into one of three categories: negative, positive, or neutral. Below is a simple prompt for doing this (for easy reading, we highlight the task description in the prompt).</p></div>
<div class="para"><p>Analyze the polarity of the following text and classify it as positive, negative, or neutral.</p></div>
<div class="para"><p>Text:The service at the restaurant was slower than expected, which was a bit frustrating.</p></div>
<div class="para"><p>The polarity of the text can be classified as negative.</p></div>
<div class="para"><p>To make the example complete, we show the response generated by the LLM (underlined text).</p></div>
<div class="para"><p>Although the answer is correct, the LLM gives this answer not in labels but in text describing the result. The problem is that LLMs are designed to generate text but not to assign labels to text and treat classification problems as text generation problems. As a result, we need another system to map the LLM’s output to the label space (call it<strong> label mapping</strong>), that is, we extract “negative”from “The polarity of the text can be classified as negative”. This is trivial in most cases because we can identify label words via simple heuristics. But occasionally, LLMs may not express the classification results using these label words. In this case, the problem becomes more complicated,as we need some way to map the generated text or words to predefined label words.</p></div>
<div class="para"><p>One method to induce output labels from LLMs is to reframe the problem as a cloze task. For example, the following shows a cloze-like prompt for polarity classification.</p></div>
<div class="para"><p>Analyze the polarity of the following text and classify it as positive, negative, or neutral.</p></div>
<div class="para"><p>Text:The service at the restaurant was slower than expected, which was a bit frustrating.</p></div>
<div class="para"><p>The polarity of the text is negative</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 107</p></div>
<div class="para"><p>We can use LLMs to complete the text and fill the blank with the most appropriate word. Ideally, we wish the filled word would be positive, negative, or neutral. However, LLMs are not guaranteed to generate these label words. One method to address this problem is to constrain the prediction to the set of label words and select the one with the highest probability. Then, the output label is given by</p></div>
<div class="para"><p>label=arg max <em>y</em><em>∈</em><em>Y</em> Pr(<em>y</em><em>|</em><strong>x</strong>)(3.1)</p></div>
<div class="para"><p>where<em> y</em> denotes the word filled in the blank, and<em> Y</em> denotes the set of label words<em>{</em>positive<em>,</em> negative<em>,</em> neutral<em>}</em>.</p></div>
<div class="para"><p>Another method of using LLMs to generate labels is to constrain the output with prompts. For example, we can prompt LLMs to predict within a controlled set of words. Here is an example.</p></div>
<div class="para"><p>Analyze the polarity of the following text and classify it as positive, negative, or neutral.</p></div>
<div class="para"><p>Text:The service at the restaurant was slower than expected, which was a bit frustrating.</p></div>
<div class="para"><p>What is the polarity of the text?</p></div>
<div class="para"><p>Just answer: positive, negative, or neutral.</p></div>
<div class="para"><p>Negative</p></div>
<div class="para"><p>Sentiment analysis is a common NLP problem that has probably been well understood by LLMs through pre-training or fine-tuning. Thus we can prompt LLMs using simple instructions to perform the task. However, for new classification problems, it may be necessary to provide additional details about the task, such as the classification standards, so that the LLMs can perform correctly. To do this, we can add a more detailed description of the task and/or demonstrate classification examples in the prompts. To illustrate, consider the following example.</p></div>
<div class="para"><p>108 <em>Prompting</em></p></div>
<div class="para"><p>Analyze the polarity of the following text and classify it as positive, negative, or neutral. Here’s what each category represents:</p></div>
<div class="para"><p>Positive: This indicates that the text conveys a positive emotion or attitude. For example, texts expressing happiness, satisfaction, excitement, or admiration are considered positive.</p></div>
<div class="para"><p>Negative: This refers to a text that expresses a negative emotion or attitude. It encompasses feelings of sadness, anger, frustration, or criticism.</p></div>
<div class="para"><p>Neutral: Neutral sentiment is used to describe texts that do not exhibit clear positive or negative emotions but instead convey informational, factual, or indifferent tones.</p></div>
<div class="para"><p>Text:The service at the restaurant was slower than expected, which was a bit frustrating.</p></div>
<div class="para"><p>What is the polarity of the text?</p></div>
<div class="para"><p>Negative</p></div>
<div class="para"><p>While it seems straightforward to use LLMs for classification problems, there are still issues that have not been well addressed. For example, when dealing with a large number of categories,it remains challenging to effectively prompt LLMs. Note that if we face a very difficult classification problem and have a certain amount of labeled data, fine-tuning LLMs or adopting “BERT +classifier”-like architectures is also desirable.</p></div>
<div class="para"><p><strong>3.1.4.2</strong> <strong>Information Extraction</strong></p></div>
<div class="para"><p>Many NLP problems can be regarded as information extraction problems, involving the identification or extraction of specific pieces of information from unstructured text. This information can include named entities, relationships, events, and other relevant data points. The goal of information extraction is to transform raw data into a format that can be easily analyzed and used in various downstream applications.</p></div>
<div class="para"><p>As information extraction covers a wide range of problems, we cannot discuss them all here.Instead, we start with the task of named entity recognition — a task that has long been a concern in NLP. Named entity recognition is a process that detects and classifies key information in text into specific groups. These key pieces of information, known as named entities, typically include proper names and are categorized into distinct classes such as people, locations, organizations,dates, monetary values, and percentages. Consider first a simple example of extracting person names from a text 3.</p></div>
<div class="para"><p>3 The text is from https://edition.cnn.com/travel</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 109</p></div>
<div class="para"><p>Identify all person names in the provided text.</p></div>
<div class="para"><p>Text:Is the UK really doing that badly or have travel trends shifted, post-pandemic?For Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I think the UK is doing perfectly well but we’ll see more people going to Europe,”he says of 2024, adding that there’s “allegedly a slight plateau of American demand for the UK.”</p></div>
<div class="para"><p>The person name identified in the provided text is: Tom Jenkins</p></div>
<div class="para"><p>Then, we can extract all named entities.</p></div>
<div class="para"><p>Identify and classify all named entities in the provided text into categories such as person names, locations, dates, and organizations. List each entity with its type on one line.</p></div>
<div class="para"><p>Text:Is the UK really doing that badly or have travel trends shifted, post-pandemic?For Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I think the UK is doing perfectly well but we’ll see more people going to Europe,”he says of 2024, adding that there’s “allegedly a slight plateau of American demand for the UK.”</p></div>
<div class="para"><p>1. Tom Jenkins - Person Name</p></div>
<div class="para"><p>2. European Tourism Organisation - Organization</p></div>
<div class="para"><p>3. UK - Location</p></div>
<div class="para"><p>4. Europe - Location</p></div>
<div class="para"><p>5. 2024 - Date</p></div>
<div class="para"><p>Given these identified named entities, we can further process the text using other information extraction tasks. For example, we can identify the relationships between these named entities (call it<strong> relation extraction</strong>). The corresponding prompt is shown as follows.</p></div>
<div class="para"><p>110 <em>Prompting</em></p></div>
<div class="para"><p>Given a text and a list of named entities identified within it, analyze and describe the relationships between these entities. Explain how each entity is contextually related to others.</p></div>
<div class="para"><p>Text:Is the UK really doing that badly or have travel trends shifted, post-pandemic?For Tom Jenkins, CEO of the European Tourism Organisation, it’s the latter. “I think the UK is doing perfectly well but we’ll see more people going to Europe,”he says of 2024, adding that there’s “allegedly a slight plateau of American demand for the UK.”</p></div>
<div class="para"><p>Named Entity:Tom Jenkins, European Tourism Organisation, UK, Europe, 2024</p></div>
<div class="para"><p>Tom Jenkins - European Tourism Organisation: Tom Jenkins is the CEO of the European Tourism Organisation. This indicates a professional association where Tom Jenkins likely has a significant role in directing the organization’s activities,especially those related to tourism in Europe.</p></div>
<div class="para"><p>...</p></div>
<div class="para"><p>If LLMs have been fine-tuned with instruction following for information extraction tasks, it is relatively easy to perform various information extraction tasks. For example, the following is a prompt template for information extraction.</p></div>
<div class="para"><p>You will be provided with a text. Your task is to<em> {∗</em>task-description<em>∗}</em></p></div>
<div class="para"><p>Text:<em> {∗</em>text<em>∗}</em></p></div>
<div class="para"><p>Here task-description is the instruction for the task. Examples of the instruction include</p></div>
<div class="para"><p><em>extract the keywords from the text.</em></p></div>
<div class="para"><p><em>extract the key events from the text.</em></p></div>
<div class="para"><p><em>detect the coreference and anaphoric links between entities from the text.</em></p></div>
<div class="para"><p>and so on. Of course, we can provide a more detailed description of the task to prompt LLMs more accurately.</p></div>
<div class="para"><p><strong>3.1.4.3</strong> <strong>Text Generation</strong></p></div>
<div class="para"><p>In NLP, the term<em> text generation</em> is used broadly to cover a wide range of different problems, many of which focus on generating text based on given text. These problems can be roughly categorized into two classes: 1) continual writing based on the input text (call it<strong> text completion</strong>), and 2)transformation of the input text into another text (call it<strong> text transformation</strong>). Here, we illustrate the prompts for the former and consider the prompts for the latter in the following discussion.</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 111</p></div>
<div class="para"><p>A simple example of text completion is continuing to write a story starting with a few given sentences.</p></div>
<div class="para"><p>You are a writer who is great at writing stories. You are provided with a few sentences to start with. Please continue the story from the provided sentences.</p></div>
<div class="para"><p>Start of the story:One sunny afternoon, Lily discovered a tiny, golden key hidden under a rock in her backyard. Curious, she picked it up and wondered what it might unlock. Just then, her cat, Whiskers, began to meow excitedly, leading her towards the old oak tree at the edge of the garden.</p></div>
<div class="para"><p>Lily followed Whiskers, her heart pounding with excitement and a touch of nervousness. The old oak tree had always seemed mysterious to her, its gnarled branches casting long shadows even on the brightest days. As they reached the tree, Whiskers darted ahead and started pawing at something buried beneath a pile of autumn leaves ...</p></div>
<div class="para"><p>Likewise, we can create a conversation given the conversational context.</p></div>
<div class="para"><p>You are provided with a conversation between a tourist and a taxi driver. Please complete the conversation.</p></div>
<div class="para"><p>Tourist: Could you take me to the downtown museum, please?</p></div>
<div class="para"><p>Taxi Driver: Of course! First time in the city?</p></div>
<div class="para"><p>Tourist: Yes, any must-see places around?</p></div>
<div class="para"><p>Taxi Driver: Definitely check out the botanical gardens after the museum.They’re nearby and quite beautiful.</p></div>
<div class="para"><p>Tourist: Great, thanks for the tip!</p></div>
<div class="para"><p>Taxi Driver: No problem. Enjoy your visit!</p></div>
<div class="para"><p>Another application of text completion is generating text based on user requirements. The requirements can include style, tone, length, and any specific content that the text should contain.For example, users may need a formal report, a creative story, or a piece of programming code.Here is a prompt for generating a five-character regulated Chinese poem.</p></div>
<div class="para"><p>112 <em>Prompting</em></p></div>
<div class="para"><p>You are a poet. Please write a traditional Chinese poem. Here is the requirement.</p></div>
<div class="para"><p>Theme: Spring’s rejuvenation</p></div>
<div class="para"><p>Structure: Five-character regulated poem</p></div>
<div class="para"><p>Emotion: Convey a sense of happiness and renewal through vivid spring imagery.</p></div>
<div class="para"><p>春 意 盎 然</p></div>
<div class="para"><p>翠 柳 风 中 舞，</p></div>
<div class="para"><p>红 花 雨 后 新。</p></div>
<div class="para"><p>山 明 水 又 绿，</p></div>
<div class="para"><p>天 宽 鸟 自 频。</p></div>
<div class="para"><p>If the LLM is trained to generate language and code, we can prompt it to perform code completion tasks. Here is an example.</p></div>
<div class="para"><p>Please write a Python function to calculate the average of a list of numbers.</p></div>
<div class="para"><p>def calculate_average(numbers):</p></div>
<div class="para"><p>if numbers:# Check if the list is not empty return sum(numbers) / len(numbers)else:return 0 # Return 0 if the list is empty</p></div>
<div class="para"><p><strong>3.1.4.4</strong> <strong>Text Transformation</strong></p></div>
<div class="para"><p>Typical text transformation tasks include machine translation, summarization, text style transfer,and so on. The prompts used to perform these tasks are not significantly different from the ones presented previously. Here, we present a few examples to showcase these prompts.</p></div>
<div class="para"><p>Translate the following text from English to Spanish.</p></div>
<div class="para"><p>Text:</p></div>
<div class="para"><p>The quick brown fox jumps over the lazy dog.</p></div>
<div class="para"><p>El rápido zorro marrón salta sobre el perro perezoso.</p></div>
<div class="para"><p><em>3.1 General Prompt Design</em> 113</p></div>
<div class="para"><p>Summarize the following article in no more than 50 words:</p></div>
<div class="para"><p>Article:</p></div>
<div class="para"><p>In recent years, urban areas have been facing significant challenges related to sustainability and environmental management. Cities are experiencing higher rates of pollution, increased traffic congestion, and greater demands on infrastructure.This growth has led to numerous environmental issues, including elevated levels of air and water pollution, increased waste production, and strained public services ...</p></div>
<div class="para"><p>Urban areas are grappling with sustainability challenges, such as rising pollution,traffic congestion, and infrastructure demands ...</p></div>
<div class="para"><p>Rewrite this text in a formal tone.</p></div>
<div class="para"><p>Text:</p></div>
<div class="para"><p>Hey, what’s up? Long time no see!</p></div>
<div class="para"><p>Hello, how have you been? It has been quite some time since we last met!</p></div>
<div class="para"><p><strong>3.1.4.5</strong> <strong>Question Answering</strong></p></div>
<div class="para"><p>The question-answering format is inherently simple. For a given question, there is an answer that corresponds to it. For example, in open-domain question answering, we expect the system to return an answer in response to a user-submitted question. Prompt templates for general-purpose question answering can be:</p></div>
<div class="para"><p><em>{∗</em>question<em>∗}</em></p></div>
<div class="para"><p>Question:<em> {∗</em>question<em>∗}</em></p></div>
<div class="para"><p>Answer:</p></div>
<div class="para"><p>Question answering is important in NLP because many problems can be framed as questionanswering tasks. In particular, many recent reasoning tasks are defined in the form of question answering. For example, in the MMLU benchmark [Hendrycks et al., 2021], each example consists of a multiple-choice question, and LLMs are required to select the correct answer. See the following for an example prompt for answering a question in this dataset.</p></div>
<div class="para"><p>114 <em>Prompting</em></p></div>
<div class="para"><p>(<em>Z,</em><em> ∗</em>) is a group with<em> a</em><em> ∗</em><em>b</em> =<em> a</em> +<em> b</em> + 1 for all<em> a</em>,<em> b</em> in<em> Z</em>. The inverse of<em> a</em> is</p></div>
<div class="para"><p>(A) 0</p></div>
<div class="para"><p>(B)<em> −</em>2</p></div>
<div class="para"><p>(C)<em> a</em><em> −</em>2</p></div>
<div class="para"><p>(D) (2 +<em> a</em>)<em> ∗−</em>1</p></div>
<div class="para"><p>D</p></div>
<div class="para"><p>Another widely-used benchmark is the GSM8K dataset [Cobbe et al., 2021]. It consists of thousands of grade school math word problems. For each problem, we use LLMs to return a solution in natural language. For example, the following is a prompt used for solving a problem in GSM8K.</p></div>
<div class="para"><p>DEMO Q: Jerry’s two daughters play softball on different teams. They each have 8 games this season. Each team practices 4 hours for every game they play. If each game lasts for 2 hours, how many hours will Jerry spend at the field watching his daughters play and practice altogether?</p></div>
<div class="para"><p>A: Jerry will spend 8 games * 2 hours per game =<em> ≪</em>8<em> ∗</em>2 = 16<em> ≫</em>16 hours watching one daughter play her games. He will spend 16<em> ∗</em>2 =<em>≪</em>16<em> ∗</em>2 = 32<em> ≫</em>32 hours watching both daughters play their games. He will spend 8 games * 4 hours of practice =<em> ≪</em>8<em> ∗</em>4 = 32<em> ≫</em>32 hours watching one daughter practice. He will spend 32<em>∗</em>2 =<em>≪</em>32<em>∗</em>2 = 64<em> ≫</em>64 hours watching both daughters practice. He will spend a total of 32 hours watching games + 64 hours watching practice =<em>≪</em>32 + 64 =96<em> ≫</em>96 hours. #### 96</p></div>
<div class="para"><p>Q: Mary bought six apples from the store. From the apples she bought,for each that Mary ate, she planted two trees from the remaining ones.How many apples did Mary eat?</p></div>
<div class="para"><p>A: She planted eight trees. This means she used half of that amount of apples, which is 8 trees<em> /</em>2 trees/apple =<em>≪</em>8<em>/</em>2 = 4<em> ≫</em>4 apples. That means that she planted four of the six apples she bought, leaving only 6 apples<em> −</em>4 apples =<em>≪</em>6<em> −</em>4 = 2<em> ≫</em>2 apples to be eaten. #### 2</p></div>
<div class="para"><p>USER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. If Boris gives Beck 10 apples, how many fewer apples does Beck have than Boris now?</p></div>
<div class="para"><p>A: Boris starts with 100 apples and gives Beck 10 apples, leaving him with 100 apples<em> −</em>10 apples =<em>≪</em>100<em> −</em>10 = 90<em> ≫</em>90 apples.Initially,Beck has 23 fewer apples than Boris,giving him a starting count of 100<em> −</em>23 =<em>≪</em>100<em> −</em>23 = 77<em> ≫</em>77 apples.After receiving 10 apples from Boris, Beck’s total comes to 77 apples +10 apples =<em>≪</em>77 + 10 = 87<em> ≫</em>87 apples. Consequently, Beck now has 90<em> −</em>87 =<em>≪</em>90<em> −</em>87 = 3<em> ≫</em>3 fewer apples than Boris. #### 3</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 115</p></div>
<div class="para"><p>Here a few-shot prompt is adopted. The LLM learns from these demonstrations of problemsolution pairs not only the way of problem-solving but also the way of formatting the output. For example, the final result of calculation follows the #### token, and<em> ≪</em><em>...</em><em> ≫</em>annotates the detailed calculation steps (called<strong> calculation annotation</strong>)4.</p></div>
<h3>3.2 Advanced Prompting Methods</h3>
<div class="para"><p>So far in this chapter, we have introduced the basic concepts related to LLM prompting and presented a number of prompts for NLP tasks. We now consider several techniques for enhancing the effectiveness of prompting.</p></div>
<div class="para"><p><strong>3.2.1</strong> <strong>Chain of Thought</strong></p></div>
<div class="para"><p>We have encountered the concept of<strong> chain of thought</strong> (<strong>CoT</strong>) several times in this chapter and previous ones [Wei et al., 2022c; Chowdhery et al., 2022]. CoT methods provide a simple way to prompt LLMs to generate step-by-step reasoning for complex problems, thereby approaching tasks in a more human-like manner. Rather than coming to a conclusion directly, the CoT methods instruct LLMs to generate reasoning steps or to learn from demonstrations of detailed reasoning processes provided in the prompts. To illustrate CoT, we consider the problem of algebraic calculation, as commonly described in the literature. Suppose we are given an algebraic problem</p></div>
<div class="para"><p><em>Calculate the average of the numbers 2, 4, and 6.</em></p></div>
<div class="para"><p>We can consider it as the question and prompt an LLM to answer it.</p></div>
<div class="para"><p>Q: Please calculate the average of the numbers 2, 4, and 9.</p></div>
<div class="para"><p>A: The answer is 6.</p></div>
<div class="para"><p>It seems difficult for the LLM to directly give a correct answer. A simple improvement is to add demonstrations of similar problems in the prompt, and thus the LLM can learn from these demonstrations.</p></div>
<div class="para"><p>Q: Please calculate the average of the numbers 1, 3, 5, and 7.</p></div>
<div class="para"><p>A: The answer is 4.</p></div>
<div class="para"><p>Q: Please calculate the average of the numbers 2, 4, and 9.</p></div>
<div class="para"><p>A: The answer is 7.</p></div>
<div class="para"><p>The problem here is that, although we have shown a similar question-answer pair, it remains difficult for the LLM to reason out the correct answer. In CoT, not only can LLMs learn from the</p></div>
<div class="para"><p>4 During prediction, a calculator is used when we see<em> ≪</em><em>...</em><em> ≫</em>. More specifically, once the LLM encounters “=”in a<em> ≪</em><em>...</em><em> ≫</em>, then the calculator calculates the expression on the left-hand side of “=”. This method helps reduce the calculation errors made by LLMs.</p></div>
<div class="para"><p>116 <em>Prompting</em></p></div>
<div class="para"><p>correspondence between questions and answers but they may gain more from detailed problemsolving steps that are used to derive the answers. To do this, we can incorporate some reasoning steps into the prompt to obtain a CoT prompt.</p></div>
<div class="para"><p>Q: Please calculate the mean square of the numbers 1, 3, 5, and 7.</p></div>
<div class="para"><p>A: Calculate the square of each number: 1 2 = 1, 3 2 = 9, 5 2 = 25, and 7 2 = 49.Sum the squares, 1 + 9 + 25 + 49 = 84. There are 4 numbers in total. Divide the sum by the number of items, 84<em>/</em>4 = 21. The answer is 21.</p></div>
<div class="para"><p>Q: Please calculate the average of the numbers 2, 4, and 9.</p></div>
<div class="para"><p>A: Calculate 2 + 4 + 9, which equals 15. There are three numbers. Divide the total sum by the count, resulting in 15<em>/</em>3 = 5. The answer is 5.</p></div>
<div class="para"><p>Here we highlight the reasoning steps in green. By providing a detailed reasoning process for a similar problem, the LLM learns to reason and thereby generates a problem-solving path that leads to the correct answer.</p></div>
<div class="para"><p>There are several benefits of using CoT prompting.First, CoT allows LLMs to decompose complex problems into smaller, sequential reasoning steps. This somewhat mirrors human problem-solving behaviors, making it particularly effective for tasks requiring detailed, multi-step reasoning. Second, CoT makes the reasoning process more transparent and interpretable. Since all reasoning steps are visible, we can understand and interpret how a conclusion was reached. Third,if users can follow the logic behind the reasoning process, they will be more likely to trust the predictions of an LLM. This is particularly important when applying LLMs in fields like medicine,education, and finance. Fourth, CoT is an in-context learning approach, and thus, it is applicable to most well-trained, off-the-shelf LLMs. Moreover, CoT provides efficient ways to adapt LLMs to different types of problems. It can even inspire more creative solutions by exploring various alternative reasoning paths, which might not be obvious when arriving at a conclusion directly.</p></div>
<div class="para"><p>The method described above requires providing one or more examples of CoT reasoning, typically called the few-shot CoT method. By contrast, the zero-shot CoT method does not require such examples. It instead prompts LLMs to reason step-by-step by incorporating specific instructions in prompts. For example, below is a zero-shot CoT prompt.</p></div>
<div class="para"><p>Q: Please calculate the average of the numbers 2, 4, and 9.</p></div>
<div class="para"><p>A: Let’s think step-by-step.</p></div>
<div class="para"><p>We have three numbers:2, 4, and 9.Add these numbers together,2 + 4 + 9 = 15. Determine how many numbers there are, which in this case is three. The average is calculated by dividing the total sum by the number of elements. Completing the division gives 15<em>/</em>3 = 5. So the answer is 5.</p></div>
<div class="para"><p>Following the instruction “Let’s think step by step”, the LLM is prompted to generate detailed reasoning steps. As discussed in Kojima et al. [2022]’s work, prompting with such instructions may result in LLMs generating only the reasoning steps without a clear conclusion. In this case,a second round of prompting can be used to extract the answer from these reasoning steps. For example, Kojima et al. [2022] create a second prompt which combines both the input and output</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 117</p></div>
<div class="para"><p>in the first round of prompting. Using this combined input, the LLM can continue its reasoning process and then generate the correct answer. Furthermore, it is possible to prompt LLMs to reason using instructions other than “Let’s think step by step”, such as “Let’s think logically” and“Please show me your thinking steps first”.</p></div>
<div class="para"><p>While we have illustrated CoT methods using an algebraic reasoning problem, these methods can be applied to a variety of different problems. Typical problem-solving scenarios for CoT include mathematical reasoning, logical reasoning, commonsense reasoning, symbolic reasoning,code generation, and so on. See Figure 3.1 for more examples of applying CoT in various tasks.</p></div>
<div class="para"><p>CoT today is one of the most active fields of prompt engineering. This has not only led to improved performance for LLM prompting but has opened the door to a wide range of methods for studying and verifying reasoning capabilities of LLMs. Although we have focused on the basic idea of CoT in this section, it can be improved in several ways. For example, we can consider the reasoning process as a problem of searching through many possible paths, each of which may consist of multiple intermediate states (i.e., reasoning steps). In general, we wish the search space to be well-defined and sufficiently large, so that we are more likely to find the optimal result. For this reason, an area of current LLM research is aimed at designing better structures for representing reasoning processes, allowing LLMs to tackle more complex reasoning challenges.These structures include tree-based structures [Yao et al., 2024], graph-based structures [Besta et al., 2024], and so on. By using these compact representations of reasoning paths, LLMs can explore a wider range of decision-making paths, analogous to System 2 thinking 5. Another line of research focuses on prompting LLMs with multi-round interactions. This involves decomposing complex problems into sub-problems, verifying and refining model outputs, employing model ensembling, and so on. Note that these methods and the issues involved are not limited to CoT. In fact, they are often used as more general approaches to improving LLMs, while CoT can be seen as a way to test the capabilities of LLMs. We will see discussions of some of these issues in the following subsections.</p></div>
<div class="para"><p>Before leaving our discussion of CoT, we should consider its practical limitations. One of them is the need for detailed, multi-step reasoning demonstrations in few-shot CoT scenarios,which may be difficult to obtain, either automatically or manually. Also, there is no standard method for breaking down complex problems into simpler problem-solving steps. This often heavily depends on the user’s experience. In addition, errors in intermediate steps can also affect the accuracy of the final conclusion. For further discussion on the pros and cons of CoT, the interested reader can refer to recent surveys on this topic [Chu et al., 2023; Yu et al., 2023; Zhang et al., 2023a].</p></div>
<div class="para"><p><strong>3.2.2</strong> <strong>Problem Decomposition</strong></p></div>
<div class="para"><p>We have seen that LLMs can benefit from solving a complex problem by breaking it down into simpler problem-solving tasks. Such an approach can be seen as an example of a broader paradigm known as<strong> problem decomposition</strong>, which has been extensively explored and discussed in psychology and computer science. From the psychological perspective, complex problem-solving refers to a process of addressing a problem using knowledge that helps overcome the barriers of</p></div>
<div class="para"><p>5 System 1 and System 2 thinking, as described by Kahneman [2011], represent two different modes of cognitive processing. System 1 is fast, automatic, intuitive, and emotional. This mode of thinking operates effortlessly and quickly, and is often what guides our daily decisions, judgments, and impressions. System 2 is slow, deliberate, and analytical. It is activated when we need to perform complex computations.</p></div>
<h3>118 Prompting</h3>
<div class="para"><p>Q: What home entertainment equipment requires cable?Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet A: The answer must require cable. Of the above choices, only television requires cable.So the answer is (c).</p></div>
<div class="para"><p>Q: Do hamsters provide food for any animals?A: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the answer is yes.</p></div>
<div class="para"><p>Q: Can you use Microsoft Office without internet?A: Yes, you can use Microsoft Office applications , although features like cloud storage and online collaboration require internet. So the answer is yes.</p></div>
<div class="para"><p>Q: What type of person typically contracts illness?Answer Choices: (a) hospital (b) head (c)sick person (d) elderly person (e) doctor’s office A:The answer must be a person susceptible to illness. Of the choices, &quot;elderly per son&quot; is correct because they typically have weaker immune systems.So the answer is (d).</p></div>
<div class="para"><p>(a) CSQA(b) StrategyQA</p></div>
<div class="para"><p>Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: [<em> {</em>A: Let’s think step by step.0: empty stack 1: [ ; stack: [2:<em> {</em> ; stack: [<em> {</em>So the answer is<em> }</em> ].</p></div>
<div class="para"><p>Q: Take the last letters of the words in &quot;Elon Musk&quot; and concatenate them.A: The last letter of &quot;Elon&quot; is &quot;n&quot;. The last letter of &quot;Musk&quot; is &quot;k&quot;. Concatenating them is &quot;nk&quot;. The answer is nk.</p></div>
<div class="para"><p>Q: Take the last letters of each word in &quot;Renata Mariela Mona Kristin&quot; and concatenate them.A: The last letter of &quot;Renata&quot; is &quot;a&quot;. The last letter of &quot;Mariela&quot; is &quot;a&quot;. The last letter of &quot;Mona&quot; is &quot;a&quot;. The last letter of&quot;Kristin&quot; is &quot;n&quot;. Concatenating them is&quot;aaan&quot;. The answer is aaan.</p></div>
<div class="para"><p>Q: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input:<em> &lt;</em> [ [A: Let’s think step by step.0: empty stack 1:<em> &lt;</em> ; stack:<em> &lt;</em>2: [ ; stack:<em> &lt;</em> [3: [ ; stack:<em> &lt;</em> [ [So the answer is ] ]<em> &gt;</em>.</p></div>
<div class="para"><p>(c) Dyck languages(d) Last Letter Concatenation</p></div>
<div class="para"><p><strong>Fig. 3.1:</strong> CoT in four different reasoning tasks, including CSQA, StrategyQA, Dyck languages, and Last Letter Concatenation. The CoT parts are highlighted in green.</p></div>
<h3>the problem 6. There are generally no standard or clear paths to a solution for a complex prob- lem. However, it is often advantageous to employ strategies that decompose the problem, thereby making it easier to tackle the corresponding sub-problems with less effort. For example, consider writing a blog about the risks of AI. If we simply prompt an LLM with the instruction “Please write a blog about the risks of AI”, the LLM may generate a blog with arbitrary structures and</h3>
<div class="para"><p>6 A relatively formal definition can be found in Frensch and Funke [2014]’s book:<em> complex problem-solving occurs</em> <em>to overcome barriers between a given state and a desired goal state by means of behavioral and/or cognitive, multi-step</em> <em>activities</em>.</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 119</p></div>
<div class="para"><p>writing styles. A better method, instead, could be to outline the blog and provide more detailed information about each section. Consider the following prompt</p></div>
<div class="para"><p>You are a blog writer. Please follow the provided outline below to write a blog about the risks of AI.</p></div>
<div class="para"><p>• Introduction Introduce AI, its relevance, and the importance of understanding its risks for youth.</p></div>
<div class="para"><p>• Privacy Concerns Discuss how AI might compromise personal privacy through interactions online.</p></div>
<div class="para"><p>• Misinformation Explore AI’s role in spreading misinformation and influencing young people’s decisions.</p></div>
<div class="para"><p>• Cyberbullying Highlight how AI tools can be utilized in cyberbullying and the impact on mental health.</p></div>
<div class="para"><p>• Tips for Safe AI Use Offer guidelines for responsible AI usage and promote critical thinking.</p></div>
<div class="para"><p>• Conclusion Recap main points and encourage proactive engagement with AI ethics.</p></div>
<div class="para"><p>Here we give the title and major points for each section. Then, the LLM can use this structure to break down the writing task by filling in content for these sections. Note that the way to structure the blog can be provided by humans or even generated automatically. For example, we can use the LLM to first generate the outline, and then ask it to follow this outline to complete the writing.</p></div>
<div class="para"><p>In computer science, decomposing complex problems is a commonly used strategy in software and hardware system design. A well-known example is the divide-and-conquer paradigm, which is often used to design algorithms for computation problems that can be reduced to simpler, more manageable problems. For example, consider a problem of determining whether a document discusses the risks of AI. We can instruct the LLM with the following prompt.</p></div>
<div class="para"><p>You are provided with a text. Please determine whether it discusses the risks of AI.</p></div>
<div class="para"><p><em>{∗</em>document<em>∗}</em></p></div>
<div class="para"><p>If the document is long, the computation will be expensive. Alternatively, we can divide the document into relatively short segments and perform the same task on each segment. These segments can be processed in parallel to further reduce the computational cost. Next, we determine</p></div>
<div class="para"><p>120 <em>Prompting</em></p></div>
<div class="para"><p>the relevancy of each segment to the topic of AI risks. The final output is then generated using another prompt.</p></div>
<div class="para"><p>Your task is to determine whether a text discusses the risks of AI. This text has been divided into segments, and you have obtained the relevancy of each segment to the topic of AI risks. Based on this, please provide your final result.</p></div>
<div class="para"><p>Segment 1:<em> {∗</em>relevancy-to-the-topic1<em>∗}</em></p></div>
<div class="para"><p>Segment 2:<em> {∗</em>relevancy-to-the-topic2<em>∗}</em></p></div>
<div class="para"><p>Segment 3:<em> {∗</em>relevancy-to-the-topic3<em>∗}</em></p></div>
<div class="para"><p>...</p></div>
<div class="para"><p>Now let us return to a more general discussion of problem decomposition in prompting. While problem decomposition can be applied to various NLP problems, it has been more extensively discussed and tested in reasoning tasks recently. For complex reasoning tasks, we often need a multi-step reasoning path to reach a correct conclusion. We can use LLMs to achieve this in three different ways. First, LLMs can directly reach the conclusion. In other words, they can predict without explicit reasoning processes, and there is a hidden and uninterpretable reasoning mechanism. Second, LLMs are prompted to generate a multi-step reasoning path that leads to the conclusion, like CoT. However, we run LLMs just once, and all intermediate steps in reasoning are generated in a single prediction. Third, we break down the original problem into a number of sub-problems, which are either addressed in separate runs of LLMs or tackled using other systems.Here we focus our attention on the third approach, which is closely related to problem decomposition. Note, however, that a more comprehensive discussion could cover all these approaches,while the first two have been discussed to some extent in this chapter.</p></div>
<div class="para"><p>A general framework for problem decomposition involves two elements.</p></div>
<div class="para"><p>•<strong> Sub-problem Generation</strong>. This involves decomposing the input problem into a number of sub-problems.</p></div>
<div class="para"><p>•<strong> Sub-problem Solving</strong>. This involves solving each sub-problem and deriving intermediate and final conclusions through reasoning.</p></div>
<div class="para"><p>These two issues can be modeled in different ways, leading to various problem decomposition methods. One approach is to treat them as separate steps in a two-step process. For example,consider the blog writing task described at the beginning of this subsection. In the first step, we decompose the entire problem into sub-problems all at once (i.e., outline the blog). In the second step, we solve the sub-problems either sequentially or in another order (i.e., fill in content for each section as needed). The final output of this process combines the results from solving each sub-problem. While this method is simple and straightforward, it assumes that the problem is compositional, making it more suitable for tasks like writing and code generation.</p></div>
<div class="para"><p>However, many real-world problems require complex reasoning. One key characteristic of these problems is that the reasoning steps may not be fixed. The reasoning path can vary for different problems, and each step of reasoning may depend on the outcomes of prior steps. In</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 121</p></div>
<div class="para"><p>such cases, it is undesirable to use fixed sub-problem generation in advance. Instead, sub-problems should be generated dynamically based on the input problem, and, if possible, generated on the fly during the reasoning process. This makes problem decomposition more challenging compared with designing divide-and-conquer algorithms. Ideally, we would like to jointly design both the systems for sub-problem generation and sub-problem solving. But a more practical and widely used approach is to adopt separate models for these tasks. A straightforward way to achieve this is to adapt an LLM for these tasks by either prompting or tuning the model.</p></div>
<div class="para"><p>Here we consider a method based on the above idea, called<strong> least-to-most prompting</strong> [Zhou et al., 2023b]. The motivation for this method arises from the challenges of solving difficult reasoning problems — those that cannot be addressed by simply generalizing from a few examples.For these problems, a more effective problem-solving strategy is to follow a progressive sequence of sub-problems that systematically lead to the conclusion. More specifically, in the least-to-most prompting method, sub-problem generation is performed by prompting an LLM with instructions and/or demonstrations. For example, below is a 2-shot prompt for sub-problem generation in least-to-most prompting.</p></div>
<div class="para"><p>TASK Your task is to decompose a problem into several sub-problems. You will be given a few examples to illustrate how to achieve this.</p></div>
<div class="para"><p>DEMO Q: In a community, 5% of the population are infants, 15% are children,40% are adults, and 40% are seniors. Which group makes up the largest portion of the population?</p></div>
<div class="para"><p>A: To answer the question “Which group makes up the largest portion of the population?”, we need to know: “How many percent are infants?”, “How many percent are children?”, “How many percent are adults?”, “How many percent are seniors?”.</p></div>
<div class="para"><p>Q: Alice, Bob, and Charlie brought beads for their group project in their craft class. Alice has twice as many beads as Bob, and Bob has five times as many beads as Charlie. If Charlie has 6 beads, how many beads can they use for their craft project?</p></div>
<div class="para"><p>A: To answer the question “How many beads can they use for their craft project?”, we need to know: “How many beads does Bob have?”, “How many beads does Alice have?”.</p></div>
<div class="para"><p>USER Q: The environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2.3 degrees Celsius. What was the duration of the environmental study?</p></div>
<div class="para"><p>A: To answer the question “What was the duration of the environmental study?”, we need to know: “When did the environmental study start?”,“When did the environmental study end?”.</p></div>
<div class="para"><p>By learning from the examples, the LLM can generate two sub-problems for answering the new problem “What was the duration of the environmental study?” (highlighted in blue and orange). Given these sub-problems, we solve them sequentially. For each sub-problem, we take all previously-generated QA pairs as context, and then produce the answer. For the example above,</p></div>
<div class="para"><p>122 <em>Prompting</em></p></div>
<div class="para"><p>we need to answer the first sub-problem by prompting the LLM, like this</p></div>
<div class="para"><p>The environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2.3 degrees Celsius.</p></div>
<div class="para"><p>SUB-PROB1 Q: When did the environmental study start?</p></div>
<div class="para"><p>A: The environmental study started in 2015.</p></div>
<div class="para"><p>Once we have the answer to the first sub-problem, we proceed to the second one. This time,we include both the first sub-problem and its corresponding answer in the input.</p></div>
<div class="para"><p>The environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2.3 degrees Celsius.</p></div>
<div class="para"><p>SUB-PROB1 Q: When did the environmental study start?</p></div>
<div class="para"><p>A: The environmental study started in 2015.</p></div>
<div class="para"><p>SUB-PROB2 Q: When did the environmental study end?</p></div>
<div class="para"><p>A: The environmental study ended in 2020.</p></div>
<div class="para"><p>Finally, we use the LLM to solve the original problem given the answers to all the subproblems.</p></div>
<div class="para"><p>The environmental study conducted from 2015 to 2020 revealed that the average temperature in the region increased by 2.3 degrees Celsius.</p></div>
<div class="para"><p>SUB-PROB1 Q: When did the environmental study start?</p></div>
<div class="para"><p>A: The environmental study started in 2015.</p></div>
<div class="para"><p>SUB-PROB2 Q: When did the environmental study end?</p></div>
<div class="para"><p>A: The environmental study ended in 2020.</p></div>
<div class="para"><p>FINAL Q: What was the duration of the environmental study?</p></div>
<div class="para"><p>A: The duration of the environmental study was 5 years.</p></div>
<div class="para"><p>The least-to-most method offers a basic approach to prompting LLMs to generate and solve sub-problems separately. We can improve it in several ways. One simple improvement is to apply various advanced prompting techniques, which do not require changes to the problem decomposition framework. For example, we can incorporate CoT into the prompting to enhance the reasoning performance of sub-problem generation and solving.</p></div>
<div class="para"><p>Another improvement is to explore methods for better decomposing problems and organizing problem-solving paths. To describe these approaches, we will use the symbol<em> p</em> 0 to denote the</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 123</p></div>
<div class="para"><p>input problem, and use the symbols<em> {</em><em>p</em> 1<em>, ..., p</em> <em>n</em><em>}</em> to denote the sub-problems corresponding to<em> p</em> 0.For least-to-most prompting, we decompose<em> p</em> 0 into<em> {</em><em>p</em> 1<em>, ..., p</em> <em>n</em><em>}</em>, given by</p></div>
<div class="para"><p><em>{</em><em>p</em> 1<em>, ..., p</em> <em>n</em><em>}</em>=<em>G</em>(<em>p</em> 0)(3.2)</p></div>
<div class="para"><p>where<em> G</em>(<em>·</em>) denotes the function of sub-problem generation. Then, we solve the sub-problems<em>{</em><em>p</em> 1<em>, ..., p</em> <em>n</em><em>}</em> sequentially, resulting in a sequence of answers<em> {</em><em>a</em> 1<em>, ..., a</em> <em>n</em><em>}</em>. For answering the<em> i</em>-th sub-problem<em> p</em> <em>i</em>, we include both the original problem<em> p</em> 0 and all previously-seen problem-answer pairs in the context for prediction. The answer<em> a</em> <em>i</em> is given by</p></div>
<div class="para"><p><em>a</em> <em>i</em>=<em>S</em> <em>i</em>(<em>p</em> <em>i</em><em>,</em><em> {</em><em>p</em> 0<em>, p</em><em>&lt;i</em><em>, a</em><em>&lt;i</em><em>}</em>)(3.3)</p></div>
<div class="para"><p>where<em> p</em><em>&lt;i</em> =<em> {</em><em>p</em> 1<em>, ..., p</em> <em>i</em><em>−</em>1<em>}</em> and<em> a</em><em>&lt;i</em> =<em> {</em><em>a</em> 1<em>, ..., a</em> <em>i</em><em>−</em>1<em>}</em>.<em> S</em> <em>i</em>(<em>·</em>) denotes the function that solves the sub-problem<em> p</em> <em>i</em> given the context<em> {</em><em>p</em> 0<em>, p</em><em>&lt;i</em><em>, a</em><em>&lt;i</em><em>}</em>. The last step is to generate the answer to the original problem<em> p</em> 0, which can be expressed in a similar manner to Eq. (3.3).</p></div>
<div class="para"><p><em>a</em> 0=<em>S</em> 0(<em>p</em> 0<em>,</em><em> {</em><em>p</em><em>≤</em><em>n</em><em>, a</em><em>≤</em><em>n</em><em>}</em>)(3.4)</p></div>
<div class="para"><p>One way to refine this model is to modify the<em> G</em>(<em>·</em>) function so that the model can dynamically generate answers. Instead of generating all sub-problems at one time, we can generate each of them during problem-solving [Dua et al., 2022]. To do this, we can replace Eq. (3.2) with</p></div>
<div class="para"><p><em>p</em> <em>i</em>=<em>G</em> <em>i</em>(<em>p</em> 0<em>,</em><em> {</em><em>p</em><em>&lt;i</em><em>, a</em><em>&lt;i</em><em>}</em>)(3.5)</p></div>
<div class="para"><p>Hence we obtain a sub-problem generation model that operates in a step-by-step manner. At each step<em> i</em>, we first generate the sub-problem<em> p</em> <em>i</em> by prompting an LLM with the original problem<em> p</em> 0 and the problem-solving history<em> {</em><em>p</em><em>&lt;i</em><em>, a</em><em>&lt;i</em><em>}</em>. We then generate the answer<em> a</em> <em>i</em> for this sub-problem using the same or a different LLM, based on the same contextual information (see Eq. (3.3)). This method effectively expands the reasoning capacity of LLMs by allowing them to dynamically generate and solve sub-problems in intermediate reasoning steps. As a result, the reasoning paths are not fixed in advance, and the models can choose and adapt their reasoning strategies during problem-solving.</p></div>
<div class="para"><p>Another way to improve the above model is to focus on developing better sub-problem solvers.In our previous discussion, we restricted<em> S</em> <em>i</em>(<em>·</em>) to LLMs that are prompted to solve the sub-problem <em>p</em> <em>i</em>. In fact, we can expand this function to any system that is capable of addressing the subproblem. For example,<em> S</em> <em>i</em>(<em>·</em>) could make calls to IR systems, thereby allowing us to access a broader range of data for problem-solving. Another example is using<em> S</em> <em>i</em>(<em>·</em>) as a calculator to accurately compute results in mathematical problem-solving. If the sub-problem<em> p</em> <em>i</em> is complex and requires multiple intermediate problem-solving steps, it is also possible to further decompose <em>p</em> <em>i</em> into smaller sub-problems. For example,<em> S</em> <em>i</em>(<em>·</em>) can be defined as a recursive program that generates and solves sub-problems. This incorporates recursion into problem-solving and allows us to address problems by iteratively decomposing them. As a result, we can define a hierarchical structure for problem-solving [Khot et al., 2023].</p></div>
<div class="para"><p>If we generalize the above formulation a bit further, we can consider it as a reinforcement learning problem. A typical method is to model a problem-solving process as a decision making process. In each step of this process, an action is taken based on the current state. These actions</p></div>
<div class="para"><p>124 <em>Prompting</em></p></div>
<div class="para"><p>can include all functions for sub-problem generation and solving (i.e.,<em> G</em> <em>i</em>(<em>·</em>) and<em> S</em> <em>i</em>(<em>·</em>)). Thus,the action sequence corresponds to a problem-solving path. Since the discussion of reinforcement learning problems is beyond the scope of this chapter, we skip the precise description of this learning task. Nevertheless, developing an agent or controller to determine when and how to generate and solve a sub-problem is also a natural choice.</p></div>
<div class="para"><p>In NLP, problem decomposition is related to a long line of research on multi-hop question answering [Mavi et al., 2024]. This task requires the system to gather and combine information from multiple pieces of text to provide an accurate answer to a complex question. For example,to answer the question “What is the capital of the country where Albert Einstein was born?”, we need to know “Where Albert Einstein was born?” and “What’s the capital of Germany?”. Earlier work in this area and related ones has investigated the issue of problem decomposition, though the methods might not be based on LLMs. For example, a popular method is to develop an additional neural model to generate simpler questions that address different aspects of the original question[Andreas et al., 2016; Talmor and Berant, 2018; Min et al., 2019]. This question generator can create questions in a batch or sequential manner.</p></div>
<div class="para"><p>Broadly speaking, problem decomposition is also related to the compositionality issue in NLP[Drozdov et al., 2022; Press et al., 2023]. For example, in semantic parsing, we map natural language sentences into structured meaning representations by breaking them down into constituent parts and understanding the sentences based on the meanings of these parts and the rules used to combine them. In early studies of this field, highly compositional sentences were considered easier for testing systems, as it is relatively straightforward to decompose such sentences and compose the meanings of their parts. However, the task becomes much more difficult when more generalization is required for modeling compositionality in new data. In this case, we want systems to have improved abilities of<strong> compositional generalization</strong>. In more recent research on LLMs,this issue has been frequently discussed in compositional reasoning tasks, such as SCAN 7, as it is considered an important aspect of testing the language understanding and reasoning abilities of LLMs. This also presents new tasks for developing and examining problem decomposition methods.</p></div>
<div class="para"><p>In LLMs, one interesting application of problem decomposition is tool use. In some cases,it is necessary to integrate external tools into LLMs to access accurate data not available during training or fine-tuning. For example, LLMs can integrate with APIs to fetch real-time data such as weather updates, stock market prices, or news feeds, enabling them to provide up-to-date responses to user queries. When using tools, LLM predictions might include markers that indicate where and how to call external APIs. This requires decomposing the problem into sub-problems,with some handled by the LLMs and others by external tools. More detailed discussions on this topic will be presented in Section 3.2.5.</p></div>
<div class="para"><p><strong>3.2.3</strong> <strong>Self-refinement</strong></p></div>
<div class="para"><p>In many cases, predictions of LLMs can be inaccurate or incorrect. Given that current LLMs can perform tasks like refinement and correction, it makes sense to explore methods for these models to self-refine their outputs. Self-refinement is a common phenomenon in human psychological</p></div>
<div class="para"><p>7 The SCAN tasks (Simplified versions of the CommAI Navigation tasks) are designed to evaluate the ability of LLMs to perform compositional generalization [Lake and Baroni, 2018]. They involve translating natural language commands into a sequence of actions. For example, a command “jump opposite left and walk thrice” can be translated into the action sequence “LTURN LTURN JUMP WALK WALK WALK”.</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 125</p></div>
<div class="para"><p>activities and daily behavior. For example, when designing a product, a designer might first create a basic prototype, then refine the design after evaluation and testing to enhance user experience and functionality. The refinement can be iterated several times until the design is satisfactory. The idea of predict-then-refine can also be found in NLP. One early example is Brill’s tagger [Brill,1992], where an initial POS tagging result of a sentence can be iteratively refined using a rulebased system. In the era of deep learning, a good deal of work on sequence-to-sequence problems,such as grammar correction and text rewriting, can also be seen as examples on this theme.</p></div>
<div class="para"><p>We can prompt LLMs to do self-refinement. Consider a simple example where we first use an LLM to translate a text from Chinese to English and then use the same model to refine the translation. Here is how we can prompt the model to translate:</p></div>
<div class="para"><p>Translate the following text from Chinese to English:</p></div>
<div class="para"><p>Chinese:</p></div>
<div class="para"><p>一 系 列 考 古发 现 奠 定 红 山 文 化 在 中 华 文 明 起 源 研 究 中 的 重 要 地 位。最 新 公 布 的 研 究 成 果 认 为，大 约 从 距 今 5800 年 开 始，中 华 大 地 上 各 个 区 域 相 继 出 现 较 为 明显 的 社 会 分 化，进 入 文 明 起 源 的 加 速 阶 段。</p></div>
<div class="para"><p>English:</p></div>
<div class="para"><p>A series of discoveries have cemented the significant role of the Hongshan culture in studies on the origins of Chinese civilization.The latest research findings suggest that, starting from around 5800 years ago, many regions across China began to experience noticeable social differentiations, entering an accelerated phase in the origin of civilization.</p></div>
<div class="para"><p>Then, we can prompt it to refine the translation:</p></div>
<div class="para"><p>NiuTrans</p></div>
<div class="para"><p>126 <em>Prompting</em></p></div>
<div class="para"><p>You are provided with a Chinese text along with its English translation. Please refine the translation to improve its accuracy and fluency if necessary.</p></div>
<div class="para"><p>Chinese:</p></div>
<div class="para"><p>一 系 列 考 古发 现 奠 定 红 山 文 化 在 中 华 文 明 起 源 研 究 中 的 重 要 地 位。最 新 公 布 的 研 究 成 果 认 为，大 约 从 距 今 5800 年 开 始，中 华 大 地 上 各 个 区 域 相 继 出 现 较 为 明显 的 社 会 分 化，进 入 文 明 起 源 的 加 速 阶 段。</p></div>
<div class="para"><p>English:</p></div>
<div class="para"><p>A series of discoveries have cemented the significant role of the Hongshan culture in studies on the origins of Chinese civilization. The latest research findings suggests that, starting from around 5800 years ago, many regions across China began to experience noticeable social differentiations, entering an accelerated phase in the origin of civilization.</p></div>
<div class="para"><p>Refinement:</p></div>
<div class="para"><p>A series of archaeological discoveries have cemented the significant role of the Hongshan culture in studies on the origins of Chinese civilization.The latest research findings suggest that, starting from around 5800 years ago, various regions across China began to experience noticeable social differentiations, entering an accelerated phase in the origin of civilization.</p></div>
<div class="para"><p>Here modifications to the input translation are highlighted in green. This process can be iterative:at each step, we use the translation from the previous step as input, and prompt the LLM to produce a potentially better translation.</p></div>
<div class="para"><p>In the above example, the refinement of the initial model output was achieved by prompting the LLM with a simple instruction like “Please refine it!”. However, the refinement is based solely on the instruction-following ability of the LLM, and there is no guidance or supervision on how and where to improve the model output. A more effective approach would be to consider feedback on specific aspects that need refinement. For example, we can prompt the LLM with “Please correct all the grammatical errors in the translation”, so that the model can focus more on grammatical error correction during refinement.</p></div>
<div class="para"><p>A general framework of self-refinement with LLMs involves three steps [Madaan et al., 2024].</p></div>
<div class="para"><p>•<strong> Prediction</strong>. We use an LLM to produce the initial model output.</p></div>
<div class="para"><p>•<strong> Feedback Collection</strong>. We obtain feedback on the model output.</p></div>
<div class="para"><p>•<strong> Refinement</strong>. We use the LLM to refine the model output based on the feedback.</p></div>
<div class="para"><p>The last two steps can be repeated multiple times, which leads to an iterative self-refinement process. In this framework, receiving accurate and detailed feedback is critical as it guides the subsequent refinement step. In general, feedback can be obtained in various ways [Fernandes et al., 2023; Pan et al., 2024]. We can manually examine the model’s predictions to identify issues. Also, we can use a feedback model to automatically generate feedback. For instance, a reward model trained on some labeled data can help evaluate the model output and return a score indicating its quality.</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 127</p></div>
<div class="para"><p>Since the focus of this chapter is on LLM prompting, we consider using LLMs to provide feedback. To illustrate, let us consider an example of generating good responses to user questions:</p></div>
<div class="para"><p>• First, we prompt an LLM to answer the input question, as usual.</p></div>
<div class="para"><p>Generate a response to the question: “What are some major environmental concerns today?”</p></div>
<div class="para"><p>Response:</p></div>
<div class="para"><p>Major environmental concerns today include climate change, air and water pollution, deforestation, loss of biodiversity, and ocean degradation.</p></div>
<div class="para"><p>• Then, using the following prompt, we prompt the same LLM again to get feedback on the generated response.</p></div>
<div class="para"><p>You will be provided with a question and its corresponding response. Evaluate the accuracy and depth of this response, and identify any factual inaccuracies or areas where additional details could enhance understanding.</p></div>
<div class="para"><p>Question:</p></div>
<div class="para"><p>What are some major environmental concerns today?</p></div>
<div class="para"><p>Response:</p></div>
<div class="para"><p>Major environmental concerns today include climate change, air and water pollution, deforestation, loss of biodiversity, and ocean degradation.</p></div>
<div class="para"><p>Feedback:</p></div>
<div class="para"><p>The response provided is accurate and highlights key environmental concerns.However, it would benefit from more specific and detailed descriptions of these concerns. Here are suggestions for improvement:</p></div>
<ul><li>Include specific examples of environmental issues.</li><li>Add information about major pollutants.</li><li>Specify the regions most affected.</li></ul>
<div class="para"><p>• Given the feedback the LLM produces, we prompt it to refine the response.</p></div>
<div class="para"><p>128 <em>Prompting</em></p></div>
<div class="para"><p>You will be provided with a question, its corresponding response, and some feedback on this response.</p></div>
<div class="para"><p>Question:</p></div>
<div class="para"><p>What are some major environmental concerns today?</p></div>
<div class="para"><p>Response:</p></div>
<div class="para"><p>Major environmental concerns today include climate change, air and water pollution, deforestation, loss of biodiversity, and ocean degradation.</p></div>
<div class="para"><p>Feedback:</p></div>
<div class="para"><p>The response provided is accurate and highlights key environmental concerns. However, it would benefit from more specific and detailed descriptions of these concerns. Here are suggestions for improvement:</p></div>
<ul><li>Include specific examples of environmental issues.</li><li>Add information about major pollutants.</li><li>Specify the regions most affected.</li></ul>
<div class="para"><p>Now refine the response based on the provided feedback.</p></div>
<div class="para"><p>Refined Response:</p></div>
<div class="para"><p>Key environmental concerns include climate change, evidenced by rising global temperatures and severe weather patterns; significant air and water pollution in urban and industrial areas; rampant deforestation in regions like the Amazon, causing biodiversity loss; and ocean degradation,highlighted by coral reef bleaching and widespread overfishing.</p></div>
<div class="para"><p>Ideally, if a strong LLM is adopted, we would like to have it perform all three steps without extra training. On the other hand, if we have enough labeled data for the task of interest, we can enhance the performance of the LLM using supervised learning. For example, we can fine-tune the LLM to better adapt it to refinement tasks, or alternatively, use task-specific models, which may not necessarily be based on LLMs [Welleck et al., 2023; Schick et al., 2023]. In a broader sense, improving LLMs for self-refinement tasks can be seen as an alignment issue. For example,it has been found that some self-correction abilities can be activated through RLHF [Ganguli et al.,2023]. However, discussing these issues is beyond the scope of this chapter. Further discussion can be found in Chapter 4.</p></div>
<div class="para"><p>In LLMs, self-refinement is related to several concepts that reveal the psychological aspects of these models, such as the ability to self-reflect. A view is that if LLMs are capable of selfreflection, their predictions can become more accurate and even possess self-correcting capabilities. This self-reflection can be activated in various ways, for example, by prompting these LLMs to engage in more in-depth and careful thinking, or by providing examples from which the models can learn and reflect. To illustrate, we consider here the<strong> deliberate-then-generate</strong> (<strong>DTG</strong>) method presented in Li et al. [2023a]’s work, where LLMs are prompted to deliberate. In DTG, we are given an initial model output which may contain errors. LLMs are then prompted to identify the error types of this model output and provide an improved output. Below is a template of DTG prompting for Chinese-to-English translation tasks.</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 129</p></div>
<div class="para"><p>Given the Chinese sentence:<em> {∗</em>source<em>∗}</em></p></div>
<div class="para"><p>The English translation is:<em> {∗</em>target<em>∗}</em></p></div>
<div class="para"><p>Please first detect the type of error, and then refine the translation.</p></div>
<div class="para"><p>Error Type:</p></div>
<div class="para"><p>We aim to first predict the error type (red), and then produce a refined translation (blue). This process of deliberation is guided by the instruction “Please first detect the type of error, and then refine the translation”. It encourages LLMs to initially engage in thoughtful analysis and then give better results. Since error type prediction and refinement are performed in a single run of LLMs,this method incorporates both steps of feedback and refinement into one process.</p></div>
<div class="para"><p>In the above prompts, we assume that the LLM we use is able to review the input translation and correctly identify its error types. However, this raises new difficulties as the model may not be good at finding errors in translations. This will in turn result in extra fine-tuning or prompting engineering efforts. So a simpler method is to reduce the burden of error identification and use LLMs for deliberation only. To do this, we can replace the input translation with a random translation and assign a default error type. An example of such a prompt is shown below.</p></div>
<div class="para"><p>Given the Chinese sentence:</p></div>
<div class="para"><p>一 系 列 考 古发 现 奠 定 红 山 文 化 在 中 华 文 明 起 源 研 究 中 的 重 要 地 位。</p></div>
<div class="para"><p>The English translation is:</p></div>
<div class="para"><p>A variety of innovative techniques have redefined the importance of modern art in contemporary cultural studies.</p></div>
<div class="para"><p>Please first detect the type of error, and then refine the translation.</p></div>
<div class="para"><p>Error Type: Incorrect Translation</p></div>
<div class="para"><p>In this example, the input translation is not generated by LLMs but is instead randomly sampled from the dataset. So it is simply an incorrect translation for the source sentence, and we can set the error type accordingly. The LLMs then generate a new translation by taking both the source sentence and the incorrect translation as input. The design of this prompt can also be considered as activating the learning capabilities of LLMs through “negative evidence” [Marcus, 1993], thereby enabling them to reflect and produce better outcomes through contrastive analysis. Nevertheless,this method does not rely on any feedback and can enhance the performance of a single LLM prediction via simple prompting.</p></div>
<div class="para"><p>Note that while DTG is non-iterative, iterative learning and refinement are commonly used in NLP. An advantage of these iterative approaches is that they mimic human learning and problemsolving, where continuous feedback and adjustments lead to progressively improved outcomes.Iterative methods can be applied to a range of LLM prompting problems. For example, in problem decomposition, one can incorporate new sub-problems and their solutions into the context at each step, and thus LLMs can progressively approach the solution of the original problem. On the other hand, iterative methods raise several issues that are absent in non-iterative methods, for example,</p></div>
<div class="para"><p>130 <em>Prompting</em></p></div>
<div class="para"><p>errors in earlier steps may negatively impact subsequent problem-solving, and determining when to stop iterating often requires additional engineering effort.</p></div>
<div class="para"><p><strong>3.2.4</strong> <strong>Ensembling</strong></p></div>
<div class="para"><p>Model ensembling for text generation has been extensively discussed in the NLP literature. The idea is to combine the predictions of two or more models to generate a better prediction. This technique can be directly applicable to LLMs. For example, we can collect a set of LLMs and run each of them on the same input. The final output is a combined prediction from these models.</p></div>
<div class="para"><p>For LLM prompting, it is also possible to improve performance by combining predictions based on different prompts. Suppose we have an LLM and a collection of prompts that address the same task. We can run this LLM with each of the prompts and then combine the predictions.For example, below are three different prompt templates for text simplification.</p></div>
<div class="para"><p>Make this text simpler.</p></div>
<div class="para"><p><em>{∗</em>text<em>∗}</em></p></div>
<div class="para"><p>Condense and simplify this text.</p></div>
<div class="para"><p><em>{∗</em>text<em>∗}</em></p></div>
<div class="para"><p>Rewrite for easy reading.</p></div>
<div class="para"><p><em>{∗</em>text<em>∗}</em></p></div>
<div class="para"><p>Each of these prompts will lead to a different prediction, and we can consider all three predictions to generate the final one.</p></div>
<div class="para"><p>Formally, let<em> {</em><strong>x</strong> 1<em>, ...,</em><strong> x</strong> <em>K</em><em>}</em> be<em> K</em> prompts for performing the same task. Given an LLM Pr(<em>·|·</em>),we can find the best prediction for each<strong> x</strong> <em>i</em> using ˆ <strong>y</strong> <em>i</em> = arg max <strong>y</strong> <em>i</em> Pr(<strong>y</strong> <em>i</em><em>|</em><strong>x</strong> <em>i</em>). These predictions can be combined to form a “new” prediction:</p></div>
<div class="para"><p>ˆ <strong>y</strong>=Combine(ˆ <strong>y</strong> 1<em>, ...,</em> ˆ <strong>y</strong> <em>K</em>)(3.6)</p></div>
<div class="para"><p>Here Combine(<em>·</em>) is the combination model, which can be designed in several different ways. For example, we can select the best prediction by voting or by identifying the one that overlaps the most with others. Another method for model combination is to perform model averaging during token prediction. Let ˆ <em>y</em> <em>j</em> be the predicted token at the<em> j</em>-th step for model combination. The probability of predicting ˆ <em>y</em> <em>j</em> is given by</p></div>
<div class="para"><p><em>K</em> X</p></div>
<div class="para"><p><em>k</em>=1 log Pr(<em>y</em> <em>j</em><em>|</em><strong>x</strong> <em>k</em><em>,</em> ˆ <em>y</em> 1<em>, ...,</em> ˆ <em>y</em> <em>j</em><em>−</em>1)(3.7)</p></div>
<div class="para"><p>ˆ <em>y</em> <em>j</em>=arg max <em>y</em> <em>j</em></p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 131</p></div>
<div class="para"><p>In ensembling for LLM prompting, it is generally advantageous to use diverse prompts so that the combination can capture a broader range of potential responses. This practice is common in ensemble learning, as diversity helps average out biases and errors that may be specific to any single model or configuration. From the Bayesian viewpoint, we can treat the prompt<strong> x</strong> as a latent variable, given the problem of interest,<em> p</em>. This allows the predictive distribution of<strong> y</strong> given<em> p</em> to be written as the distribution Pr(<strong>y</strong><em>|</em><strong>x</strong>) marginalized over all possible prompts</p></div>
<div class="para"><p>Pr(<strong>y</strong><em>|</em><em>p</em>)=Z Pr(<strong>y</strong><em>|</em><strong>x</strong>) Pr(<strong>x</strong><em>|</em><em>p</em>)<em>d</em> <strong>x</strong>(3.8)</p></div>
<div class="para"><p>The integral computes the total probability of<strong> y</strong> by considering all possible values of<strong> x</strong>, weighted by their likelihoods given<em> p</em>. Here Pr(<strong>y</strong><em>|</em><strong>x</strong>) is given by the LLM, and Pr(<strong>x</strong><em>|</em><em>p</em>) is the prior distribution of prompts for the problem. This is a good model because the integral effectively accounts for the uncertainty in the choice of<strong> x</strong>, ensuring that the final predictive distribution Pr(<strong>y</strong><em>|</em><em>p</em>) is robust and encompasses all potential variations and biases in the prompts. However, computing this integral directly can be computationally infeasible due to the potentially infinite space of<strong> x</strong>.One approach to addressing this issue is to employ methods like Monte Carlo sampling, which approximate the integral using a manageable, finite number of prompts.</p></div>
<div class="para"><p>While the Bayesian treatment is mathematically well-defined, it is common practice in NLP to assume a non-informative or uniform prior and focus instead on constructing a set of diverse prompts. Consequently, the output can be computed using a straightforward combination model,as described in Eq. (3.6). The issue of creating high-quality, diverse prompts has been studied in CoT and other in-context learning areas. Most of the research focuses on incorporating a variety of demonstration examples across different prompts. Here, we list some of these methods.</p></div>
<div class="para"><p>• Given a problem, we manually create a number of demonstrations and use different ones for different prompts.</p></div>
<div class="para"><p>• Given a problem, we use LLMs to automatically generate demonstrations and prompts.</p></div>
<div class="para"><p>• Given a prompt, we create different prompts by changing the order of demonstrations in the prompt.</p></div>
<div class="para"><p>• Given a prompt, we use LLMs to generate a number of similar prompts.</p></div>
<div class="para"><p>• Given a prompt, we transform it into other forms, e.g., translating it into other languages.</p></div>
<div class="para"><p>Of course, in practice, we can combine these methods to achieve greater diversity. An underlying assumption here is that diverse prompts can lead to diverse model outputs. This is particularly the case when the problem we deal with is relatively new and difficult. For stronger and more robust LLMs, the variance in output for similar prompts might not be large. In this case, the benefit of involving multiple prompts can be modest.</p></div>
<div class="para"><p>In addition to providing diverse prompts for LLMs, another approach is to make use of the inherent variance in the outputs of LLMs. One simple way to generate multiple outputs is to sample outputs from the hypothesis space. This is straightforward for LLMs because their search algorithms are naturally suited to producing multiple results. For example, if beam search is used, we can easily collect all the complete hypotheses in the beam to generate multiple output</p></div>
<div class="para"><p>132 <em>Prompting</em></p></div>
<div class="para"><p>candidates. Also, we can modify the search algorithms to implement more powerful sampling methods and explore a wider range of high-quality hypotheses.</p></div>
<div class="para"><p>The next question is how to combine diverse model predictions. This leads us back to the model combination issue as described in Eq. (3.6). Here we consider the<strong> self-consistency</strong> method,which outputs not the prediction with the highest probability, but rather the one that best aligns with other predictions [Wang et al., 2022a; 2023b]. First, an LLM is prompted with CoT as usual and generates multiple reasoning paths by sampling. Then, the frequency of each answer is counted across these reasoning paths. The final output is the answer that has the highest count.Below shows three different predictions by prompting an LLM with CoT.</p></div>
<div class="para"><p>Prediction 1 (Correct):</p></div>
<div class="para"><p>Q: Three friends play a game where they flip a fair coin. Each friend flips the coin once. What is the probability that exactly one of them flips heads?</p></div>
<div class="para"><p>A: Let’s think step by step.</p></div>
<div class="para"><p>Each coin flip is an independent event with a 50% chance of landing heads(H) and 50% tails (T). Consider the possible outcomes for three flips: HHH,HHT, HTH, HTT, THH, THT, TTH, TTT. Out of these, only HHH, HHT, HTH,and THH are relevant. We are only interested in scenarios with exactly one head: HTT, THT, and TTH. There are 3 such outcomes out of 8 total, giving a probability of 3<em>/</em>8.</p></div>
<div class="para"><p>Prediction 2 (Correct):</p></div>
<div class="para"><p>Q: Three friends play a game where they flip a fair coin. Each friend flips the coin once. What is the probability that exactly one of them flips heads?</p></div>
<div class="para"><p>A: Let’s think step by step.</p></div>
<div class="para"><p>Using a binomial probability formula, where<em> p</em> (success or heads) is 0<em>.</em>5, and each of the 3 trials (flips) is independent, we look for exactly one success(head).The formula for exactly<em> k</em> successes in<em> n</em> trials is given by the binomial coefficient formula</p></div>
<div class="para"><p> <em>n</em> <em>k</em><em>p</em> <em>k</em>(1<em> −</em><em>p</em>)<em>n</em><em>−</em><em>k</em>. Plugging in the values, we get 3 1(0<em>.</em>5)1(0<em>.</em>5)3<em>−</em>1 = 3<em> ·</em> 0<em>.</em>5<em> ·</em> 0<em>.</em>25 = 0<em>.</em>375.</p></div>
<div class="para"><p>Prediction 3 (Wrong):</p></div>
<h3>3.2 Advanced Prompting Methods 133</h3>
<h3>Q: Three friends play a game where they flip a fair coin. Each friend flips the coin once. What is the probability that exactly one of them flips heads?</h3>
<h3>A: Let’s think step by step.</h3>
<h3>Since each coin flip has a 50% chance of landing heads and a 50% chance of landing tails, and we are looking for the case where only one of the three flips results in heads, we might initially think this is simply a matter of picking which flip is the heads. There are three flips, so one might incorrectly assume that the chance of picking one specific outcome like this would be 1 out of 3. Thus, they might conclude that the probability of exactly one head is 1/3 = 33.3%.</h3>
<h3>Predictions 1 and 2 correctly identify the three cases where exactly one head is flipped, both obtaining a probability of 37.5%. The reasoning in Prediction 3 fails to account for the total num- ber of outcomes possible with three coin flips, thus giving a wrong answer of 33.3%. Therefore, we select 37.5% as the final answer because it is the consensus.</h3>
<h3>Self-consistency provides a criterion for determining the best prediction in a pool of can- didates. Since the prompt and the model are fixed in this method, it is not strictly a prompt ensembling method. Instead, it can be seen as an instance of output ensembling methods, also known as hypothesis selection methods, which have long been explored in NLP, particularly for text generation problems [Xiao et al., 2013]. In these methods, multiple outputs are generated by varying model architectures or parameters. Each output is then assigned a score by some criterion, and the outputs are re-ranked based on these scores. There are various ways to define the scoring function, such as measuring the agreement between an output and others, and using a stronger model to rescore each output 8. Figure 3.2 shows a comparison of different ensembling methods for LLMs.</h3>
<h3>Now, let us briefly review the methods we have discussed so far in this section, such as problem decomposition and self-refinement. It is apparent that these methods enhance decision-making by introducing more “choices” into the reasoning process. To some extent, they all involve evaluating and providing feedback on the results of LLMs. For example, in self-refinement, we need to offer suggestions for improving the prediction of LLMs, and in output ensembling, we select the optimal output from a pool of candidates. In this sense, these methods fall under the broader category of predict-then-verify approaches, where predictions are initially made, then verified and refined. The fundamental problem here involves verifying and evaluating the reasoning results or intermediate steps. This issue is somewhat related to the problem of training reward models in RLHF, although RLHF addresses a different aspect. In fact, the development of verifiers has been explored and implemented in reasoning with LLMs. Most work, rather than developing heuristic-based inference-time algorithms, focuses on learning verifiers in a supervised manner. A straightforward method is to train verifiers as binary classifiers, such as classifying an answer</h3>
<div class="para"><p>8 An interpretation of self-consistency is to view it as a minimum Bayes risk search process. It searches for the best output by minimizing the Bayes risk. More specifically, a risk function<em> R</em>(<strong>y</strong><em>,</em><strong> y</strong> <em>r</em>) is defined on each pair of outputs(denoted by (<strong>y</strong><em>,</em><strong> y</strong> <em>r</em>)), representing the cost of replacing<strong> y</strong> with<strong> y</strong> <em>r</em>. Given a set of outputs<em> Ω</em>, the risk of an output <strong>y</strong><em> ∈</em><em>Ω</em> is given by</p></div>
<div class="para"><p>Risk(<strong>y</strong>)=E <strong>y</strong> <em>r</em><em>∼</em>Pr(<strong>y</strong> <em>r</em><em>|</em><strong>x</strong>)<em>R</em>(<strong>y</strong><em>,</em><strong> y</strong> <em>r</em>)</p></div>
<div class="para"><p>=X</p></div>
<div class="para"><p><strong>y</strong> <em>r</em><em>∈</em><em>Ω</em> <em>R</em>(<strong>y</strong><em>,</em><strong> y</strong> <em>r</em>)<em> ·</em> Pr(<strong>y</strong> <em>r</em><em>|</em><strong>x</strong>)(3.9)</p></div>
<div class="para"><p>134 <em>Prompting</em></p></div>
<div class="para"><p>Combine/Select</p></div>
<div class="para"><p>Prediction3</p></div>
<div class="para"><p>LLM2</p></div>
<div class="para"><p>Prompt Prediction2</p></div>
<div class="para"><p>LLM2</p></div>
<div class="para"><p>Final Prediction</p></div>
<div class="para"><p>Prediction1</p></div>
<div class="para"><p>LLM1</p></div>
<div class="para"><p>(a) Model Ensembling</p></div>
<div class="para"><p>Combine/Select</p></div>
<div class="para"><p>Prompt3</p></div>
<div class="para"><p>Prediction3</p></div>
<div class="para"><p>LLM Prompt2</p></div>
<div class="para"><p>Prediction2</p></div>
<div class="para"><p>Final Prediction</p></div>
<div class="para"><p>Prompt1</p></div>
<div class="para"><p>Prediction1</p></div>
<div class="para"><p>(b) Prompt Ensembling</p></div>
<div class="para"><p>Combine/Select</p></div>
<div class="para"><p>Sample</p></div>
<div class="para"><p>Prediction3</p></div>
<div class="para"><p>LLM Prompt Prediction2</p></div>
<div class="para"><p>Final Prediction</p></div>
<div class="para"><p>Prediction1</p></div>
<div class="para"><p>(c) Output Ensembling</p></div>
<div class="para"><p><strong>Fig. 3.2:</strong> Ensembling methods for LLMs. In standard model ensembling (a), multiple LLMs varying in architectures or parameters are used. Each LLM receives the same prompt and produces a prediction. These predictions are combined to generate the final prediction. In prompt ensembling (b), we have one LLM and multiple prompts. The LLM produces a prediction for each prompt, and these predictions are combined as usual. In output ensembling (c), the LLM samples multiple predictions over the prediction space given a prompt. It can be seen as a method to boost the performance of the LLM itself. Note that these ensembling methods can be combined to increase the diversity of predictions. For example, we can use both prompt ensembling and output ensembling to obtain more diverse predictions.</p></div>
<div class="para"><p>as correct or incorrect, although these verifiers are typically used as scoring models. Given a reasoning path for a problem, the verifiers can be used to score either the entire path (called outcome-based approaches) [Cobbe et al., 2021], or each individual reasoning step (called processbased approaches) [Uesato et al., 2022; Lightman et al., 2024].</p></div>
<h3>3.2.5 RAG and Tool Use</h3>
<div class="para"><p>RAG is generally employed when standard LLMs, which rely solely on pre-trained knowledge,lack accuracy and depth in the generated text. By drawing from external databases and documents,</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 135</p></div>
<div class="para"><p>RAG can significantly improve the quality of responses, ensuring they are both contextually relevant and factually correct. Such an approach is particularly useful in scenarios that require high factual accuracy and up-to-date information, such as complex question answering.</p></div>
<div class="para"><p>The concept of RAG has been mentioned several times in the previous sections and chapters.For completeness, we outline the key steps involved in RAG here.</p></div>
<div class="para"><p>• We prepare a collection of texts which are treated as an additional source of knowledge we can access.</p></div>
<div class="para"><p>• We retrieve relevant texts for a given query.</p></div>
<div class="para"><p>• We input both the retrieved texts and the query into an LLM, which is then prompted to produce the final prediction.</p></div>
<div class="para"><p>Steps 1 and 2 can be implemented by using an external information retrieval system. For example, we can store the collection of texts in a vector database and then retrieve the most similar texts through vector-based search techniques. Since information retrieval is not the focus of this chapter, we will assume that such systems are available off-the-shelf and use them directly.</p></div>
<div class="para"><p>Here we present how to prompt LLMs to make use of retrieved texts. To illustrate, consider an example of using LLMs to answer the following question.</p></div>
<div class="para"><p><em>Where will the 2028 Olympics be held?</em></p></div>
<div class="para"><p>We can simply input this question into an online search engine. It will then return the relevant pieces of text found on the internet, for example,</p></div>
<div class="para"><p>(Wikipedia)</p></div>
<div class="para"><p><em>The 2028 Summer Olympics, officially the Games of the XXXIV Olympiad and com-</em><em>monly known as Los Angeles 2028 or LA28, is an upcoming international multi-sport</em> <em>event scheduled to take place from July 14-30, 2028, in the United States. ...</em></p></div>
<div class="para"><p>(The Sporting News)</p></div>
<div class="para"><p><em>In 2028, Los Angeles will become the third city, following London and Paris respectively,</em><em>to host three Olympics after hosting the Summer Games in 1932 and 1984. It will also</em> <em>be the first time the United States has hosted an Olympic Games since the 2002 Winter</em> <em>Games in Salt Lake City. ...</em></p></div>
<div class="para"><p>...</p></div>
<div class="para"><p>We can use these retrieved texts as additional context, and prompt an LLM to generate a response based on these texts. Below is an example RAG prompt.</p></div>
<div class="para"><p>136 <em>Prompting</em></p></div>
<div class="para"><p>Your task is to answer the following question. To help you with this, relevant texts are provided. Please base your answer on these texts.</p></div>
<div class="para"><p>Question:</p></div>
<div class="para"><p>Where will the 2028 Olympics be held?</p></div>
<div class="para"><p>Relevant Text 1:</p></div>
<div class="para"><p>The 2028 Summer Olympics, officially the Games of the XXXIV Olympiad and commonly known as Los Angeles 2028 or LA28 ...</p></div>
<div class="para"><p>Relevant Text 2:In 2028, Los Angeles will become the third city, following London and Paris respectively, to host three Olympics after ...</p></div>
<div class="para"><p>...</p></div>
<div class="para"><p>The 2028 Olympics will be held in Los Angeles.</p></div>
<div class="para"><p>This prompt assumes that the provided texts are relevant to the question and expects the LLM to generate a faithful response using these texts. However, the information retrieval system may sometimes provide irrelevant or incorrect texts, which may lead the LLM to produce an incorrect answer. One straightforward way to address this issue is to improve the accuracy of the information retrieval system. Nevertheless, as with most AI systems, errors may still occur. Therefore, it is also necessary to enhance the robustness of the LLM, so that it can make reasonable predictions even when the input is inaccurate. Below is a new prompt that enables the LLM to be more faithful to the facts, and allows it to choose not to answer questions when the information provided is inaccurate.</p></div>
<div class="para"><p>Your task is to answer the following question. To help you with this, relevant texts are provided. Please base your answer on these texts.</p></div>
<div class="para"><p>Please note that your answers need to be as accurate as possible and faithful to the facts. If the information provided is insufficient for an accurate response, you may simply output &quot;No answer!&quot;.</p></div>
<div class="para"><p>Question:</p></div>
<div class="para"><p>Where will the 2028 Olympics be held?</p></div>
<div class="para"><p>Relevant Text 1:</p></div>
<div class="para"><p>The 2024 Summer Olympics, officially the Games of the XXXIII Olympiad and branded as Paris 2024, were an international multi-sport event ...</p></div>
<div class="para"><p>...</p></div>
<div class="para"><p>No answer!</p></div>
<div class="para"><p>In this example, the LLM refuses to answer because the provided information is insufficient and irrelevant to the question.</p></div>
<div class="para"><p>Both RAG and fine-tuning are common methods for adapting LLMs using task-specific data.Standard RAG is training-free and can be directly applied to LLMs. To further improve RAG, it</p></div>
<div class="para"><p><em>3.2 Advanced Prompting Methods</em> 137</p></div>
<div class="para"><p>is also possible to fine-tune LLMs, though this will require some training effort. For example, we can fine-tune LLMs using human-labelled data to supervise them in learning to refuse to answer.Note that, while the examples shown above seem simple, RAG is not trivial. From the prompt engineering perspective, different use cases may require different prompts, though our somewhat“greedy” goal is to develop a universal prompting strategy that can adapt to different tasks. In many cases, we need to control how much we depend on the retrieved context to make predictions.Sometimes, LLMs must derive responses strictly from the provided texts, while at other times,they may need to generate responses using their pre-trained knowledge if the provided texts are insufficient. There are many aspects of RAG, such as improvements to the retrieval systems, that cannot be covered in this chapter. Interested readers can refer to surveys of RAG techniques for more information [Li et al., 2022; Gao et al., 2023c].</p></div>
<div class="para"><p>One reason we discuss RAG here is that it can be broadly regarded as an instance of the general problem decomposition framework (see Section 3.2.2). RAG divides problem-solving into two steps. In the first step, we collect relevant and supporting information for a given query from various knowledge sources. In the second step, we use LLMs to generate responses based on the collected information. If we extend the concept of problem decomposition further, we will find that many tasks requiring the use of external systems or tools can be treated as similar problems. One such example is tool use in LLMs. In many applications, LLMs need to employ external databases, APIs, and even simulation tools to generate accurate responses. For example,LLMs can access real-time data from financial markets to provide up-to-date investment advice or integrate with healthcare databases to offer personalized medical insights. This integration extends the capabilities of LLMs by allowing them to interact with, and in some contexts, influence or control external systems. Consequently, LLMs function more as<strong> autonomous agents</strong> rather than mere text generators [Franklin and Graesser, 1996].</p></div>
<div class="para"><p>The issue of tool use is broad and vast. Here we narrow our discussion to tasks that can be facilitated by calling external APIs to solve some of the sub-problems [Parisi et al., 2022; Gao et al., 2023b]. Consider again the example of asking an LLM to answer “Where will the 2028 Olympics be held?”. Suppose the LLM can access a web search tool. We can then prompt the LLM to answer the question with web search, like this</p></div>
<div class="para"><p>Your task is to answer the following question. You may use external tools, such as web search, to assist you.</p></div>
<div class="para"><p>Question:</p></div>
<div class="para"><p>Where will the 2028 Olympics be held?</p></div>
<div class="para"><p>The information regarding this question is given as follows:</p></div>
<div class="para"><p>{tool: web-search, query: &quot;2028 Olympics&quot;}</p></div>
<div class="para"><p>So the answer is: Los Angeles</p></div>
<div class="para"><p>Here {tool:web-search, query:&quot;2028 Olympics&quot;} indicates a request to the web search system using the query “2028 Olympics”. When the LLM sees this string, it executes a web search and uses the result to replace the string. Then, in subsequent steps of prediction, the LLM uses this web search result as context to produce the correct answer.</p></div>
<div class="para"><p>Consider another example where we ask the LLM to solve a mathematical problem.</p></div>
<div class="para"><p>138 <em>Prompting</em></p></div>
<div class="para"><p>Problem:</p></div>
<div class="para"><p>A swimming pool needs to be filled with water. The pool measures 10 meters in length, 4 meters in width, and 2 meters in depth. Calculate the volume of the pool in cubic meters and then determine how many liters of water are needed to fill it (considering 1 cubic meter equals 1000 liters).</p></div>
<div class="para"><p>Solution:To solve this problem, the LLM needs to first calculate the volume of the pool by using the formula for the volume of a rectangular prism: Length<em>×</em>Width<em> ×</em> Depth<em>.</em> Therefore, The volume is 10 m<em> ×</em> 4 m<em> ×</em> 2 m = {tool:calculator, expression: 10 * 4 * 2} m 3. Next, to find out how many liters of water are needed, the LLM multiplies the volume in cubic meters by 1000 (since 1 cubic meter equals 1000 liters). Thus, 80<em> ×</em>1000= {tool: calculator, expression: 80*1000} liters<em>.</em></p></div>
<div class="para"><p>Here the string {tool:calculator, expression:10 * 4 * 2} triggers the invocation of a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,80) will replace {tool:calculator, expression:10 * 4 * 2} and can be referred to in the following token predictions. For example, in the last step of problem-solving, 80 is used instead of {tool:calculator, expression:10 * 4 * 2}.</p></div>
<div class="para"><p>A key difference between the tool use examples here and the previously discussed RAG examples is that in tool use, external functions can be called during inference. In contrast, in RAG,the retrieved texts are provided before the prediction process begins. However, from the language modeling perspective, they are actually doing the same thing: before generating the final result,we use external tools, either manually or automatically, to obtain sufficient and relevant context. A high-level interpretation of these approaches is that they both rely on an “agent” that can determine where and how to call external functions to generate the context necessary for prediction.</p></div>
<div class="para"><p>An issue with tool use is that the original LLMs are not trained to generate the necessary markers for tool use. Therefore, we need to fine-tune the LLMs to adapt them for these tasks[Schick et al., 2024]. As this chapter focuses on prompting, we will not present the details of this fine-tuning process. To put it simply, we first need to annotate data. For each fine-tuning example,we replace parts of the output that require the use of external tools with predefined commands or markers. Then, we use this labeled data to fine-tune the parameters of the LLM as usual. As a result, the LLM can gain the ability to generate commands for calling external tools. During inference, we can execute these tool use commands in the model outputs to get assistance from external tools.</p></div>
<h3>3.3 Learning to Prompt</h3>
<div class="para"><p>So far in this chapter, we have considered several basic prompting strategies and various refinements to them. However, all the prompts we have discussed were designed manually. This leads to a number of problems: First, designing high-quality prompts is inherently difficult and requires substantial manual effort. For example, extensive experimentation with different prompts is often needed to identify the most effective ones. Since different LLMs may respond better to certain</p></div>
<div class="para"><p><em>3.3 Learning to Prompt</em> 139</p></div>
<div class="para"><p>types of prompts, developing universally effective prompts can be even more resource-intensive.Second, manual prompt design relies heavily on human expertise, which can limit the diversity of approaches and overlook potentially effective prompts that are not immediately obvious to humans. Third, prompts created by humans can be complex and redundant, leading to longer inputs for LLMs and higher computational costs.</p></div>
<div class="para"><p>In this section, we discuss techniques for automated prompting. These methods aim to automatically create, optimize, and represent prompts so that the downstream tasks can be addressed more effectively and efficiently. In particular, we consider three issues here.</p></div>
<div class="para"><p>• How can we automate the process of designing and optimizing prompts for LLMs?</p></div>
<div class="para"><p>• Are there other forms of representing prompts beyond strings, and how can we learn such representations?</p></div>
<div class="para"><p>• How can we make prompts more concise and compact, thereby reducing their complexity and length?</p></div>
<div class="para"><p>Note that there are many settings in which we can investigate these issues. For example, we might specify that prompts are developed specifically for a particular LLM, or that the development is independent of the LLM used. These settings can lead to different methods and application scenarios, but these methods may overlap in some ways. In the following discussion, we will cover several different scenarios and discuss the connections between various methods.</p></div>
<div class="para"><p><strong>3.3.1</strong> <strong>Prompt Optimization</strong></p></div>
<div class="para"><p>Given that prompt design is difficult and labor-intensive, it is desirable to use machine learning models to discover the optimal prompt for a specific task (call it<strong> automatic prompt design</strong> or <strong>prompt optimization</strong>). This approach can broadly be regarded as an instance of<strong> automated ma-</strong><strong>chine learning</strong> (<strong>AutoML</strong>), which aims to reduce or eliminate the need for expert-driven manual design of machine learning models. Although our focus here is on the design of prompts, prompts themselves are discrete structures. Therefore, designing prompts is very similar to designing machine learning models, such as discrete model architectures. Perhaps one of the most related fields is<strong> neural architecture search</strong> (<strong>NAS</strong>), where the most optimal neural networks are identified by exploring a space of possible neural networks [Zoph and Le, 2016; Elsken et al., 2019]. If we consider prompt optimization as a search process, then we can describe a general prompt optimization framework involving the following components:</p></div>
<div class="para"><p>•<strong> Prompt Search Space</strong>. This defines all possible prompts that the algorithms can explore.For example, one can edit some seed prompts to generate a set of diverse candidate prompts.</p></div>
<div class="para"><p>•<strong> Performance Estimation</strong>. Once a prompt is chosen, it needs to be evaluated. For example,a straightforward way is to input it to an LLM and measure its performance on a validation set.</p></div>
<div class="para"><p>•<strong> Search Strategy</strong>. The search process is generally the same as that used in many AI systems. At each step, the system explores a set of promising prompts in the search space and</p></div>
<div class="para"><p>140 <em>Prompting</em></p></div>
<div class="para"><p>evaluates them. This process continues as more prompts are explored. The outcome of the search is the best-performing prompt observed until the search stops.</p></div>
<div class="para"><p>This is a very general framework, and different prompt optimization systems can vary in their design of each component. A widely-used approach is to use LLMs as the basis to develop these components. Initially, a few prompts are provided. Then, the following process is iterated until a stopping criterion is met: 1) the prompts are evaluated on a validation set; 2) a candidate pool is maintained by keeping only the most promising prompts; and 3) new prompts are created by employing LLMs to infer similar prompts from this candidate pool. One benefit of this approach is that it allows us to use off-the-shelf LLMs to perform the tasks mentioned above without the need for substantial system development. To achieve this, we can prompt or fine-tune LLMs to adapt them to these tasks. Here we consider Zhou et al. [2023c]’s method for illustrating LLM-based prompt optimization. It involves the following steps.</p></div>
<div class="para"><p>•<strong> Initialization</strong>. Let<em> C</em> represent the pool of the candidate prompts we intend to explore. The first step is to add initial prompts into<em> C</em>. We can do this in several ways. A simple method is to create such prompts by hand for a given task. However, in many cases where humans have limited knowledge about how to write effective prompts for the task, developing prompts becomes challenging. In these cases, it is desirable to use LLMs to generate prompts.For example, we can directly instruct LLMs to produce prompts, providing them with a description of the task.</p></div>
<div class="para"><p>You are given a task to complete using LLMs. Please write a prompt to guide the LLMs.</p></div>
<div class="para"><p><em>{∗</em>task-description<em>∗}</em></p></div>
<div class="para"><p>This method is straightforward, but it still requires a human-provided description of the task.An alternative method is to use LLMs to generate prompts given examples of the input and output of the task. Here is a prompt template.</p></div>
<div class="para"><p>You are provided with several input-output pairs for a task. Please write an instruction for performing this task.</p></div>
<div class="para"><p>Input:<em> {∗</em>input1<em>∗}</em> Output:<em> {∗</em>output1<em>∗}</em></p></div>
<div class="para"><p>Input:<em> {∗</em>input2<em>∗}</em> Output:<em> {∗</em>output2<em>∗}</em>...</p></div>
<div class="para"><p>As such, LLMs can infer the corresponding instruction for the task from the provided inputs and outputs.</p></div>
<div class="para"><p>•<strong> Evaluation</strong>. Once we obtain the candidate pool<em> C</em>, we need to evaluate the prompts in<em> C</em>.One method is to feed each prompt into an LLM and assess the results on the downstream</p></div>
<div class="para"><p><em>3.3 Learning to Prompt</em> 141</p></div>
<div class="para"><p>task. For example, we can evaluate the output of the LLM given an input using a pre-defined metric, or alternatively, use the log-likelihood of the output as a measure of the quality of the prompt.</p></div>
<div class="para"><p>•<strong> Pruning</strong>. If<em> C</em> contains a large number of prompts, it is reasonable to prune the unpromising prompts within it, thus reducing the computational burden in subsequent steps. This is a standard pruning problem. Given the evaluation score for each prompt, a simple method is to keep only a certain percentage of the prompts and discard the rest.</p></div>
<div class="para"><p>•<strong> Expansion</strong>. Expansion is a key operation in search algorithms used to explore different states in the search space. The expansion operation here can be defined as a function</p></div>
<div class="para"><p><em>C</em><em>′</em>=Expand(<em>C, f</em>)(3.10)</p></div>
<div class="para"><p>where<em> C</em><em>′</em> is the set of new prompts generated from<em> C</em> using the model<em> f</em>. If we consider<em> f</em> as an LLM, we can perform the expansion operation by instructing<em> f</em> to generate new and relevant prompts based on<em> C</em>. Below is an example.</p></div>
<div class="para"><p>Below is a prompt for an LLM. Please provide some new prompts to perform the same task.</p></div>
<div class="para"><p>Input:<em> {∗</em>prompt<em>∗}</em></p></div>
<div class="para"><p>Then, we replace<em> C</em> with<em> C</em><em>′</em>. The steps of evaluation, pruning and expansion can be repeated, and so we can gradually explore a wider range of prompts.</p></div>
<div class="para"><p>In prompt optimization, the expansion step plays a key role, as it defines how we explore the search space, and our goal is to find optimal results with minimal effort. One improvement to this step is to treat the problem as a paraphrasing task. A simple method is to apply off-theshelf paraphrasing systems, either based on LLMs or other models, to transform input prompts into semantically equivalent forms [Jiang et al., 2020]. Alternatively, we can define specific edit operations, such as insertions and modifications, for each token. A given prompt can be edited into new prompts by applying these operations [Prasad et al., 2023]. Also, further evaluation and pruning can be applied to filter out low-quality prompts. In addition to framing prompt generation as a paraphrasing problem, we can improve the quality of prompts during expansion by learning from feedback [Pryzant et al., 2023]. This approach is somewhat related to the self-refinement issue discussed in Section 3.2.3. An LLM can be used to generate feedback on an input prompt,which is then revised based on this feedback. This feedback-and-revision cycle can be repeated multiple times until the result converges or the desired outcome is achieved.</p></div>
<div class="para"><p>Another approach to prompt optimization is to apply classic optimization techniques. For example, the problem can be framed as an evolutionary computation problem, where prompts are treated as candidates that evolve generation by generation as the optimization progresses [Guo et al., 2024]. Since many powerful optimization algorithms have been developed in related fields,they can be directly applied to this problem.</p></div>
<div class="para"><p>142 <em>Prompting</em></p></div>
<div class="para"><p>In practice, we might be tempted to use existing LLM APIs to implement the steps described above. Such an approach, however, would be strongly dependent on the inference and in-context learning abilities of the LLMs. If these LLMs are not strong and lack adaptation to the tasks, they may introduce errors into search, for example, generating incorrect prompts during expansion. In such cases, it is preferable to train models that are better suited to the tasks. One approach in this research direction appeals to reinforcement learning, which has been widely used in solving discrete decision making and optimization problems. For example, Deng et al. [2022] developed a prompt generator by integrating an FFN-based adaptor into an LLM. The prompt generator is trained as a typical policy network, but only the parameters of the adaptor are updated while the remaining parameters of the model are kept unchanged. During training, the reward is obtained by testing the generated prompts using another LLM, similar to the evaluation method as discussed above. Once the training is complete, the prompt generator is then employed to generate new prompts.</p></div>
<div class="para"><p>Note that, in our discussion here, prompts are simply seen as sequences of tokens, and the output of prompt optimization is such a sequence. However, in a strict sense, prompts have complex structures and include different fields such as user input, instruction, and demonstration. While our discussed approaches are mostly general, much work in prompt optimization has focused on learning better instructions for prompting. Specifically, the goal is to generate instructions that effectively guide LLMs based on a given task. Of course, the concept of prompt optimization can also be extended to learning other parts of prompts. For example, there has been substantial research interest in learning to select or generate demonstrations in CoT [Liu et al., 2022; Rubin et al., 2022; Zhang et al., 2023b]. One of the differences between learning instructions and learning demonstrations is that generating high-quality demonstrations using LLMs is relatively easy and the focus of learning demonstrations is typically on how to sample appropriate demonstrations from a pool of candidates. In contrast, the difficulty in learning instructions is partly because pre-trained LLMs are not suited to predict the quality of instructions, and testing these instructions on downstream tasks is computationally expensive. This makes the optimization methods costly to apply, and exploring a wide variety of instructions poses significant challenges.</p></div>
<div class="para"><p><strong>3.3.2</strong> <strong>Soft Prompts</strong></p></div>
<div class="para"><p>Although developing natural language prompts, either manually or automatically, is a straightforward and widely applied approach, it presents some problems. One problem is that natural language prompts can be complex and lengthy, resulting in significant computational burdens when processed via LLMs. In many applications, users may need to perform a task repeatedly,and inputting the same long prompt into the LLMs a large number of times is clearly inefficient.Another problem is that while prompts are typically represented as discrete token sequences (call them<strong> hard prompts</strong>) in regular LLM input, the LLMs encode them as low-dimensional realvalued vectors. This raises the question of whether there are more compact and efficient ways to represent prompts.</p></div>
<div class="para"><p>In this subsection, we introduce the concept of<strong> soft prompts</strong>, which can be viewed as hidden,distributed representations of prompts. When prompting LLMs, we are concerned with communicating tasks or questions to elicit the desired responses. We can define hard prompts as explicit,predefined text sequences that users input directly into LLMs to guide the responses. In contrast,we can think of soft prompts as implicit, adaptable prompting patterns embedded within LLMs.Unlike hard prompts, which are expressed in natural language and should be understandable for</p></div>
<h3>3.3 Learning to Prompt 143</h3>
<div class="para"><p>....................................</p></div>
<div class="para"><p>Soft Prompt</p></div>
<div class="para"><p>...<strong>h</strong> <em>j</em> <strong>h</strong> <em>j</em>+1<strong> h</strong> <em>j</em>+2<strong> h</strong> <em>j</em>+3<strong> h</strong> <em>j</em>+4<strong> h</strong> <em>j</em>+5<strong> h</strong> <em>j</em>+6<strong> h</strong> <em>j</em>+7<strong> h</strong> <em>j</em>+8<strong> h</strong> <em>j</em>+9...</p></div>
<h1>Transformer</h1>
<div class="para"><p>...Translate this into Chinese.I have a cat....</p></div>
<div class="para"><p>Hard Prompt (Instruction)</p></div>
<div class="para"><p><strong>Fig. 3.3:</strong> Illustration of hard and soft prompts. Here the hard prompt is the instruction we input to the LLM for performing the task. The LLM encodes this instruction as usual, and the intermediate representations corresponding to the instruction can be viewed as some sort of soft prompt.</p></div>
<h3>humans, soft prompts are encoded in a format that is more comprehensible to the model rather than to humans. To illustrate, consider a simple prompt</h3>
<h3>Translate the sentence into Chinese.</h3>
<h3>Consider it done!</h3>
<h3>Here, the instruction “Translate the sentence into Chinese” can be seen as a hard prompt, denoted by the token sequence c 1...c 5. By feeding these tokens into an LLM, they are transformed into a sequence of real-valued vectors h 1...h 5, each corresponding to a token. We can roughly think of h 1...h 5 as a soft prompt, as illustrated in Figure 3.3.</h3>
<h3>While the above example shows that soft prompts can be generated by transforming hard prompts, there is not necessarily a direct correspondence between them. In fact, we do not even need to interpret soft prompts using meaningful text. They are instead simply hidden states in LLMs and can be learned as standard parameters of the models through continuous optimiza- tion. Such a treatment allows us to explore prompting methods beyond text. As another benefit, soft prompts provide dense, low-dimensional, and learnable representations for encoding how we guide LLMs to generate specific outputs. The training and application of these representa- tions require significantly lower computational costs than those required for processing long hard prompts. This approach would be of great practical value in LLM inference applications where the same prompt is repeatedly used.</h3>
<h3>3.3.2.1 Adapting LLMs with Less Prompting</h3>
<h3>One obvious way to adapt an LLM for a particular task is to simply fine-tune the model using labeled data. This leads to a variety of LLM alignment methods, such as supervised fine-tuning, which update the model parameters by aligning the responses to given prompts with supervision signals. Fine-tuned LLMs embed task-related information in model parameters, and thus these</h3>
<div class="para"><p>144 <em>Prompting</em></p></div>
<div class="para"><p>models can respond correctly when dealing with similar prompts with those in fine-tuning.</p></div>
<div class="para"><p>If we take this idea further, we can expect LLMs to absorb the knowledge about prompting of a task as much as possible during fine-tuning. Consequently, the prompting information is partially captured in the model parameters, and the fine-tuned LLMs can perform the task with less prompting. Here we consider a simple form of prompt, where only an instruction (denoted by <strong>c</strong>) and a user input (denoted by<strong> z</strong>) are included. A prompt can be expressed using the following tuple</p></div>
<div class="para"><p><strong>x</strong>=(<strong>c</strong><em>,</em><strong> z</strong>)(3.11)</p></div>
<div class="para"><p>Given a set of prompt-response pairs<em> D</em> =<em> {</em>(<strong>x</strong><em>,</em><strong> y</strong>)<em>}</em>, the objective of fine-tuning is to minimize the total loss incurred over this set. A popular method is to minimize the negative log-likelihood(i.e., maximize the log-likelihood) with respect to the model parameters<em> θ</em>:</p></div>
<div class="para"><p>ˆ <em>θ</em>=arg max <em>θ</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(<strong>x</strong><em>,</em><strong>y</strong>)<em>∈D</em> log Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(<strong>x</strong><em>,</em><strong>y</strong>)<em>∈D</em> log Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>c</strong><em>,</em><strong> z</strong>)(3.12)</p></div>
<div class="para"><p>=arg max <em>θ</em></p></div>
<div class="para"><p>where Pr <em>θ</em>(<em>·|·</em>) is the probability predicted by an LLM with the parameters<em> θ</em> 9.</p></div>
<div class="para"><p>In general, the instruction in each fine-tuning example should follow the guideline of prompt design, for example, a good instruction should be as clear as possible and provide a detailed description of the task. However, the method described in the above equation does not restrict the instruction to any particular form. This flexibility allows us to instruct LLMs in any way that we want. Consider an example where we intend to instruct LLMs to translate an English sentence into Chinese. Of course, as mentioned earlier in this chapter, we can prompt LLMs using the instruction</p></div>
<div class="para"><p><em>Translate the following sentence from English to Chinese.</em></p></div>
<div class="para"><p>If we want the instruction to be simpler, we may rephrase it into a simpler form</p></div>
<div class="para"><p><em>Translate this into Chinese.</em></p></div>
<div class="para"><p>Even, we can define the instruction as a single phrase</p></div>
<div class="para"><p><em>Translate!</em></p></div>
<div class="para"><p>With certain fine-tuning effort, we can adapt LLMs to follow any of these instructions. From an efficient prompting perspective, there are computational advantages in simplifying instructions in prompting. For example, we can use simple instructions like “Translate!” to perform tasks that would typically require more complex and detailed instructions. This can make subsequent</p></div>
<div class="para"><p>9 In practice, we initialize<em> θ</em> with the parameters obtained from pre-training, and then adjust<em> θ</em> moderately to ensure that the results after fine-tuning do not deviate too much from the pre-trained results.</p></div>
<div class="para"><p><em>3.3 Learning to Prompt</em> 145</p></div>
<div class="para"><p>Teacher Model:</p></div>
<div class="para"><p>Full Context User Input+Pr <em>t</em>(<strong>y</strong><em>|</em><strong>c</strong><em>,</em><strong> z</strong>)</p></div>
<div class="para"><p><strong>c</strong> <strong>z</strong> <strong>y</strong></p></div>
<div class="para"><p>Loss</p></div>
<div class="para"><p>Student Model:</p></div>
<div class="para"><p>Simplified Context User Input+Pr <em>s</em>(<strong>y</strong><em>|</em><strong>c</strong><em>′</em><em>,</em><strong> z</strong>)</p></div>
<div class="para"><p><strong>c</strong><em>′</em><strong>z</strong> <strong>y</strong></p></div>
<div class="para"><p><strong>Fig. 3.4:</strong> Illustration of context distillation [Snell et al., 2022]. The teacher model is a standard LLM, which takes both the context and the user input as model input and produces a prediction as model output. Then, we simplify the context(e.g., simplifying the instruction in prompting) and use the student model to make predictions based on the simplified context and the user input. The student model is trained by minimizing the loss between the predictions produced by the two models.</p></div>
<div class="para"><p>prompting during inference much easier. On the other hand, fine-tuning LLMs with overly simplified instructions may be harmful to the generalization of the models. Since simplified instructions can lead to a loss of information, it is more likely that the LLMs will overfit the fine-tuning data and fail to generalize beyond those instructions. In scenarios involving both complex and simplified instructions for fine-tuning, this problem is more severe because the labeled data available for fine-tuning is usually limited, and accommodating a variety of instructions is costly.</p></div>
<div class="para"><p>An alternative way to adapt LLMs for simplified instructions is through knowledge distillation.As an example, we consider the context distillation method [Snell et al., 2022]. The goal of this method is to learn a student model that can make use of simplified instructions from a well-trained instruction-following teacher model. Figure 3.4 shows an illustration of this approach. Building the teacher model follows a standard fine-tuning process: we first collect a certain amount of data that includes instructions, user inputs, and correct responses, and then we continue to train a pre-trained model with this dataset. For building the student model, we need to construct a new dataset<em> D</em><em>′</em> where each sample is a tuple consisting of an instruction, a corresponding simplified instruction, and a user input, denoted by<strong> x</strong><em>′</em> = (<strong>c</strong><em>,</em><strong> c</strong><em>′</em><em>,</em><strong> z</strong>). Knowledge distillation is performed by minimizing a loss function defined on the outputs of the teacher and student models</p></div>
<div class="para"><p>ˆ <em>θ</em>=arg min <em>θ</em></p></div>
<div class="para"><p><strong>x</strong><em>′</em><em>∈D</em><em>′</em>Loss(Pr <em>t</em>(<em>·|·</em>)<em>,</em> Pr <em>s</em> <em>θ</em>(<em>·|·</em>)<em>,</em><strong> x</strong><em>′</em>)(3.13)</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>where Pr <em>t</em>(<em>·|·</em>) denotes the pre-trained teacher model, and Pr <em>s</em> <em>θ</em>(<em>·|·</em>) denotes the student model with the parameters<em> θ</em>. To keep the notation simple we will write Loss(Pr <em>t</em>(<em>·|·</em>)<em>,</em> Pr <em>s</em> <em>θ</em>(<em>·|·</em>)<em>,</em><strong> x</strong>) as Loss for short. A commonly-used loss is the sequence-level loss, which has the basic form:</p></div>
<div class="para"><p><strong>y</strong> Pr <em>t</em>(<strong>y</strong><em>|</em><strong>c</strong><em>,</em><strong> z</strong>) log Pr <em>s</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>c</strong><em>′</em><em>,</em><strong> z</strong>)(3.14)</p></div>
<div class="para"><p>Loss=X</p></div>
<div class="para"><p>But this function is computationally infeasible because it requires summing over an exponentially large number of outputs. A variant of this method is to train the student model using outputs generated by the teacher model. For each sample, we use the teacher model to produce an output</p></div>
<div class="para"><p>146 <em>Prompting</em></p></div>
<div class="para"><p>ˆ <strong>y</strong> = arg max <strong>y</strong> log Pr <em>t</em>(<strong>y</strong><em>|</em><strong>c</strong><em>,</em><strong> z</strong>). Then we consider ˆ <strong>y</strong> as the target for learning, and the loss function is given by</p></div>
<div class="para"><p>Loss=log Pr <em>s</em> <em>θ</em>(ˆ <strong>y</strong><em>|</em><strong>c</strong><em>′</em><em>,</em><strong> z</strong>)(3.15)</p></div>
<div class="para"><p>Alternatively, we can minimize the distances between the probability distributions outputted by the two models [Askell et al., 2021]. For example, the loss function can be defined as the KL divergence between the two output distributions</p></div>
<div class="para"><p>Loss=KL(P <em>t</em><em> </em><em>||</em> P <em>s</em> <em>θ</em>)(3.16)</p></div>
<div class="para"><p>where</p></div>
<div class="para"><p>P <em>t</em>=Pr <em>t</em>(<em>·|</em><strong>c</strong><em>,</em><strong> z</strong>)(3.17)</p></div>
<div class="para"><p>P <em>s</em> <em>θ</em>=Pr <em>s</em> <em>θ</em>(<em>·|</em><strong>c</strong><em>′</em><em>,</em><strong> z</strong>)(3.18)</p></div>
<div class="para"><p>Although we have restricted ourselves to knowledge distillation for instructions, the approaches discussed here are general. By learning from the outputs of the teacher model, the knowledge in prompting can be distilled into the parameters of the student model. Therefore, the distilled model can be considered as encoding some sort of soft prompt. This method can be applied to many other problems in prompt learning, such as compressing long contexts and learning soft prompts as specific components of LLMs.</p></div>
<div class="para"><p><strong>3.3.2.2</strong> <strong>Learning Soft Prompts for Parameter-efficient Fine-tuning</strong></p></div>
<div class="para"><p>Updating all parameters is a common method for adapting LLMs to tasks of interest. Although fine-tuning is considered computationally cheaper than pre-training, it is still costly to apply in practice. This issue motivates the development of parameter-efficient fine-tuning methods, which aim to minimize the number of parameters that need to be updated.</p></div>
<div class="para"><p>One approach, known as<strong> prefix fine-tuning</strong>, is to append a series of trainable vectors, or prefixes, at the beginning of the input of each Transformer layer [Li and Liang, 2021]. These prefixes can be thought of as soft prompts that serve as additional context to guide the behavior of the model under specific tasks. During fine-tuning, we need only to learn the prefixes for embedding task-specific knowledge. Thus, this method is efficient because it only modifies a small part of the model rather than adjusting the entire set of model parameters.</p></div>
<div class="para"><p>Specifically, let the input of a layer at depth<em> l</em> be denoted by<strong> H</strong> <em>l</em> =<strong> h</strong> <em>l</em> 0 <strong>h</strong> <em>l</em> 1<em>...</em><strong>h</strong> <em>l</em> <em>m</em>. The output of the layer can be expressed as</p></div>
<div class="para"><p><strong>H</strong> <em>l</em>+1=Layer(<strong>H</strong> <em>l</em>)(3.19)</p></div>
<div class="para"><p>In prefix fine-tuning, we extend the sequence<strong> h</strong> <em>l</em> 0 <strong>h</strong> <em>l</em> 1<em>...</em><strong>h</strong> <em>l</em> <em>m</em> by adding a few vectors at the beginning, which we denote as<strong> p</strong> <em>l</em> 0 <strong>p</strong> <em>l</em> 1<em>...</em><strong>p</strong> <em>l</em> <em>n</em>. Hence<strong> H</strong> <em>l</em> can be written in the form</p></div>
<div class="para"><p><strong>H</strong> <em>l</em> =<strong> p</strong> <em>l</em> 0<strong> </strong><strong>p</strong> <em>l</em> 1<em> </em><em>...</em><strong> p</strong> <em>l</em> <em>n</em>|{z}trainable</p></div>
<div class="para"><p><strong>h</strong> <em>l</em> 0<strong> </strong><strong>h</strong> <em>l</em> 1<em> </em><em>...</em><strong> h</strong> <em>l</em> <em>m</em>|{z}previous layer output</p></div>
<div class="para"><p>(3.20)</p></div>
<div class="para"><p><em>3.3 Learning to Prompt</em> 147</p></div>
<div class="para"><p>The output of the layer is the last<em> m</em> + 1 representations.</p></div>
<div class="para"><p><strong>H</strong> <em>l</em>+1=Layer(<strong>H</strong> <em>l</em>)[<em>−</em><em>m</em><em> −</em>1 :]</p></div>
<div class="para"><p>=<strong>h</strong> <em>l</em>+1 0 <strong>h</strong> <em>l</em>+1 1<em>...</em><strong> h</strong> <em>l</em>+1 <em>m</em>(3.21)</p></div>
<div class="para"><p>where [<em>−</em><em>m</em><em>−</em>1 :] denotes the slicing operation that extracts the last<em> m</em>+1 elements of a sequence.Given<strong> H</strong> <em>l</em>+1, the input of the next layer can be expressed in the same form of Eq. (3.20):</p></div>
<div class="para"><p><strong>H</strong> <em>l</em>+1=<strong>p</strong> <em>l</em>+1 0 <strong>p</strong> <em>l</em>+1 1<em>...</em><strong> p</strong> <em>l</em>+1 <em>n</em> <strong>H</strong> <em>l</em>+1</p></div>
<div class="para"><p>=<strong>p</strong> <em>l</em>+1 0 <strong>p</strong> <em>l</em>+1 1<em>...</em><strong> p</strong> <em>l</em>+1 <em>n</em> <strong>h</strong> <em>l</em>+1 0 <strong>h</strong> <em>l</em>+1 1<em>...</em><strong> h</strong> <em>l</em>+1 <em>m</em>(3.22)</p></div>
<div class="para"><p>Here each<strong> p</strong> <em>i</em><em> ∈</em>R <em>d</em> can be seen as a learnable parameter. During training,<strong> p</strong> <em>l</em> 0 <strong>p</strong> <em>l</em> 1<em>...</em><strong>p</strong> <em>l</em> <em>n</em> are trained as usual, and the parameters of the original Transformer model are kept fixed.</p></div>
<div class="para"><p>Figure 3.5 shows an illustration of prefix fine-tuning for a translation task. Here, only the prefix vectors<strong> p</strong> <em>l</em> 0 and<strong> p</strong> <em>l</em> 1 are updated by receiving the error gradients from the output (i.e., the Chinese translation). By adjusting these vectors for the translation task, the model adapts accordingly. This makes<strong> p</strong> <em>l</em> 0 and<strong> p</strong> <em>l</em> 1 serve as prompts which activate the LLM to perform the task without needing explicit input prompts like “Translate the following sentence from English to Chinese”. At test time, we prepend the optimized<strong> p</strong> <em>l</em> 0 and<strong> p</strong> <em>l</em> 1 to the layer, and the LLM will then translate the input sentence. Note that prefix fine-tuning introduces additional<em> L</em><em> ×</em><em> n</em><em> ×</em><em> d</em> parameters, where<em> L</em> is the number of layers,<em> n</em> is the number of prefixes, and<em> d</em> is the dimensionality of each prefix. However,this number is much smaller compared to the total number of parameters in the LLM, making the fine-tuning process highly efficient.</p></div>
<div class="para"><p>While prefix fine-tuning is simple, it still requires modifications to LLMs. Alternatively, separating soft prompts from the LLMs allows us to preserve the original model architecture, making it more efficient for deployment across different tasks without the need to adjust the core model.One such method is prompt tuning [Lester et al., 2021]. Like prefix fine-tuning, prompt tuning incorporates trainable vectors so that LLMs can adapt to given tasks by adjusting these vectors.However, prompt tuning differs in that it modifies only the embedding layer.</p></div>
<div class="para"><p>Recall that in LLMs each input token<em> z</em> <em>i</em> is represented by an embedding<strong> e</strong> <em>i</em>. These embeddings are generally learned through a token embedding model and are then used as the real inputs to the LLMs, replacing the symbolically represented tokens. In prompt tuning, a number of pseudo embeddings<strong> p</strong> 0<em>...</em><strong>p</strong> <em>n</em> are added at the beginning of the token embedding sequence. So the actual input to the LLMs can be expressed as</p></div>
<div class="para"><p><strong>p</strong> 0<strong> p</strong> 1<em> ...</em><strong> p</strong> <em>n</em>|{z}trainable <strong>e</strong> 0<strong> e</strong> 1<em> ...</em><strong> e</strong> <em>m</em>|{z}token embeddings</p></div>
<div class="para"><p>Note that a pseudo embedding needs not to correspond to any token in natural language. Instead these embeddings can be seen as “soft prompt embeddings” that serve to condition the LLMs.By training soft prompt embeddings on task-specific data, they learn to interact adaptively with the token embeddings<strong> e</strong> 0<em>...</em><strong>e</strong> <em>m</em> and guide the behavior of LLMs. Since prompt tuning does not change the underlying parameters of pre-trained LLMs, it is considered a lightweight and efficient method of fine-tuning, improving task-specific performance while maintaining their generalization capabilities. See Figure 3.6 for an illustration of prompt tuning.</p></div>
<h3>148 Prompting</h3>
<div class="para"><p>Loss Loss</p></div>
<ul><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li></ul>
<h3>Layer l + 1</h3>
<div class="para"><p><strong>p</strong> <em>l</em>+1 0 <strong>p</strong> <em>l</em>+1 1 <strong>h</strong> <em>l</em>+1 0 <strong>h</strong> <em>l</em>+1 1 <strong>h</strong> <em>l</em>+1 3 <strong>h</strong> <em>l</em>+1 4 <strong>h</strong> <em>l</em>+1 5</p></div>
<h3>Layer l</h3>
<div class="para"><p><strong>p</strong> <em>l</em> 0 <strong>p</strong> <em>l</em> 1 <strong>h</strong> <em>l</em> 0 <strong>h</strong> <em>l</em> 1 <strong>h</strong> <em>l</em> 2 <strong>h</strong> <em>l</em> 3 <strong>h</strong> <em>l</em> 4</p></div>
<div class="para"><p>trainable prefixes</p></div>
<h3>Layer l −1</h3>
<div class="para"><p><strong>p</strong> <em>l</em><em>−</em>1 0 <strong>p</strong> <em>l</em><em>−</em>1 1 <strong>h</strong> <em>l</em><em>−</em>1 0 <strong>h</strong> <em>l</em><em>−</em>1 1 <strong>h</strong> <em>l</em><em>−</em>1 3 <strong>h</strong> <em>l</em><em>−</em>1 4 <strong>h</strong> <em>l</em><em>−</em>1 5</p></div>
<ul><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li></ul>
<div class="para"><p>Look out!小 心!</p></div>
<div class="para"><p>User Input LLM Prediction Soft Prompt</p></div>
<div class="para"><p><strong>Fig. 3.5:</strong> Illustration of prefix fine-tuning for a translation task (Look out!<em> →</em>小 心!). For each layer, we add two prefixes<strong> p</strong> <em>l</em> 0 and<strong> p</strong> <em>l</em> 1 at the beginning. The LLM is trained to minimize the loss on the predictions given the input.During this process, only the prefixes are optimized while the rest of the parameters remain fixed. Therefore, the model can adapt to the given task in a very efficient manner. At inference time, the LLM works with optimized prefixes, and can perform the task without the need of explicit hard prompts.</p></div>
<h3>Since p 0 p 1 ... p n is itself a sequence, we can employ sequence models to better represent it. For example, a Transformer model can encode this sequence, and the resulting representation can then be used as the input to the LLM. In other words, we can develop an additional model for encoding soft prompts. Another way to improve prompting is by combining soft and hard prompts, thereby taking advantage of both types [Liu et al., 2023b]. In the embedding sequence, we can arrange or intersperse these prompts. This would result in different prompt patterns. For example, a simple pattern that uses both two types of prompt is</h3>
<div class="para"><p>Soft Prompt Hard Prompt User Input and Response</p></div>
<h3>p 0 p 1 · · p n q 0 q 1 · · q m′ e 0 e 1 · · e m</h3>
<h3>c 0 c 1 · · c m′ z 0 z 1 · · z m</h3>
<h3>where c 0...c m′ denotes the hard prompt and q 0...q m′ denotes the corresponding embedding se- quence.</h3>
<h3>Here we have considered methods for inserting soft prompts in LLMs. But we skip the details of training these soft prompts and assume that the reader is familiar with the standard supervised learning process, that is, maximizing the likelihood of the correct model output given the model</h3>
<div class="para"><p><em>3.3 Learning to Prompt</em> 149</p></div>
<div class="para"><p>Loss Loss</p></div>
<ul><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li></ul>
<div class="para"><p>Layer<em> l</em> + 1</p></div>
<div class="para"><p>Layer<em> l</em></p></div>
<div class="para"><p>Layer<em> l</em><em> −</em>1</p></div>
<ul><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li><li>· ·</li></ul>
<div class="para"><p><strong>p</strong> 0 <strong>p</strong> 1 <strong>e</strong> 0 <strong>e</strong> 1 <strong>e</strong> 2 <strong>e</strong> 3 <strong>e</strong> 4</p></div>
<div class="para"><p>trainable prompt embeddings</p></div>
<div class="para"><p>Look out!小 心!</p></div>
<div class="para"><p>User Input LLM Prediction Soft Prompt</p></div>
<div class="para"><p><strong>Fig. 3.6:</strong> Illustration of prompt tuning for a translation task (Look out!<em> →</em>小 心!). Instead of using fixed textual prompts, soft prompts are learnable embeddings that are added at the beginning of the embedding sequence. During fine-tuning, only these prompt embeddings are optimized to efficiently adapt the LLM to the given task. Once optimized, the prompt embeddings are used to instruct the LLM to perform the task as new data arrives.</p></div>
<div class="para"><p>input. In fact, learning soft prompts can be related to many issues in LLM fine-tuning. For example, if we consider it as a context compression problem, we can apply the knowledge distillation methods described previously. In Mu et al. [2024]’s work, prompts are compressed and represented as a few pseudo tokens, which are appended to each input sequence. The embeddings of these pseudo tokens are optimized to mimic the predictions of a standard-prompted model. In other words, the prompting knowledge is distilled from a teacher model into the pseudo tokens.</p></div>
<div class="para"><p>Broadly speaking, many parameter-efficient fine-tuning methods can be thought of as learning some sort of soft prompt [Lialin et al., 2023]. When we fine-tune a part of an LLM for a task, this process can essentially be seen as injecting task-related prompting information into that specific part of the model. Another widely-used approach to parameter-efficient fine-tuning is to add an adaptor layer between the existing model layers. This approach allows us to fine-tune only the adaptor layer on specific tasks without altering the underlying architecture or retraining the entire model. In this sense, adaptor layers can be viewed as soft prompts that encode prompting and taskrelated information and interact with the original LLM to help it adapt. To summarize, Figure 3.7 shows a comparison of different methods of using soft prompts in LLMs.</p></div>
<div class="para"><p><strong>3.3.2.3</strong> <strong>Learning Soft Prompts with Compression</strong></p></div>
<div class="para"><p>Another approach to learning soft prompts is from the perspective of compression. As a simple example, consider the problem of approximating a long context using a continuous representation[Wingate et al., 2022]. Suppose we have a user input<strong> z</strong> and its context<strong> c</strong> (such as long instructions and demonstrations). Now we want to develop a compressed representation of the context, denoted</p></div>
<div class="para"><p>150 <em>Prompting</em></p></div>
<div class="para"><p>LLM</p></div>
<div class="para"><p>LLM</p></div>
<div class="para"><p>(a) Soft Prompts as Prefixes</p></div>
<div class="para"><p>(b) Soft Prompts as Inputs (Embeddings)</p></div>
<div class="para"><p>Layer</p></div>
<div class="para"><p>Adaptor</p></div>
<div class="para"><p>LLM</p></div>
<div class="para"><p>LLM</p></div>
<div class="para"><p>(c) Fine-tuning Parts of the Model</p></div>
<div class="para"><p>(d) Fine-tuning the Adaptor</p></div>
<div class="para"><p><strong>Fig. 3.7:</strong> Illustrations of using soft prompts in LLMs. Here tunable soft prompts are shown in blue, and components whose parameters are fixed during fine-tuning are shown in gray. In sub-figure (a), soft prompts are prefixes appended to each layer of the LLM. In sub-figure (b), soft prompts are used as input embeddings for the LLM. In sub-figures (c)and (d), soft prompts are broadly treated as components of the model that are fine-tuned for task adaptation.</p></div>
<div class="para"><p>by<em> σ</em>, such that the prediction based on<strong> z</strong> and<em> σ</em> is as close as possible to the prediction based on<strong> z</strong> and<strong> c</strong>. This goal can be expressed in the form</p></div>
<div class="para"><p>ˆ <em>σ</em>=arg min <em>σ</em> <em>s</em>(ˆ <strong>y</strong><em>,</em> ˆ <strong>y</strong> <em>σ</em>)(3.23)</p></div>
<div class="para"><p>where ˆ <strong>y</strong> = arg max <strong>y</strong> Pr(<strong>y</strong><em>|</em><strong>c</strong><em>,</em><strong> z</strong>) and ˆ <strong>y</strong> <em>σ</em> = arg max <strong>y</strong> <em>σ</em> Pr(<strong>y</strong><em>|</em><em>σ,</em><strong> z</strong>) are the LLM predictions given the full context and the compressed context, respectively. The function<em> s</em>(<em>·</em><em>,</em><em> ·</em>) typically represents a loss or similarity measure, aiming to minimize the difference in predictions between the two context representations.</p></div>
<div class="para"><p>One general framework for achieving this is knowledge distillation, where ˆ <strong>y</strong> and ˆ <strong>y</strong> <em>σ</em> can be seen as the predictions of the teacher model and the student model, respectively. This formalization links our discussion to the context distillation problem discussed earlier. The training objective can be obtained by analogy with Eqs. (3.15) and (3.16). For example, a simple training objective is given by</p></div>
<div class="para"><p>ˆ <em>σ</em>=arg max <em>σ</em> log Pr(ˆ <strong>y</strong><em>|</em><em>σ,</em> z)(3.24)</p></div>
<div class="para"><p>Alternatively, we can minimize the KL divergence between the output distributions, giving</p></div>
<div class="para"><p>ˆ <em>σ</em>=arg min <em>σ</em> KL(Pr(<em>·|</em><strong>c</strong><em>,</em><strong> z</strong>)<em> ||</em> Pr(<em>·|</em><em>σ,</em><strong> z</strong>))(3.25)</p></div>
<div class="para"><p>The difference with the models in Eqs. (3.15) and (3.16) is that here the compressed context is represented as real-valued vectors (call them<strong> prompt embeddings</strong>), rather than as normal tokens.By applying the above methods, we distill the context from the token sequence<strong> c</strong> into the embeddings<em> σ</em>. Note that the teacher model Pr(<em>·|</em><strong>c</strong><em>,</em><strong> z</strong>) and the student model Pr(<em>·|</em><em>σ,</em><strong> z</strong>) may not share the same architecture or model settings. In practice, we generally wish for the teacher model to be</p></div>
<h3>3.3 Learning to Prompt 151</h3>
<div class="para"><p><strong>h</strong><em>&lt;i</em> 1 <strong>h</strong><em>&lt;i</em> 1 <strong>h</strong> 1 <strong>h</strong> 2 <strong>h</strong> 3 <strong>h</strong> 4 <em>σ</em><em>&lt;i</em>+1 1 <em>σ</em><em>&lt;i</em>+1 2</p></div>
<div class="para"><p>Soft Prompts at the Current Step</p></div>
<h2>Transformer Layers</h2>
<div class="para"><p><em>σ</em><em>&lt;i</em> 1 <em>σ</em><em>&lt;i</em> 2 <strong>e</strong> <em>i</em> 1 <strong>e</strong> <em>i</em> 2 <strong>e</strong> <em>i</em> 3 <strong>e</strong> <em>i</em> 4<em>⟨</em><strong>e</strong> 1<em>⟩</em><em>⟨</em><strong>e</strong> 2<em>⟩</em></p></div>
<div class="para"><p>Soft Prompts at Step<em> i</em><em> −</em>1</p></div>
<div class="para"><p><em>z</em> <em>i</em> 1 <em>z</em> <em>i</em> 2 <em>z</em> <em>i</em> 3 <em>z</em> <em>i</em> 4<em>⟨</em>g 1<em>⟩</em><em>⟨</em>g 2<em>⟩</em></p></div>
<div class="para"><p><strong>Fig. 3.8:</strong> Illustration of compressing a context segment into soft prompts (<em>κ</em> = 2 and<em> m</em> <em>i</em> = 4). The input to the LLM includes the soft prompts from the previous step (<em>σ</em><em>&lt;i</em> 1 and<em> σ</em><em>&lt;i</em> 2 ), the tokens of the segment (<em>z</em> 1<em>, z</em> 2<em>, z</em> 3<em>,</em> and<em> z</em> 4),and the summary tokens (<em>⟨</em>g 1<em>⟩</em>and<em> ⟨</em>g 2<em>⟩</em>). Given these, the LLM operates as usual. We then extract the outputs at the last Transformer layer that correspond to the summary tokens. These outputs can be viewed as the soft prompts that accumulated up to this segment.</p></div>
<h3>stronger, while the student model should be smaller and more efficient.</h3>
<h3>While compressing full context into continuous representations is a straightforward approach to learning soft prompts, it requires a teacher model that can deal with long input sequences. In many cases, however, the context is so long that applying an LLM is too costly or infeasible. Modeling long input sequences can fall under the broad family of efficient methods for long- context LLMs. Many techniques have been developed to address this issue. For example, one can use a fixed-size KV cache to store the past information at each step during inference. Efficient Transformer architectures and long-context LLMs have been intensively discussed in this book. For more detailed discussions of these topics, interested readers can refer to Chapter 2.</h3>
<h3>There are also methods specifically designed to compress long context into soft prompts. Here we consider Chevalier et al. [2023]’s method as an example. The basic idea is that we learn soft prompts gradually by accumulating the fixed-size context representation over the context sequence. Given a long context, we first divide it into a number of segments z 1, ..., z K. We then process these segments in sequence, each time generating a representation of the context we have processed so far, denoted by σ&amp;lt;i+1. To do this, a few summary tokens ⟨g 1⟩, ..., ⟨g κ⟩ are introduced. At each step, we take a segment z i = z i 1...z i m i, along with the previous context representation σ&amp;lt;i and the summary tokens ⟨g 1⟩, ..., ⟨g κ⟩as input, and use an LLM to produce the corresponding hidden representation sequence at the last Transformer layer. An example of this process is illustrated in Figure 3.8.</h3>
<h3>Here σ&amp;lt;i is essentially a memory. The model operates in an RNN fashion. Each time we take a segment and update this memory by encoding both the previous memory state and the segment. Therefore, the σ&amp;lt;i produced at the last segment is a representation of the entire context sequence. The Transformer model for learning these representations can be a standard LLM but we need to fine-tune it to adapt to this context representation task.</h3>
<h3>Note that here we simply consider prompt and context as similar terms, even though they are not the same. Although we are somewhat “misusing” the concept prompt, we can often view it as a type of context. From this perspective, the methods discussed here can be applied to general text compression problems.</h3>
<div class="para"><p>152 <em>Prompting</em></p></div>
<div class="para"><p><strong>3.3.3</strong> <strong>Prompt Length Reduction</strong></p></div>
<div class="para"><p>While soft prompts provide dense, hidden representations, they are not directly interpretable. The lack of interpretability can be a significant barrier for users trying to understand how their inputs influence LLM outputs. Moreover, although soft prompts are efficient for fine-tuning and deployment, they are inflexible and do not allow for easy adjustments without extensive fine-tuning or modification. This inflexibility can limit their utility in dynamic environments where prompt changes are frequently needed.</p></div>
<div class="para"><p>One alternative way to develop efficient prompts is to simplify the text used for prompting.For example, below is a prompt for answering questions on healthcare and finance.</p></div>
<div class="para"><p>The task involves developing a language model capable of understanding and responding to user inquiries across various domains, with a particular emphasis on healthcare and finance. Considering the broad range of potential queries,from the specifics of medical diagnoses to the nuances of financial regulations,the model must ensure a comprehensive understanding and accurate responses.</p></div>
<div class="para"><p>Question:</p></div>
<div class="para"><p>What are the best practices for using artificial intelligence in diagnosing cardiovascular diseases?</p></div>
<div class="para"><p>We can simplify the task description by deleting the unimportant parts.</p></div>
<div class="para"><p><em>The task involves developing a language model capable of understanding</em> <em>and responding to user inquiries</em><em> across various domains, with a particular</em> <em>emphasis</em><em> on healthcare and finance.</em><em> Considering the broad range of potential</em> <em>queries, from the specifics of medical diagnoses to the nuances of financial</em> <em>regulations,</em><em> The model must ensure a comprehensive understanding and ac-</em><em>curate responses.</em></p></div>
<div class="para"><p>We can also paraphrase it as a shorter text.</p></div>
<div class="para"><p><em>The task involves developing a language model focused on healthcare and</em> <em>finance, capable of understanding and accurately responding to a wide range</em> <em>of user inquiries.</em></p></div>
<div class="para"><p>This problem can be viewed as a classic NLP issue — text simplification. So the methods used can be general and not restricted to the problem of simplifying prompts. There are many ways to achieve this. One simple method is to define some heuristics and identify redundant words that can be eliminated without losing essential information. For example, we can examine each token in a sequence in terms of its contribution to the overall meaning and remove those that provide minimal value [Li et al., 2023c; Jiang et al., 2023b]. Another method involves framing the problem as a sequence-to-sequence task. With labeled data for text simplification, we can train an encoderdecoder model to transform each input text into its simplified form. In addition, given that many</p></div>
<div class="para"><p><em>3.4 Summary</em> 153</p></div>
<div class="para"><p>LLMs have been fine-tuned and aligned to perform text simplification tasks, it is straightforward to use these models to simplify prompts. For example, we can prompt an LLM to simplify a text under certain constraints, such as limiting the length of the simplified text.</p></div>
<h3>3.4 Summary</h3>
<div class="para"><p>In this chapter, we have discussed a variety of issues related to LLM prompting. Our discussion has focused mainly on two aspects:</p></div>
<div class="para"><p>• How to design basic prompts to guide the predictions of LLMs and refine these prompts for more effective and efficient problem-solving?</p></div>
<div class="para"><p>• How to automate the design and representation of prompts?</p></div>
<div class="para"><p>Solutions to these issues involve both general prompt designs and more advanced techniques, such as CoT and prompt learning, which have been explored extensively in recent research.</p></div>
<div class="para"><p>In NLP, prompting can be viewed as a technology that has evolved along with LLMs, and in a sense, it has opened the door to the practical application of these models in an impressive range of problem domains. In fact, if we expand the concept of prompts to some extent, it can be traced back to the early days of machine learning and NLP. For example, many NLP systems use hand-crafted features and templates to “prompt” specific tasks. Imagine developing a feature to indicate whether a text is formal or informal. We can feed this feature into a machine translation system to condition the translation on the type of the input text.</p></div>
<div class="para"><p>The widespread use of the modern concept of prompts began with the rise of large pre-trained models in the field of NLP. Initially, these models, such as BERT, were adapted to specific downstream tasks mainly through fine-tuning. However, researchers soon discovered that by designing specific &quot;prompts&quot; — adding certain words or sentences to the input — the models could be triggered to respond to specific tasks without extensive fine-tuning. This motivated the NLP community to develop and apply universal foundation models that can be prompted to address various tasks without changing the underlying architecture and the pre-training procedure.</p></div>
<div class="para"><p>Prompting approaches were first experimented with smaller models and later demonstrated impressive capabilities with large models like GPT-3, which could generate high-quality text in response to simple prompts across various tasks. As prompting technology evolved, prompt engineering emerged as a critical area of research. As discussed in this chapter, it broadly involves designing effective prompts to maximize model performance, encompassing both hand-crafted and automatically generated prompts. More recent research has explored how to enhance the effectiveness of prompting through techniques like few-shot learning, zero-shot learning, and CoT reasoning, enabling LLMs to work effectively across a wide range of scenarios. A general discussion of prompting can be very broad, and we cannot cover all details in this chapter. For more advanced techniques of prompting, the reader can refer to recent surveys. Topics include in-context learning [Li, 2023; Dong et al., 2022], CoT [Chu et al., 2023; Yu et al., 2023; Zhang et al., 2023a], efficient prompting [Chang et al., 2024], and general prompt engineering [Liu et al.,2023c; Chen et al., 2023a].</p></div>
<div class="para"><p>154 <em>Prompting</em></p></div>
<div class="para"><p>Note that although we would ideally like to develop general prompting methods without adjusting model architectures and parameters, the results of prompting generally depend heavily on the quality and size of the given LLMs. For stronger models, such as commercialized online LLMs, simple prompts may be sufficient to instruct these models to perform tasks correctly. In this case, prompt engineering is relatively easy, though we still need certain efforts to make LLMs work properly. By contrast, if the LLMs are not powerful enough, we may need to carefully design the prompts to achieve the desired results. In many cases, fine-tuning is still necessary to adapt the models to sophisticated prompting strategies.</p></div>
<div class="para"><p>https://github.com/NiuTrans/NLPBook</p></div>
<div class="para"><p>https://niutrans.github.io/NLPBook</p></div>
<h3>C HAPTER 4</h3>
<h1>Alignment</h1>
<div class="para"><p>Alignment is not a new concept in NLP, but its meaning varies across different domains and over time. In traditional NLP, the term<em> alignment</em> typically refers to the tasks that link corresponding elements in two sets, such as aligning words between a Chinese sentence and an English sentence.As LLMs become increasingly important in NLP research, this term is more broadly used to refer to aligning model outputs with human expectations. The problem that alignment addresses is that the output of a model may not align with the specific goals or contexts intended by users.For example, pre-trained LLMs may not be able to follow user instructions because they were not trained to do so. Another example is that LLMs may generate harmful content or perpetuate biases inherent in their training data. This poses new challenges in ensuring that LLM outputs are not only accurate and relevant, but also ethically sound and non-discriminatory.</p></div>
<div class="para"><p>Simply pre-training LLMs can result in a variety of alignment problems. Our ultimate goal is to resolve or mitigate all these problems to ensure LLMs are both accurate and safe. There is an interesting issue here: since large language models are trained on vast amounts of data,we have reason to believe that if we have sufficient data covering a variety of tasks and aligned with human preferences, pre-training could make LLMs accurate and safe enough, perhaps even eliminating the need for alignment. However, the reality is that it is nearly impossible to gather data that encompasses all tasks or adequately represents human preferences. This makes it difficult to achieve model alignment through pre-training alone, or at least, at this stage, alignment remains a very necessary and critical step in the development of LLMs.</p></div>
<div class="para"><p>In this chapter, we will focus on alignment methods for LLMs. We will begin by discussing the general alignment tasks. Then we will consider two widely-used approaches, known as<strong> instruc-</strong><strong>tion alignment</strong> and<strong> human preference alignment</strong>, respectively. The former resorts to supervised fine-tuning techniques and guides the LLMs to generate outputs that adhere closely to user instructions. On the other hand, the latter typically relies on reinforcement learning techniques, where the LLMs are trained based on feedback from humans. While these methods are motivated by different goals, they are commonly used together to develop well-aligned LLMs.</p></div>
<h3>4.1 An Overview of LLM Alignment</h3>
<div class="para"><p>Alignment can be achieved in several different ways. We need different methods for LLM alignment because this problem is itself complicated and multifaceted, requiring a blend of technical considerations. Here we consider three widely-used approaches to aligning LLMs.</p></div>
<div class="para"><p>The first approach is to fine-tune LLMs with labeled data. This approach is straightforward as it simply extends the pre-existing training of a pre-trained LLM to adapt it to specific tasks.An example of this is<strong> supervised fine-tuning</strong> (<strong>SFT</strong>), in which the LLM is further trained on a dataset comprising task-specific instructions paired with their expected outputs. The SFT dataset is generally much smaller compared to the original training set, but this data is highly specialized.The result of SFT is that the LLM can learn to execute tasks based on user instructions. For example, by fine-tuning the LLM with a set of question-answer pairs, the model can respond to specific questions, even if not directly covered in the SFT dataset. This method proves particularly</p></div>
<div class="para"><p>156 <em>Alignment</em></p></div>
<div class="para"><p><strong>Pre-training</strong></p></div>
<div class="para"><p><strong>Alignment</strong></p></div>
<div class="para"><p><strong>Stage</strong></p></div>
<div class="para"><p><strong>Stage</strong></p></div>
<div class="para"><p>Pre-training Prompting Instruction Alignment(e.g., SFT)</p></div>
<div class="para"><p>Human Preference Alignment(e.g., RLHF)</p></div>
<div class="para"><p>Training &amp; Fine-tuning Inference</p></div>
<div class="para"><p><strong>Fig. 4.1:</strong> Schematic illustration of the pre-train-then-align method for developing LLMs. In the pre-training stage, we train an LLM on vast amounts of data using next token prediction. Then, in the alignment stage, we align the LLM to user instructions, intents, and preferences. This includes instruction alignment, human preference alignment, and prompting.</p></div>
<div class="para"><p>useful when it is relatively easy to describe the input-output relationships and straightforward to annotate the data.</p></div>
<div class="para"><p>The second approach is to fine-tune LLMs using reward models. One difficulty in alignment is that human values and expectations are complex and hard to describe. In many cases, even for humans themselves, articulating what is ethically correct or culturally appropriate can be challenging. As a result, collecting or annotating fine-tuning data is not as straightforward as it is with SFT.Moreover, aligning LLMs is not just a task of fitting data, or in other words, the limited samples annotated by humans are often insufficient to comprehensively describe these behaviors. What we really need here is to teach the model how to determine which outputs are more in line with human preferences, for example, we not only want the outputs to be technically accurate but also to align with human expectations and values. One idea is to develop a reward model analogous to a human expert. This reward model would work by rewarding the LLM whenever it generates responses that align more closely with human preferences, much like how a teacher provides feedback to a student. To obtain such a reward model, we can train a scoring function from human preference data. The trained reward model is then used as a guide to adjust and refine the LLM. This frames the LLM alignment task as a reinforcement learning task. The resulting methods, such as<strong> rein-</strong><strong>forcement learning from human feedback</strong> (<strong>RLHF</strong>), have been demonstrated to be particularly successful in adapting LLMs to follow the subtleties of human behavior and social norms.</p></div>
<div class="para"><p>The third approach is to perform alignment during inference rather than during training or fine-tuning. From this perspective, prompting in LLMs can also be seen as a form of alignment,but it does not involve training or fine-tuning. So we can dynamically adapt an LLM to various tasks at minimal cost. Another method to do alignment at inference time is to rescore the outputs of an LLM. For example, we could develop a scoring system to simulate human feedback on the outputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.</p></div>
<div class="para"><p>The three methods mentioned above are typically used in sequence once the pre-training is complete: we first perform SFT, then RLHF, and then prompt the LLM in some way during inference. This roughly divides the development of LLMs into two stages — the pre-training stage and the alignment stage. Figure 4.1 shows an illustration of this. Since prompting techniques have been intensively discussed in the previous chapter, we will focus on fine-tuning-based alignment methods in the rest of this chapter.</p></div>
<div class="para"><p><em>4.2 Instruction Alignment</em> 157</p></div>
<h3>4.2 Instruction Alignment</h3>
<div class="para"><p>One feature of LLMs is that they can follow the prompts provided by users to perform various tasks. In many applications, a prompt consists of a simple instruction and user input, and we want the LLM to follow this instruction to perform the task correctly. This ability of LLMs is also called the instruction-following ability. For example, below is a prompt where we want the LLM to extract key points and provide a concise summary for a lengthy article.</p></div>
<div class="para"><p>Instruction Summarize this text in three sentences.</p></div>
<div class="para"><p>Input Daylight Savings Time (DST) - the process of moving clocks forward by one hour in the summer - was started in Germany in 1916 ...</p></div>
<div class="para"><p>Output</p></div>
<div class="para"><p>This task requires the LLM to understand the instruction “Summarize this text in three sentences”and perform the summarization accordingly. However, LLMs are typically trained for next-token prediction rather than for generating outputs that follow instructions. Applying a pre-trained LLM to the above example would likely result in the model continuing to write the input article instead of summarizing the main points. The goal of instruction alignment (or<strong> instruction fine-tuning</strong>) is to tune the LLM to accurately respond to user instructions and intentions. The rest of this section will discuss some issues related to instruction alignment, including fine-tuning LLMs to follow instructions, generating or collecting instruction data, and generalizing instruction alignment.</p></div>
<div class="para"><p><strong>4.2.1</strong> <strong>Supervised Fine-tuning</strong></p></div>
<div class="para"><p>One straightforward approach to adapting LLMs to follow instructions is to fine-tune these models using annotated input-output pairs [Ouyang et al., 2022; Wei et al., 2022a]. Unlike standard language model training, here we do not wish to maximize the probability of generating a complete sequence, but rather maximize the probability of generating the rest of the sequence given its prefix. This approach makes instruction fine-tuning a bit different from pre-training. The SFT data is a collection of such input-output pairs (denoted by<em> S</em>), where each output is the correct response for the corresponding input instruction. For example, below is an SFT dataset</p></div>
<div class="para"><p><strong>x</strong> (instruction + user input)<strong>y</strong> (output)Summarize the following article.<em>{∗</em>summary<em>∗}</em>Article: In recent years, solar energy has seen unprecedented growth, becoming the fastest-growing ...</p></div>
<div class="para"><p>Extract the main financial figures from the following Revenue: $10 million,earnings report.Profit Margin: 15%Report: The company reported a revenue of $10 million in the first quarter with a profit margin of 15% ...</p></div>
<div class="para"><p>Classify the following email as spam or not spam.Spam Text: Congratulations! You’ve won a $500 gift card.Click here to claim now.</p></div>
<div class="para"><p>Provide a solution to the following technical issue.First, check for ...Issue: my computer is running slow and often freezes.</p></div>
<div class="para"><p>158 <em>Alignment</em></p></div>
<div class="para"><p>where the instructions are highlighted. This dataset contains instructions and the corresponding outputs for several different NLP problems, and so we can fine-tune an LLM to handle multiple tasks simultaneously.</p></div>
<div class="para"><p>Let<strong> x</strong> =<em> x</em> 0<em>...x</em> <em>m</em> be an input sequence (e.g., instruction + user input) and<strong> y</strong> =<em> y</em> 1<em>...y</em> <em>n</em> be the corresponding output sequence. In SFT, we aim to maximize the probability of the output<strong> y</strong> given the input<strong> x</strong>. Consider an LLM with pre-trained parameters ˆ <em>θ</em>. The fine-tuning objective can then be formulated as:</p></div>
<div class="para"><p>˜<em>θ</em>=arg max ˆ <em>θ</em>+</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(<strong>x</strong><em>,</em><strong>y</strong>)<em>∈D</em> log Pr ˆ <em>θ</em>+(<strong>y</strong><em>|</em><strong>x</strong>)(4.1)</p></div>
<div class="para"><p>where ˜<em>θ</em> denotes the parameters optimized via fine-tuning, and ˆ <em>θ</em>+ represents an adjustment to ˆ <em>θ</em>.Here we will omit the superscript + and use<em> θ</em> to represent ˆ <em>θ</em>+ to keep the notation uncluttered. But the reader should keep in mind that the fine-tuning starts from the pre-trained parameters rather than randomly initialized parameters.</p></div>
<div class="para"><p>The objective function log Pr <em>θ</em>(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>) is computed by summing the log-probabilities of the tokens in<strong> y</strong>, conditional on the input<strong> x</strong> and all the previous tokens<strong> y</strong><em>&lt;i</em>:</p></div>
<div class="para"><p><em>n</em> X</p></div>
<div class="para"><p>log Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)=</p></div>
<div class="para"><p><em>i</em>=1 log Pr <em>θ</em>(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)(4.2)</p></div>
<div class="para"><p>This formulation is equivalent to minimizing the cross-entropy loss.</p></div>
<div class="para"><p>Note that minimizing the conditional log-probability log Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>) is not a standard language model training problem. If we concatenate<strong> x</strong> and<strong> y</strong> as a single sequence, a more general form of language modeling is based on the joint log-probability log Pr <em>θ</em>(<strong>x</strong><em>,</em><strong> y</strong>), that is, we minimize the loss over all tokens of the sequence seq <strong>x</strong><em>,</em><strong>y</strong> = [<strong>x</strong><em>,</em><strong> y</strong>]. We can write the probability of this sequence using the chain rule</p></div>
<div class="para"><p>log Pr <em>θ</em>(seq <strong>x</strong><em>,</em><strong>y</strong>)=log Pr <em>θ</em>(<strong>x</strong><em>,</em><strong> y</strong>)</p></div>
<div class="para"><p>+ log Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)|{z}loss computation</p></div>
<div class="para"><p>=log Pr <em>θ</em>(<strong>x</strong>)|{z}set to 0</p></div>
<div class="para"><p>(4.3)</p></div>
<div class="para"><p>There are two terms on the right-hand side of the equation. We can simply set the first term log Pr <em>θ</em>(<strong>x</strong>) to 0, focusing solely on the second term log Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>) for loss computation. As a result, the training can be implemented using standard LLMs. For the sequence seq <strong>x</strong><em>,</em><strong>y</strong>, we first run the forward pass as usual. Then, during the backward pass, we force the loss corresponding to<strong> x</strong> to be zero. Figure 4.2 shows an illustration of this process.</p></div>
<div class="para"><p>By taking log Pr <em>θ</em>(seq <strong>x</strong><em>,</em><strong>y</strong>) as the objective function, we can describe SFT using a regular form of language model training:</p></div>
<div class="para"><p>˜<em>θ</em>=arg max <em>θ</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(<strong>x</strong><em>,</em><strong>y</strong>)<em>∈D</em> log Pr <em>θ</em>(seq <strong>x</strong><em>,</em><strong>y</strong>)(4.4)</p></div>
<div class="para"><p>The problem we considered above is fundamentally a<strong> single-round prediction</strong> problem, where the LLM generates a response based on a single input without any further interaction or feedback from the user. The input is processed, and the output is generated in one go. This is typical in</p></div>
<div class="para"><p><em>4.2 Instruction Alignment</em> 159</p></div>
<div class="para"><p>Loss = 0 Loss<em> ̸</em>= 0</p></div>
<div class="para"><p>Input Output</p></div>
<div class="para"><p><em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>y</em> 1 <em>y</em> 2</p></div>
<div class="para"><p><em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>y</em> 1 <em>y</em> 2</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>y</em> 1</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1 <em>x</em> 2 <em>x</em> 3 <em>y</em> 1</p></div>
<div class="para"><p>(a) Forward Pass</p></div>
<div class="para"><p>(b) Backward Pass</p></div>
<div class="para"><p><strong>Fig. 4.2:</strong> Illustration of supervised fine-tuning for LLMs. We concatenate the input and the output into a single sequence. During the forward pass, we run the LLM as usual. During the backward pass, we compute the loss only for the output part and simply set the loss for the input part to 0.</p></div>
<div class="para"><p>scenarios where a single question is asked, and a single answer is provided, with no follow-up questions or clarifications. However, in practice, we sometimes have to handle multi-round prediction problems, for example, an LLM engages in a dialogue over multiple turns. In this setting,the LLM not only generates responses based on the initial input but also incorporates subsequent inputs that might refine or expand on earlier interactions. For example, we can use the LLM to act as a healthcare assistant chatbot and have a conversation with the user, like this</p></div>
<div class="para"><p>User I’ve been feeling very tired lately.</p></div>
<div class="para"><p>Chatbot I’m sorry to hear that. Besides feeling tired, have you noticed any other symptoms?</p></div>
<div class="para"><p>User Yes, I’m also experiencing headaches frequently.</p></div>
<div class="para"><p>Chatbot How long have these symptoms been going on?</p></div>
<div class="para"><p>User About a week now.</p></div>
<div class="para"><p>Chatbot It might be good to check in with a healthcare professional. Would you like help setting up an appointment?</p></div>
<div class="para"><p>User Yes, please. Can it be after work hours?</p></div>
<div class="para"><p>Chatbot Sure, I can arrange that. There are slots available next Wednesday</p></div>
<div class="para"><p>and Thursday after 5 PM. Which day works better for you?</p></div>
<div class="para"><p>...</p></div>
<div class="para"><p>In this task, there are several rounds of conversation, each involving the generation of a response based on the user’s request or question and the conversational history. Suppose we have <em>K</em> rounds of conversation, denoted by<em> {</em><strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>,</em><strong> x</strong> 2<em>,</em><strong> y</strong> 2<em>, ...,</em><strong> x</strong> <em>K</em><em>,</em><strong> y</strong> <em>K</em><em>}</em>. Here<strong> x</strong> <em>k</em> and<strong> y</strong> <em>k</em> denote the user request and the response, respectively, for each round<em> k</em>. The log-probability of generating the response can be written as log Pr <em>θ</em>(<strong>y</strong> <em>k</em><em>|</em><strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>, ...,</em><strong> x</strong> <em>k</em>). Our goal is then to maximize the sum of these log-probabilities</p></div>
<div class="para"><p><em>K</em> X</p></div>
<div class="para"><p>˜<em>θ</em>=arg max <em>θ</em></p></div>
<div class="para"><p><em>k</em>=1 log Pr <em>θ</em>(<strong>y</strong> <em>k</em><em>|</em><strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>, ...,</em><strong> x</strong> <em>k</em>)(4.5)</p></div>
<div class="para"><p>160 <em>Alignment</em></p></div>
<div class="para"><p>A straightforward implementation of this involves calculating the conditional probability for each<em> k</em>. However, it requires running the LLM<em> K</em> times, each time with an increased conversational history to make predictions. A more efficient method is to perform loss computation of all responses in a single run of the LLM. To do this, we represent the conversation as a sequence seq <strong>x</strong> 1<em>,</em><strong>y</strong> 1<em>,...,</em><strong>x</strong> <em>K</em><em>,</em><strong>y</strong> <em>K</em> = [<strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>, ...,</em><strong> x</strong> <em>K</em><em>,</em><strong> y</strong> <em>K</em>] (or seq for short). The log-probability of this sequence is given by</p></div>
<div class="para"><p>log Pr <em>θ</em>(seq)=log Pr <em>θ</em>(<strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>, ...,</em><strong> x</strong> <em>K</em><em>,</em><strong> y</strong> <em>K</em>)</p></div>
<div class="para"><p>=log Pr <em>θ</em>(<strong>x</strong> 1)|{z}set to 0</p></div>
<div class="para"><p>+ log Pr <em>θ</em>(<strong>y</strong> 1<em>|</em><strong>x</strong> 1)|{z}loss computation</p></div>
<div class="para"><p>+<em> · · ·</em> +</p></div>
<div class="para"><p>log Pr <em>θ</em>(<strong>x</strong> <em>K</em><em>|</em><strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>, ...,</em><strong> y</strong> <em>K</em><em>−</em>1)|{z}set to 0</p></div>
<div class="para"><p>+</p></div>
<div class="para"><p>log Pr <em>θ</em>(<strong>y</strong> <em>K</em><em>|</em><strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>, ...,</em><strong> x</strong> <em>K</em>)|{z}loss computation</p></div>
<div class="para"><p>(4.6)</p></div>
<div class="para"><p>The trick here is that we ignore the loss for generating user inputs, as illustrated in Figure 4.3.Hence we only compute the probabilities of generating the responses given their conversational histories, in other words, the value on the right-hand side of Eq. (4.6) is actually equal to the value on the right-hand side of Eq. (4.5). As with Eq. (4.4), the training of this multi-round prediction model can be achieved by maximizing the log likelihood over a training dataset<em> D</em>:</p></div>
<div class="para"><p>˜<em>θ</em>=arg max <em>θ</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>seq<em>∈D</em> log Pr <em>θ</em>(seq)(4.7)</p></div>
<div class="para"><p>While implementing the SFT methods introduced above seems trivial as they are fundamentally the same as regular language model training, there are still issues that need to be considered in practice. For example,</p></div>
<div class="para"><p>• SFT requires labeled data. This makes SFT quite different from pre-training, where raw text is used as training data and is readily available. As in other supervised machine learning problems, data annotation and selection in SFT are not simple tasks. In general, we wish to develop SFT data that is both substantial in quantity and high in quality, and this data should be highly relevant to the tasks the LLM will perform. On the other hand, there is a need to fine-tune LLMs with less data to minimize computational and data construction costs. Often, the quality of LLMs is highly dependent on the data used in SFT. Thus, such data must be carefully developed and examined. As we will see in later subsections, SFT can be more efficient and effective through more advanced techniques for data construction.</p></div>
<div class="para"><p>• SFT is still computationally expensive for LLMs due to their large size. As a result, maintaining and updating such models is resource-intensive. For example, applying gradient updates to billions of parameters within an LLM requires significant computational power and memory. This often requires high-performance computing environments, which are costly to operate. To address these challenges, various optimization strategies, such as pruning,quantization, and the use of more efficient training algorithms, have been explored. In particular, there has been significant interest in parameter-efficient fine-tuning methods which are designed to maintain state-of-the-art performance without the need for extensive computational resources. We have seen in Chapter 3 that applying techniques like soft prompts can</p></div>
<div class="para"><p><em>4.2 Instruction Alignment</em> 161</p></div>
<div class="para"><p>Loss = 0 Loss<em> ̸</em>= 0 Loss = 0 Loss<em> ̸</em>= 0</p></div>
<div class="para"><p>Pr <em>θ</em>(<strong>x</strong> 1)Pr <em>θ</em>(<strong>y</strong> 1<em>|</em><strong>x</strong> 1)Pr <em>θ</em>(<strong>x</strong> 2<em>|</em><strong>x</strong> 1<em>,</em><strong> y</strong> 1)Pr <em>θ</em>(<strong>y</strong> 2<em>|</em><strong>x</strong> 1<em>,</em><strong> y</strong> 1<em>,</em><strong> x</strong> 2)</p></div>
<div class="para"><p><strong>x</strong> 1 <strong>y</strong> 1 <strong>x</strong> 2 <strong>y</strong> 2<em>· · ·</em></p></div>
<div class="para"><p>User: I’ve been feeling very tired lately.</p></div>
<div class="para"><p>Chatbot: I’m sorry to hear that. Besides feeling tired,</p></div>
<div class="para"><p>have you noticed any other symptoms?</p></div>
<div class="para"><p>User: Yes, I’m also experiencing headaches frequently.</p></div>
<div class="para"><p>Chatbot: How long have these symptoms been going on?</p></div>
<div class="para"><p>...</p></div>
<div class="para"><p><strong>Fig. 4.3:</strong> Illustration of supervised fine-tuning for conversational models. Here the LLM acts as a chatbot to respond to each request based on the conversational history. The conversation progresses by alternating between the user and the chatbot. In SFT, we treat the entire conversation as a sequence, just like in standard LLMs, but compute the loss only for the responses of the LLM.</p></div>
<div class="para"><p>make the fine-tuning process more efficient. For further discussion on parameter-efficient methods, the reader can refer to related papers on this issue [Houlsby et al., 2019; Hu et al.,2022; Han et al., 2024].</p></div>
<div class="para"><p>• SFT can be regarded as a post-training step following pre-training. It is a separate training phase designed to preserve the advantages of the initial pre-training while incorporating new adjustments. This may seem paradoxical because updating a pre-trained LLM with further data potentially causes the model to forget some of its prior knowledge. Imagine a scenario where we have a large amount of SFT data and extensively fine-tune the LLM. In this case, the LLM could overfit the data, which in turn may reduce generalization performance or cause catastrophic forgetting. A common strategy to mitigate this issue is to employ regularization and early stopping techniques. Another practical approach is to use a smaller learning rate to gently adjust the weights of the LLM. In addition, fine-tuning with data from diverse sources and problem domains can also be beneficial. Nevertheless, in practice, the SFT step is often carefully examined and requires substantial engineering and experimental efforts to optimize.</p></div>
<h3>4.2.2 Fine-tuning Data Acquisition</h3>
<div class="para"><p>Fine-tuning data is so important that much recent work in LLM has focused on developing various datasets for instruction fine-tuning. As with most work in machine learning, there are generally two approaches to data acquisition — manual data generation and automatic data generation.</p></div>
<div class="para"><p>162 <em>Alignment</em></p></div>
<div class="para"><p><strong>4.2.2.1</strong> <strong>Manually Generated Data</strong></p></div>
<div class="para"><p>One straightforward method is to recruit human annotators to create input-output pairs for the tasks of interest. Unlike data annotation in conventional NLP, such as text classification, where annotators simply assign labels to collected texts according to guidelines, creating fine-tuning data for LLMs requires more steps and effort, making it thus more challenging. Suppose we want to obtain fine-tuning data for the English-to-Chinese machine translation task. The first step is to write a prompt template to describe the task and format the problem clearly. For example,</p></div>
<div class="para"><p>Instruction Translate the text from English to Chinese.</p></div>
<div class="para"><p>User Input<em>{∗</em>text<em>∗}</em></p></div>
<div class="para"><p>Output<em>{∗</em>translation<em>∗}</em></p></div>
<div class="para"><p>Then, we collect pairs of source and target texts (i.e., Chinese texts and the corresponding translations), and replace the variables<em> {∗</em>text<em>∗}</em> and<em> {∗</em>translation<em>∗}</em> to generate the fine-tuning samples. For example, given a pair of English and Chinese sentences</p></div>
<div class="para"><p>How’s the weather today?<em>→</em>今 天天 气 怎 么 样？<em>{∗</em>text<em>∗}</em><em>{∗</em>translation<em>∗}</em></p></div>
<div class="para"><p>we can generate a fine-tuning sample using the prompt template, like this</p></div>
<div class="para"><p>Instruction Translate the text from English to Chinese.</p></div>
<div class="para"><p>User Input How’s the weather today?</p></div>
<div class="para"><p>Output 今 天天 气 怎 么 样？</p></div>
<div class="para"><p>That is,</p></div>
<div class="para"><p><strong>x</strong>=Translate the text from English to Chinese.<em>\</em>n How’s the weather today?</p></div>
<div class="para"><p><strong>y</strong>=今 天天 气 怎 么 样？</p></div>
<div class="para"><p>We can use this (<strong>x</strong><em>,</em><strong> y</strong>) pair to fine-tune the LLM, as described in the previous subsection.</p></div>
<div class="para"><p>One difficulty here is that there are many, many different ways to write prompt templates for the same task, and different people may produce prompt templates with varying qualities and complexities. Sometimes, we may write prompt templates with overly complex or verbose instructions. Sometimes, we may not even know exactly what the target task is and how to describe it. A widely-adopted strategy is to create prompt templates for existing NLP tasks, given that there have been so many well-established NLP problems and benchmarks [Bach et al., 2022; Wang et al., 2022b; Mishra et al., 2022]. In this case, annotators can be given the original task description and many examples. Then, they can use their own ways to express how to prompt the LLM to perform the tasks. Note that, while such a method can ease the process of creating and writing prompts, we still need annotation frameworks and crowdsourcing systems to manage the work and conduct quality control. For example, we generally need to design annotation guidelines and</p></div>
<div class="para"><p><em>4.2 Instruction Alignment</em> 163</p></div>
<div class="para"><p>a unified format for writing prompt templates, especially when many annotators are contributing to the same task. One advantage of inducing prompts from existing NLP tasks is that, once the prompt templates have been developed, it is easy to generate prompts using the annotated samples in the original tasks. For example, given a bilingual dataset for English-to-Chinese translation, we can easily create a number of fine-tuning examples by filling the slots in the above template with the sentence pairs in this dataset.</p></div>
<div class="para"><p>Another approach is to directly use the naturally existing data available on the internet. A common example is by collecting question-and-answer pairs from QA websites to fine-tune LLMs for open-domain QA tasks [Joshi et al., 2017]. Many benchmarks in QA are built in this way because there are so many types of questions that it is impossible to think of them all by a small group of people. Instead, using data from those websites can ensure that the LLM fine-tuning data is at a good or acceptable level in terms of quantity and quality.</p></div>
<div class="para"><p>In addition to employing existing resources, another straightforward way to develop a finetuning dataset is to crowdsource the data. A simple approach is to allow users to input any question, after which responses are either manually given or automatically generated by an LLM and then manually annotated and corrected. It is thus possible to capture real user behavior and consequently gather inputs and outputs for a large number of “new” problems that traditional NLP tasks do not cover.</p></div>
<div class="para"><p>An issue related to the construction of the fine-tuning datasets is that we usually want the data to be as diverse as possible. Many studies have found that increasing the diversity of finetuning data can improve the robustness and generalization ability of LLMs. For this reason, there has been considerable interest in involving more diverse prompts and tasks in LLM fine-tunining datasets. We will provide further discussion on the generalization of fine-tuning in Section 4.2.4.</p></div>
<div class="para"><p><strong>4.2.2.2</strong> <strong>Automatically Generated Data</strong></p></div>
<div class="para"><p>One limitation of manual data generation is that the quality and diversity largely depend on human experience and creativity. Therefore, if we want LLMs to handle a broad range of tasks, that is, to effectively execute any instruction, relying on human-annotated data for LLM fine-tuning is often inefficient. Moreover, the coverage of such data can be limited, and the data may even contain biases introduced by the annotators themselves. An alternative approach is to generate data automatically. For example, we can collect a number of questions through crowdsourcing, and employ a well-tuned LLM to generate answers to the questions. These question-answer pairs are then used as fine-tuning samples as usual. This method, though very simple, has been extensively applied to generate large-scale fine-tuning data for LLMs.</p></div>
<div class="para"><p>The above way of producing synthetic fine-tuning data is similar to those used in data augmentation for NLP. If we have an LLM, we can produce a prediction in response to any input.Repeating this process for different inputs allows us to create a sufficient number of fine-tuning samples. Such a method is particularly useful for fine-tuning new LLMs using a well-tuned LLM.However, one disadvantage of this approach is that it relies on human-crafted or collected inputs for data generation, which may turn out to be inappropriate for generalizing LLMs. In many LLM applications, a significant challenge arises from the broad range of users’ questions and requests,many of which are not covered in existing NLP tasks and datasets. In these cases, it becomes necessary to generate not only the predictions but also the inputs themselves.</p></div>
<div class="para"><p>Here we consider<strong> self-instruct</strong> as an example to illustrate how to generate LLM fine-tuning</p></div>
<h2>164 Alignment</h2>
<h3>Initialize the task pool with a number of instructions and corresponding input-output samples.</h3>
<h2>Initialization</h2>
<div class="para"><p>Sample 1:(Instruction<em>,</em> User-input<em>,</em> Output)</p></div>
<div class="para"><p>Task Pool</p></div>
<div class="para"><p>Sample 2:(Instruction<em>,</em> User-input<em>,</em> Output)</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<h3>Draw a few instructions from the pool</h3>
<h2>Sampling</h2>
<div class="para"><p>Instruction <em>a</em></p></div>
<div class="para"><p>Task Pool sampling</p></div>
<div class="para"><p>Instruction <em>b</em></p></div>
<div class="para"><p>Instruction <em>c</em></p></div>
<h3>Prompt the LLM to generate a new instruction based on the drawn instructions.</h3>
<h2>Instruction Generation</h2>
<div class="para"><p>You are provided several different instructions for performing some tasks.Please generate an instruction based on these.Task 1: Instruction <em>a</em> Task 2: Instruction <em>b</em> Task 3: Instruction <em>c</em> New Task: Instruction new</p></div>
<h3>Given the newly-generated instruction and a few input-output samples, generate a new sample.</h3>
<h2>Sample Generation</h2>
<div class="para"><p>You are provided with a set of input-output samples tasks,each composed of an instruction, a user input, and an output.Please generate a new sample based on these.Sample 1: Sample <em>a</em> Sample 2: Sample <em>b</em> New Sample: Instruction new User-input new Output new</p></div>
<h3>Filter out invalid and low-quality samples. Add the remaining samples into the pool.</h3>
<h2>Filtering</h2>
<div class="para"><p><strong>Fig. 4.4:</strong> Illustration of self-instruct [Wang et al., 2023b]. This method maintains a pool of instructions and corresponding input-output samples. Initially, the pool contains a number of hand-crafted instructions and samples. Each time,we draw a few instructions from the pool. An LLM is then prompted to generate new instructions and samples based on those drawn. Finally, the newly-generated instructions and samples are filtered and added to the pool.</p></div>
<h2>samples [Wang et al., 2023d; Honovich et al., 2023]. The idea is that we can prompt an LLM to create a new instruction by learning from other instructions. Given this instruction, the LLM can then fill in other fields (such as the user input) and produce the predictions. Figure 4.4 shows a schematic illustration of self-instruct. Here we give a brief outline of the key steps involved.</h2>
<h2>The self-instruct algorithm maintains a pool of tasks. Initially it contains a number of seed hand-crafted tasks, each with an instruction and input-output sample. As the algorithm proceeds, LLM-generated instructions and samples will be added to this pool.</h2>
<div class="para"><p><em>4.2 Instruction Alignment</em> 165</p></div>
<div class="para"><p>• At each step, a small number of instructions are drawn from the instruction pool. For example, we can randomly select a few human-written instructions and a few LLM-generated instructions to ensure diversity.</p></div>
<div class="para"><p>• The selected instructions are then used as demonstration examples. Thus, the LLM can in-context learn from these examples and produce a new instruction. Below is an example template for prompting the LLM.</p></div>
<div class="para"><p>You are provided several different instructions for performing some tasks.Please generate an instruction based on these.</p></div>
<div class="para"><p>Task 1:<em> {</em>instruction1<em>}</em></p></div>
<div class="para"><p>Task 2:<em> {</em>instruction2<em>}</em></p></div>
<div class="para"><p>Task 3:<em> {</em>instruction3<em>}</em></p></div>
<div class="para"><p>Task 4:<em> {</em>instruction4<em>}</em></p></div>
<div class="para"><p>New Task:</p></div>
<div class="para"><p>• Given the generated instruction, the LLM is then prompted to complete the sample by filling in the remaining input fields and generating the corresponding output. Below is a prompt template.</p></div>
<div class="para"><p>You are provided with a set of input-output samples, each composed of an instruction, a user input, and an output. Please generate a new sample based on these.</p></div>
<div class="para"><p>Sample 1:<em> {</em>instruction1<em>}</em></p></div>
<div class="para"><p>Input:<em> {</em>user-input1<em>}</em></p></div>
<div class="para"><p>Output:<em> {</em>output1<em>}</em></p></div>
<div class="para"><p>Sample 2:<em> {</em>instruction2<em>}</em></p></div>
<div class="para"><p>Input:<em> {</em>user-input2<em>}</em></p></div>
<div class="para"><p>Output:<em> {</em>output2<em>}</em></p></div>
<div class="para"><p>New Sample:<em> {</em>new-instruction<em>}</em></p></div>
<div class="para"><p>• This newly-generated sample is examined by some heuristic rules (such as filtering out samples or instructions that are similar to those already in the pool). If it passes, the sample and instruction are added to the pool.</p></div>
<div class="para"><p>This generation process can be repeated many times to obtain a sufficient number of finetuning samples. Note that, above, we just show simple prompt templates for generating instruction and fine-tuning samples. Of course, we can develop better templates to generate more diverse and accurate instruction and fine-tuning samples. For example, for certain tasks like text classification,</p></div>
<div class="para"><p>166 <em>Alignment</em></p></div>
<div class="para"><p>the LLM may tend to produce biased predictions, for example, most generated samples belong to a single class. In such cases, we can adjust the order of generation of different fields. More specifically, we can specify the output (i.e., the class) with some prior, and prompt the LLM to generate user input given both the instruction and the output. This method resembles<strong> input</strong> <strong>inversion</strong>, where the LLM generates the input based on the specified output [Longpre et al., 2023].</p></div>
<div class="para"><p>Using LLM-generated instructions and fine-tuning samples has been a common method for developing LLMs, especially given that manually developing such data is so expensive that most research groups cannot afford it. In several well-tuned LLMs, their fine-tuning datasets include a certain amount of synthetic data, which has proved useful [Ouyang et al., 2022; Taori et al.,2023; Chiang et al., 2023]. There have been further studies on synthetic data generation for LLM fine-tuning. For example, one can generate more diverse instructions by introducing evolutionary algorithms [Xu et al., 2024], or use synthetic data as supervision signals in a more advanced finetuning process [Chen et al., 2024b]. More recently, there has also been considerable interest in using synthetic data in the pre-training stage [Gunasekar et al., 2023; Allal et al., 2024].</p></div>
<div class="para"><p>In many applications, a real-world scenario is that, given a task, we can collect or annotate a relatively small amount of fine-tuning data, for example, we can recruit experts to create questions for QA tasks in a specific domain. But the quantity and diversity of this data are in general not sufficient. In this case, we can use self-instruct techniques to generate more diverse questionanswer pairs, and thus augment the fine-tuning data. This provides a way of bootstrapping the LLM starting from a seed set of fine-tuning samples. Note that using self-generated data is a common practice and has long been applied in NLP. For example, this approach has been successfully used in parsing and machine translation [Charniak, 1997; Sennrich et al., 2016].</p></div>
<div class="para"><p><strong>4.2.3</strong> <strong>Fine-tuning with Less Data</strong></p></div>
<div class="para"><p>With the increasing prominence of instruction fine-tuning, there has been a surge in demand for large-scale, high-quality fine-tuning data. For example, the FLAN fine-tuning dataset, which is compiled from 1,836 tasks, contains 15 million samples [Longpre et al., 2023]. Fine-tuning LLMs with such large datasets is typically a computationally expensive task, especially given that updating the large number of parameters in LLMs is resource-intensive. One approach for mitigating this issue is to explore efficient model training methods, for example, one can use parameter-efficient methods to update only a small portion of the model. However, many finetuning datasets contain a large amount of synthetic data, where errors and biases are still inevitable.</p></div>
<div class="para"><p>Another approach to efficient fine-tuning is to consider only the most relevant and impactful examples for fine-tuning. We can thus reduce the amount of data that needs to be processed while still maintaining the quality of the model updates. There are several methods to achieve this. For example, Zhou et al. [2023a] built an instruction-following dataset containing only 1,000 samples by carefully crafting the prompts and collecting samples from a variety of NLP tasks.They showed that the LLaMa 65B model fine-tuned with this dataset could be competitive with or even better than models with much more fine-tuning effort. This suggests that LLMs can be adapted to respond to diverse tasks without necessarily needing fine-tuning on all types of instruction-following data. Chen et al. [2024a] developed a system based on the GPT-3.5 model to assess the quality of each instruction-following sample. Therefore, they could select highquality samples from existing datasets, showing better fine-tuning performance with fewer finetuning samples. Researchers have also developed methods to either select or filter out data using</p></div>
<div class="para"><p><em>4.2 Instruction Alignment</em> 167</p></div>
<div class="para"><p>heuristics [Zhao et al., 2024; Ge et al., 2024], or to prioritize data that more significantly influences the fine-tuning process [Xia et al., 2024]. In fact, most of these methods can be seen as instances of larger families of data selection and filtering methods. And it is often the case that using higher quality (but maybe less) data is beneficial for training NLP models.</p></div>
<div class="para"><p>The discoveries in instruction fine-tuning somewhat differ from traditional views in NLP: the ability of models to handle complex problems can be activated with a small amount of annotated data, rather than requiring massive amounts of supervised data for extensive training. One possible explanation is that the ability of generating correct responses given instructions has been learned during pre-training, but such instruction-response mappings are not with high probabilities during inference. Fine-tuning can slightly adjust the models to get them to follow instructions, requiring significantly less training effort than pre-training. This is closely related to what is known as the<strong> superficial alignment hypothesis</strong>, which suggests that learning primarily occurs during pretraining, and the subsequent fine-tuning or alignment phase does not significantly contribute to the underlying knowledge base of an LLM [Zhou et al., 2023a]. Since the core abilities and knowledge of the model are already established from pre-training, effective fine-tuning for alignment with user needs can be achieved with relatively small training fine-tuning effort. This implies the possibility of fine-tuning LLMs with very little data. In another direction, it may not be necessary to restrict fine-tuning to paired instruction-response data. For example, Hewitt et al. [2024] found that instruction-following can be implicitly achieved by fine-tuning LLMs only on responses,without corresponding instructions.</p></div>
<div class="para"><p>A concept related to the discussion here is sample efficiency. A machine learning method is called<strong> sample efficient</strong> if it can learn effectively from a small number of training examples.In this sense, instruction fine-tuning is sample efficient compared with pre-training. From the perspective of machine learning, sample-efficient methods can be seen as efficient ways to sample the space of data, and are advantageous as they make optimal use of scarce data. Therefore,sampling-based learning techniques, such as many reinforcement learning algorithms, can benefit from these sample efficient approaches. For example, in human preference alignment, we can either efficiently sample preference data via reward models [Liu et al., 2024b] or improve the sampling efficiency in policy learning [Wang et al., 2024].</p></div>
<div class="para"><p><strong>4.2.4</strong> <strong>Instruction Generalization</strong></p></div>
<div class="para"><p>In many machine learning and NLP problems, training a model to generalize is a fundamental goal. For example, in text classification, we expect our model to correctly classify new texts that were not seen during training. However, generalization poses additional challenges in instruction fine-tuning. We expect instruction-fine-tuned LLMs to not only generate appropriate responses for different inputs within a task but also to accurately perform various tasks as described by different instructions. To illustrate this issue, consider an LLM Pr(<strong>y</strong><em>|</em><strong>c</strong><em>,</em><strong> z</strong>), where<strong> c</strong> is an instruction,<strong> z</strong> is a user input, and<strong> y</strong> is the corresponding model output (i.e., the response). Suppose that the performance of this model is evaluated in terms of a metric, written as Performance(Pr(<strong>y</strong><em>|</em><strong>c</strong><em>,</em><strong> z</strong>))or P(<strong>c</strong><em>,</em><strong> z</strong><em>,</em><strong> y</strong>) for short. Informally, when we say this model can generalize within a given task(indicated by the instruction<strong> c</strong><em>∗</em>), we mean that there may be a value<em> ϵ</em> such that the average performance on new inputs is above this value:</p></div>
<div class="para"><p>1<em>|Z|</em></p></div>
<div class="para"><p><strong>z</strong><em>′</em><em>∈Z</em> P(<strong>c</strong><em>∗</em><em>,</em><strong> z</strong><em>′</em><em>,</em><strong> y</strong><em>′</em>)<em> &gt; ϵ</em>(4.8)</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>168 <em>Alignment</em></p></div>
<div class="para"><p>where<em> Z</em> is the set of new inputs, and<strong> z</strong><em>′</em> and<strong> y</strong><em>′</em> are an input in this set and the corresponding output, respectively.</p></div>
<div class="para"><p>Likewise, we can say that this model can generalize across tasks if the average performance over all instruction-input pairs is above some<em> ϵ</em>:</p></div>
<div class="para"><p>1<em>|D|</em></p></div>
<div class="para"><p>(<strong>c</strong><em>′</em><em>,</em><strong>z</strong><em>′</em>)<em>∈D</em> P(<strong>c</strong><em>′</em><em>,</em><strong> z</strong><em>′</em><em>,</em><strong> y</strong><em>′</em>)<em> &gt; ϵ</em>(4.9)</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>where<em> D</em> is the set of new instruction-input pairs.</p></div>
<div class="para"><p>Here, we need to deal with variations in two dimensions: instruction and user input. This makes the generalization problem very complex, because, intuitively, a model needs to learn from a vast number of tasks and different input-output pairs associated with each task to achieve good generalization. As we have discussed several times in this book, achieving such generalization incurs much lower cost than pre-training. In general, fine-tuning LLMs with instruction-response data to some extent can lead to models yielding instruction following on new tasks. Nevertheless,it is typically believed that certain efforts are still needed to adapt LLMs to make them understand and execute instructions broadly.</p></div>
<div class="para"><p>One way to generalize instruction fine-tuning is to increase the diversity of the fine-tuning data. In earlier studies on instruction fine-tuning, researchers developed many datasets, covering a wide variety of NLP tasks and different instructions for each task [Wang et al., 2022b; Sanh et al., 2022; Longpre et al., 2023]. By transforming these tasks into a unified format, one can finetune an LLM with a sufficiently large number of samples, for example, there have been several instruction fine-tuning datasets that involve over 100 NLP tasks and 1M samples. However, these early datasets mostly focus on existing academic problems, but not those that users want to deal with in real-world applications. Much recent work has shifted focus to addressing new and more practical problems. For example, there has been considerable interest in constructing datasets that contain large and complicated demonstrations and responses from SOTA models to real user queries [Wang et al., 2023c; Teknium, 2023].</p></div>
<div class="para"><p>Perhaps the use of large and diverse fine-tuning datasets has its origins in attempts to scale LLMs in different dimensions. Indeed, scaling laws have been used broadly to motivate the development of a wide range of different instruction-fine-tuned LLMs. And it is reasonable to scale instruction fine-tuning to make an LLM follow broad instructions. From the perspective of LLM alignment, however, scaling instruction fine-tuning might not be efficient to achieve generalization.</p></div>
<div class="para"><p>One problem is that instruction fine-tuning relies on supervised learning that learns to generalize and perform tasks based on instruction-response mappings. However, such an approach does not capture subtle or complex human preferences (e.g., tone, style, or subjective quality) because these are hard to encode as explicit instruction-response data. Moreover, the generalization performance is bounded by the diversity and quality of the instruction-response dataset. Given these limitations, we would instead like to employ preference models as an additional fine-tuning step following instruction fine-tuning, so the LLMs can generalize further (see Section 4.3).</p></div>
<div class="para"><p>Another view is that some instruction-response mappings may already be learned during pretraining, and so the pre-trained LLMs have encoded such mappings. However, since we often do not know exactly what data is used in the pre-training, it is hard to judge whether we need to learn such mappings in the fine-tuning. A related question is whether out-of-distribution generalization</p></div>
<div class="para"><p><em>4.2 Instruction Alignment</em> 169</p></div>
<div class="para"><p>is primarily achieved during pre-training or fine-tuning. While directly answering this question is beyond the scope of this chapter, it has been shown that pre-training on large and diverse datasets is effective in improving out-of-distribution performance [Hendrycks et al., 2020; Radford et al.,2021; Gunasekar et al., 2023]. This raises an interesting problem: if an LLM has been well pretrained at scale, fine-tuning may not be as essential for out-of-distribution generalization, since the model may have already encountered sufficient distributional variation. This prompts researchers to fine-tune LLMs with modest effort or to explore new methods to achieve instruction-following.As discussed in the previous sub-section, for example, instruction following can be yielded by fine-tuning on a small number of carefully selected instruction-response pairs [Zhou et al., 2023a],or even by using methods that are not explicitly designed to do so [Kung and Peng, 2023].</p></div>
<div class="para"><p>The above discussion provides two different strategies: one requires scaling up fine-tuning datasets for larger diversity, the other requires small but necessary fine-tuning datasets for efficient LLM adaptation. However, in practice, involving diverse instructions often helps. In many cases,we need to adapt our LLM for specific purposes. But the LLM, which has possibly encoded broad instruction-following mappings during pre-training, might tend to behave as a general-purpose instruction executor even with modest fine-tuning. An interesting phenomenon is that when finetuning on math data, the resulting LLM might not specialize in math outputs. Instead, this model might respond normally to general instructions, for example, it could generate poetry if instructed to do so [Hewitt, 2024]. This is not a bad thing, but it shows that LLMs may not easily change their nature of following general instructions. In this case, additional adaptations with more diverse data may help adjust the way the LLM follows instructions, particularly for those tasks we aim to address.</p></div>
<div class="para"><p><strong>4.2.5</strong> <strong>Using Weak Models to Improve Strong Models</strong></p></div>
<div class="para"><p>So far we have explored a variety of instruction fine-tuning methods based on labeled data. One of the limitations of many such methods is that they require the data to be annotated by humans or generated by strong LLMs, which can provide accurate supervision signals in fine-tuning. However, in many cases, the LLM we have in hand is already strong (or at least is advantageous in specific aspects of problem solving), and thus it is not easy to find a superior model for supervision. Even for human experts, when the problem becomes complex, providing correct and detailed answers might be difficult, or sometimes infeasible. For example, when faced with an extremely long document, the experts would find it challenging to identify any inconsistencies, subtle biases,or missing key points without conducting an exhaustive and time-consuming review.</p></div>
<div class="para"><p>One may ask at this point: can we use weak LLMs to supervise strong LLMs? This seems to be a significant challenge, but it may reflect a future scenario where we need to supervise AI systems that are smarter than humans or any other AI systems [Burns et al., 2023b]. The problem of using smaller, less complex models to improve the training of larger, more complex models is also called the<strong> weak-to-strong generalization</strong> problem. While there have not been mature approaches to weak-to-strong generalization, using smaller models to assist stronger models has indeed proven useful in several areas of LLMs.</p></div>
<div class="para"><p>For instruction fine-tuning, one of the simplest ways of applying weak LLMs is to use these models to generate synthetic fine-tuning data. Suppose we have a collection of inputs<em> X</em>, where each input includes an instruction and a user input if necessary. For each<strong> x</strong><em> ∈</em><em>X</em>, we use a weak LLM Pr <em>w</em>(<em>·</em>) to generate a prediction ˆ <strong>y</strong> = arg max <strong>y</strong> Pr <em>w</em>(<strong>y</strong><em>|</em><strong>x</strong>). Then, the strong LLM Pr <em>s</em> <em>θ</em>(<em>·</em>) can</p></div>
<div class="para"><p>170 <em>Alignment</em></p></div>
<div class="para"><p>be trained on these generated predictions (see Eq. (4.1)):</p></div>
<div class="para"><p>˜<em>θ</em>=arg max <em>θ</em></p></div>
<div class="para"><p><strong>x</strong><em>∈</em><em>X</em> log Pr <em>s</em> <em>θ</em>(ˆ <strong>y</strong><em>|</em><strong>x</strong>)(4.10)</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>where<em> θ</em> is the model parameters.</p></div>
<div class="para"><p>The above form transforms the fine-tuning problem into a knowledge distillation problem, in other words, we distill knowledge from the weak model to the strong model. Consequently, we can employ various knowledge distillation methods to achieve this goal. However, explaining weak-to-strong fine-tuning from the perspective of knowledge distillation is not straightforward.A major concern is that the strong model may merely imitate or overfit the errors of the weak model and fail to generalize. For example, the fine-tuned strong model still cannot solve difficult problems that the weak model cannot accurately predict. Fortunately, preliminary experiments in this line of research have shown positive and promising results. For example, Burns et al. [2023a]found that fine-tuning the strong pre-trained GPT-4 model with GPT-2-level supervision could improve generalization across several NLP tasks. To measure how the weak model improves the generalization of the strong model, we define the following terms:</p></div>
<div class="para"><p>•<strong> Weak Performance</strong> (P weak). This is the test-set performance of the weak model, which can be regarded as the baseline performance.</p></div>
<div class="para"><p>•<strong> Weak-to-strong Performance</strong>(P weak<em>→</em>strong). This is the test-set performance of the strong model that is fine-tuned with the weak model.</p></div>
<div class="para"><p>•<strong> Strong Ceiling Performance</strong> (P ceiling). This is the test-set performance of the strong model that is fine-tuned with ground truth data. For example, we fine-tune the strong model with human-annotated predictions and take the resulting model as a ceiling.</p></div>
<div class="para"><p>Then, the<strong> performance gap recovered</strong> (<strong>PGR</strong>) can be defined as</p></div>
<div class="para"><p>PGR=max n 0<em>,</em> P weak<em>→</em>strong<em> −</em>P weak</p></div>
<div class="para"><p>o</p></div>
<div class="para"><p>(4.11)</p></div>
<div class="para"><p>P ceiling<em> −</em>P weak</p></div>
<div class="para"><p>This metric measures how much of the performance gap between the ceiling model and the weak model can be recovered by the weak-to-strong model. A PGR of 1 indicates that the weakto-strong fine-tuning can completely closes the performance gap, whereas a PGR of 0 indicates no improvement. In Burns et al. [2023a]’s work, it is shown that PGR can be around 0<em>.</em>8 on 22 NLP classification tasks. It should be noted that, while the potential of weak-to-strong fine-tuning is promising, achieving substantial weak-to-strong generalization remains a challenging goal that needs further investigation [Aschenbrenner, 2024].</p></div>
<div class="para"><p>Fine-tuning LLMs with weak supervision is just one choice for using small models to improve large models. Although this section primarily focuses on fine-tuning LLMs, we also mention other methods here to give a more complete discussion (see Figure 4.5 for illustrations of these methods).</p></div>
<div class="para"><p>• Instead of using small models to generate synthetic data, it is also straightforward to incorporate knowledge distillation loss based on these models. For example, a simple loss</p></div>
<h3>4.2 Instruction Alignment 171</h3>
<div class="para"><p>KD Loss LM Loss</p></div>
<div class="para"><p>Small Model</p></div>
<div class="para"><p>Compute Loss &amp; Train</p></div>
<div class="para"><p>Compute Loss &amp; Train</p></div>
<div class="para"><p>Small Model</p></div>
<div class="para"><p><strong>y</strong></p></div>
<div class="para"><p>ˆ <strong>y</strong></p></div>
<div class="para"><p>Input Predict</p></div>
<h3>Large Model</h3>
<h3>Large Model</h3>
<h3>Dataset =⇒ x ˆ y</h3>
<h3>Dataset =⇒ x y</h3>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p>(a) Fine-tuning on data generated by a small model</p></div>
<div class="para"><p>(b) Fine-tuning with KD Loss from a small model</p></div>
<div class="para"><p>(weak-to-strong generalization)</p></div>
<div class="para"><p>(weak-to-strong generalization)</p></div>
<div class="para"><p>Dataset</p></div>
<div class="para"><p><strong>y</strong></p></div>
<div class="para"><p>Compute Loss &amp; Train</p></div>
<div class="para"><p>Combination Model</p></div>
<div class="para"><p><strong>y</strong></p></div>
<div class="para"><p>Small Model Data Selection</p></div>
<div class="para"><p>Small Model 2 Small Model 1 Small Model 3</p></div>
<h3>Large Model</h3>
<h3>Dataset =⇒ x y</h3>
<div class="para"><p><strong>x</strong> <strong>x</strong> <strong>x</strong></p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p>(d) Ensemble of multiple small models</p></div>
<div class="para"><p>(c) Data selection with a small model</p></div>
<div class="para"><p>If Step 1 is not satisfactory, go to Step 2</p></div>
<div class="para"><p>Step 2(expensive)Step 1(cheap)</p></div>
<div class="para"><p><strong>y</strong> 2</p></div>
<div class="para"><p><strong>y</strong> 1</p></div>
<h3>Large Model</h3>
<div class="para"><p>Small Model</p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p>(e) Cascading (at inference time)</p></div>
<div class="para"><p><strong>Fig. 4.5:</strong> Illustrations of using small models to improve large models in LLMs. One approach involves using smaller models for the fine-tuning or pre-training of larger models. This includes generating synthetic data (a), incorporating auxiliary loss (b), and selecting appropriate data (c). Another approach involves combining small models and large models. This includes learning a strong model by aggregating multiple small models (d), and cascading small models with large models (e).</p></div>
<h3>function that measures the difference between the small and large models can be defined as:</h3>
<h3>Loss kd = KL(Pr w(·|x) || Pr s θ(·|x)) (4.12)</h3>
<h3>Then, we can add this loss to the original loss of language modeling, and yield the following training objective</h3>
<h3>˜θ = arg max θ</h3>
<h3>(x,y)∈D log Pr s θ(y|x) −λ · Loss kd (4.13)</h3>
<div class="para"><p>X</p></div>
<div class="para"><p>172 <em>Alignment</em></p></div>
<div class="para"><p>where<em> D</em> is the set of input and output pairs, and<em> λ</em> is the coefficient of the interpolation. This method can be employed in either the pre-training or fine-tuning phase. We can adjust<em> λ</em> to control how much the small model influences the training. For example, we can gradually decrease<em> λ</em> to make the training rely more on the original language modeling loss as the large model becomes more capable.</p></div>
<div class="para"><p>• Another approach to involving small models in LLM pre-training and fine-tuning is to use them to do data selection or filtering. Given a sequence, we can compute the likelihood or cross-entropy using a small model. These quantities can then be used as criteria for selecting or filtering data. For example, sequences with low likelihood or high cross-entropy might be excluded from the training set, as they are less aligned with the small model’s learned distribution. Conversely, sequences with high likelihood or low cross-entropy can be prioritized, ensuring that the training focuses on more relevant or high-quality data.</p></div>
<div class="para"><p>• Ensemble learning is a simple and effective way to build a strong model by combining multiple weak models. Applying this technique to LLMs is straightforward. We can aggregate distributions predicted by multiple small models or specialized submodels, and derive the final prediction from the aggregated results. This aggregation can be done using methods such as majority voting, weighted averaging, or stacking.</p></div>
<div class="para"><p>• Small models can also be employed at inference time to improve overall efficiency. Suppose we have a large model that is slow but more accurate, and a small model that is fast but less accurate. In model cascading, the small model first processes the input data, quickly generating preliminary results. If these results meet certain pre-defined criteria, they can be directly used. However, if the initial results are not sufficiently good, the input is then passed to the larger, more accurate model to produce a better result. This approach significantly reduces computational costs and latency, as the small model can effectively handle many inputs without access to the large model.</p></div>
<h3>4.3 Human Preference Alignment: RLHF</h3>
<div class="para"><p>So far in this chapter, we have focused on fine-tuning LLMs using input-output paired labeled data.This approach allows us to adapt LLMs for instruction-following via supervised learning. In many applications, however, LLMs are required not only to follow instructions but also to act in ways that are more aligned with human values and preferences. Consider a scenario where a user asks an LLM how to hack into a computer system. If the LLM is not appropriately aligned, it may respond by providing details on how to perform this illegal activity. Instead, a more desirable response might be to advise the user against engaging in illegal activities and offer a general overview of the consequences of such actions. The difficulty in achieving this is that the ethical nuances and contextual considerations required for an LLM to respond appropriately in such scenarios are not always straightforward to encode into a fine-tuning dataset. What’s even more challenging is that,often, humans themselves cannot precisely express their own preferences.</p></div>
<div class="para"><p>In this section, we discuss an alternative LLM fine-tuning method, called reinforcement learning from human feedback or RLHF for short [Christiano et al., 2017; Stiennon et al., 2020]. The basic idea behind RLHF is that LLMs can learn from comparisons of model outputs using reward</p></div>
<div class="para"><p><em>4.3 Human Preference Alignment: RLHF</em> 173</p></div>
<div class="para"><p><strong>y</strong></p></div>
<div class="para"><p>Objective (MLE):</p></div>
<div class="para"><p>Gold-standard Predictions(<em>n</em> one-hot distributions)</p></div>
<div class="para"><p>max Pr(<strong>y</strong><em>|</em><strong>x</strong>)</p></div>
<h2>LLM</h2>
<div class="para"><p>Predicted Token Distributions(<em>n</em> token distributions)</p></div>
<div class="para"><p>where</p></div>
<div class="para"><p><strong>x</strong>: input <strong>y</strong>: gold-standard output</p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p>(a) Supervised fine-tuning (maximizing the prediction probability given the input)</p></div>
<div class="para"><p>Reward Model Human preference data train</p></div>
<div class="para"><p>Generate multiple outputs via sampling</p></div>
<div class="para"><p><strong>y</strong> 1 <strong>y</strong> 2</p></div>
<div class="para"><p>Objective (RL Loss Minimization):</p></div>
<div class="para"><p>Prediction<strong> y</strong> 1</p></div>
<div class="para"><p>min<em> L</em>(<strong>x</strong><em>,</em><em> {</em><strong>y</strong> 1<em>,</em><strong> y</strong> 2<em>}</em><em>,</em><em> r</em>)</p></div>
<h2>LLM</h2>
<div class="para"><p>Prediction<strong> y</strong> 2</p></div>
<div class="para"><p>where</p></div>
<div class="para"><p><em>L</em>(<em>·</em>): loss function <em>r</em>(<em>·</em>): reward model</p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p>(b) Reinforcement Learning from Human Feedback</p></div>
<div class="para"><p><strong>Fig. 4.6:</strong> Supervised fine-tuning vs. reinforcement learning from human feedback. In supervised fine-tuning, we optimize the LLM by maximizing the probability of the prediction given the input. In reinforcement learning from human feedback, we first train a reward model on human preference data (on each pair of predictions, evaluators are asked to choose which one they prefer). Then, we use this reward model to supervise the LLM during fine-tuning.</p></div>
<div class="para"><p>models (see Figure 4.6). To do this, we can recruit human experts who indicate their preferences between pairs of outputs generated by the LLM. This preference data is used to train a reward model that can predict the perceived quality of LLM outputs. Once trained, the reward model provides feedback by assigning scores to new outputs that the LLM generates in response to the inputs. The LLM uses these scores to update its parameters through reinforcement learning algorithms. In the rest of this section, we will first introduce the basic knowledge of reinforcement learning to facilitate the discussion, and then discuss methods for training reward models and aligning LLMs with these models.</p></div>
<h3>4.3.1 Basics of Reinforcement Learning</h3>
<div class="para"><p>We begin by looking at some basic concepts of reinforcement learning. Note that the notation used here slightly differs from that used in the previous sections and chapters because we want to make our description more consistent with those in the reinforcement learning literature. Nevertheless,we will show how this notation corresponds to the language modeling notation. The reader who is already familiar with reinforcement learning techniques may skip or skim this subsection.</p></div>
<div class="para"><p>A general reinforcement learning framework describes how an agent interacts with a dynamic environment. This interaction is modeled as a sequence of actions taken by the agent in response to the state of the environment. At each time step, the agent observes the current state, chooses an action based on its policy, performs the action, and then receives feedback from the environment in the form of a reward and a new state. This sequence of observe-act-receive feedback is repeated</p></div>
<div class="para"><p>174 <em>Alignment</em></p></div>
<div class="para"><p>until the agent achieves its goal.</p></div>
<div class="para"><p>A reinforcement learning system involves several components:</p></div>
<div class="para"><p>•<strong> Agent</strong>. This is the learner or decision-maker in reinforcement learning. In the context of LLMs, it can be seen as the LLM itself.</p></div>
<div class="para"><p>•<strong> Environment</strong>. This includes everything external to the agent with which the agent interacts.But the environment in LLMs is less about a physical or virtual space and more about the framework within which the agent (e.g., an LLM) receives feedback and learns.</p></div>
<div class="para"><p>•<strong> State</strong> (<em>s</em>). A state represents the current situation of the environment. Given a sequence of tokens for language modeling, a state at a time step can be viewed as the tokens we observed so far, that is, the context tokens we take to predict the next token. For example, we can define (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>) as the state when predicting the next token at the time step<em> t</em>.</p></div>
<div class="para"><p>•<strong> Action</strong> (<em>a</em>). Actions represent possible decisions the agent can make. We can see them as possible predicted tokens in the vocabulary.</p></div>
<div class="para"><p>•<strong> Reward</strong> (<em>R</em>). The reward is the feedback from the environment that evaluates the success of an action. For example,<em> r</em>(<em>s, a, s</em><em>′</em>) denotes the reward the agent receives for taking the action<em> a</em> at the state<em> s</em> and moving to the next state<em> s</em><em>′</em>. If the state-action sequence is given,we can denote the reward at the time step<em> t</em> as<em> r</em> <em>t</em> =<em> r</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1). Also note that if the decision-making process is deterministic, we can omit<em> s</em> <em>t</em>+1 because it can be determined by<em> s</em> <em>t</em> and<em> a</em> <em>t</em>. In such cases, we can use<em> r</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) as shorthand for<em> r</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1).</p></div>
<div class="para"><p>•<strong> Policy</strong> (<em>π</em>). For an LLM, a policy is defined as the probability distribution over the tokens that the LLM predicts, given the preceding context tokens. Formally, this can be expressed as</p></div>
<div class="para"><p><em>π</em>(<em>a</em><em>|</em><em>s</em>) = Pr(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)(4.14)</p></div>
<div class="para"><p>where<em> a</em> corresponds to the token<em> y</em> <em>t</em>, and<em> s</em> corresponds to the context (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>). Figure 4.7 illustrates how an LLM can be treated as a policy in the reinforcement learning framework.</p></div>
<div class="para"><p>•<strong> Value Function</strong> (<em>V</em> and<em> Q</em>). A<strong> state-value function</strong> (or value function, for short) assesses the expected discounted return (i.e., accumulated rewards) for an agent starting from a particular state<em> s</em> and following a specific policy<em> π</em>. It is defined as:</p></div>
<div class="para"><p><em>V</em> (<em>s</em>)=E h <em>r</em>(<em>s</em> 0<em>, a</em> 0<em>, s</em> 1) +<em> γr</em>(<em>s</em> 1<em>, a</em> 1<em>, s</em> 2) +<em> γ</em> 2 <em>r</em>(<em>s</em> 2<em>, a</em> 2<em>, s</em> 3) +<em> · · ·</em><em> s</em> 0 =<em> s, π</em> i</p></div>
<div class="para"><p>=E h <em>r</em> 0 +<em> γr</em> 1 +<em> γ</em> 2 <em>r</em> 2 +<em> · · ·</em><em> s</em> 0 =<em> s, π</em> i</p></div>
<div class="para"><p>=E h<em> </em><em>∞</em>X</p></div>
<div class="para"><p><em>t</em>=0 <em>γ</em> <em>t</em> <em>r</em> <em>t</em> <em> s</em> 0 =<em> s, π</em> i</p></div>
<div class="para"><p>(4.15)</p></div>
<div class="para"><p>where<em> γ</em><em> ∈</em>[0<em>,</em> 1] is the discount factor that adjusts the importance of future rewards,<em> s</em> 0 =<em> s</em> indicates that the agent starts with the state<em> s</em>, and the expectation E is performed over all possible trajectories (i.e., state-action sequences). Similarly, an<strong> action-value function</strong> (or</p></div>
<h3>4.3 Human Preference Alignment: RLHF 175</h3>
<div class="para"><p>Feedback</p></div>
<div class="para"><p>Value Functions <em>V</em> (<em> s</em> <em>t</em> ) and<em> Q</em>(<em> s</em> <em>t</em>,<em> a</em> <em>t</em> )</p></div>
<div class="para"><p>Action<em> a</em> <em>t</em></p></div>
<div class="para"><p><em>y</em> 1 <em>y</em> 2<em>...</em><em>y</em> <em>t</em></p></div>
<h3>Policy (LLM)</h3>
<div class="para"><p><em>x</em> 0 <em>x</em> 1<em>...</em><em>x</em> <em>m</em> <em>y</em> 1<em>...</em><em>y</em> <em>t</em><em>−</em>1</p></div>
<div class="para"><p>Reward Model <em>R</em>(<em> s</em> <em>t</em>,<em> a</em> <em>t</em> )</p></div>
<div class="para"><p>State<em> s</em> <em>t</em> (<strong>x</strong> and<strong> y</strong><em>&lt;t</em>)</p></div>
<div class="para"><p><strong>Fig. 4.7:</strong> LLM as policy in reinforcement learning. At each step<em> t</em>, the LLM predicts a token<em> y</em> <em>t</em> given the model input<strong> x</strong> and the previously-generated tokens<strong> y</strong><em>&lt;t</em>. This process can be framed as a reinforcement learning problem,where<em> y</em> <em>t</em> serves as the action, (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>) as the state, and the predicted distribution Pr(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>) as the policy. Once <em>y</em> <em>t</em> is predicted, the LLM inputs both (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>) and<em> y</em> <em>t</em> to the reward model, which evaluates how effectively the chosen token contributes to achieving the desired textual outcome. This evaluation generates reward scores which are used to compute the value functions<em> V</em> (<em>s</em> <em>t</em>) and<em> Q</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>). These functions then provide feedback to the LLM and guide the policy training.</p></div>
<h3>Q-value function) measures the expected return starting from a state s taking an action a and thereafter following a policy π, given by</h3>
<h3>Q(s, a) = E h ∞ X</h3>
<h3>t=0 γ t r t s 0 = s, a 0 = a, π i</h3>
<h3>(4.16)</h3>
<h3>where a 0 = a indicates that the action taken at the initial state is a.</h3>
<h3>The goal of reinforcement learning is to learn a policy that maximizes the cumulative re- ward (or return) the agent receives over the long run. Given a state-action sequence τ = {(s 1, a 1), ..., (s T , a T )}1, the cumulative reward over this sequence can be written as</h3>
<div class="para"><p><em>T</em> X</p></div>
<h3>R(τ) =</h3>
<h3>t=1 r t (4.17)</h3>
<h3>The expectation of this cumulative reward over a space of state-action sequences is given in the form</h3>
<h3>h R(τ) π θ i</h3>
<h3>J(θ) = E τ∼D</h3>
<h3>= X</h3>
<h3>τ∈D Pr θ(τ)R(τ)</h3>
<div class="para"><p><em>T</em> X</p></div>
<h3>= X</h3>
<h3>τ∈D Pr θ(τ)</h3>
<h3>t=1 r t (4.18)</h3>
<div class="para"><p>1 We assume the state-action sequence begins with<em> s</em> 1 and<em> a</em> 1, rather than<em> s</em> 0 and<em> a</em> 0, to align with the notation commonly used in this chapter, where the prediction<strong> y</strong> typically starts from<em> y</em> 1. Of course, it is also common to denote a state-action sequence as<em> {</em>(<em>s</em> 0<em>, a</em> 0)<em>, ...,</em> (<em>s</em> <em>T</em><em> , a</em> <em>T</em> )<em>}</em> or<em> {</em>(<em>s</em> 0<em>, a</em> 0)<em>, ...,</em> (<em>s</em> <em>T</em><em> −</em>1<em>, a</em> <em>T</em><em> −</em>1)<em>}</em> in the literature. But this variation in notation does not affect the discussion of the models presented here.</p></div>
<div class="para"><p>176 <em>Alignment</em></p></div>
<div class="para"><p>where<em> τ</em><em> ∼D</em> indicates that<em> τ</em> is drawn from the state-action sequence space<em> D</em>, and the subscript <em>θ</em> indicates the parameters of the policy.<em> J</em>(<em>θ</em>) is also called the<strong> performance function</strong>.</p></div>
<div class="para"><p>Then the training objective is to maximize<em> J</em>(<em>θ</em>):</p></div>
<div class="para"><p>˜<em>θ</em>=arg max <em>θ</em> <em>J</em>(<em>θ</em>)(4.19)</p></div>
<div class="para"><p>Now, we have a simple reinforcement learning approach: 1) we sample a number of stateaction sequences; then, 2) we evaluate each sequence using the performance function; then, 3) we update the model to maximize this performance function. If we take Eq. (4.18) and use gradient descent to optimize the policy, this approach would constitutes a form of policy gradient methods[Williams, 1992].</p></div>
<div class="para"><p>Note that in many NLP problems, such as machine translation, rewards are typically sparse.For instance, a reward is only received at the end of a complete sentence. This means that<em> r</em> <em>t</em> = 0 for all<em> t &lt; T</em>, and<em> r</em> <em>t</em> is non-zero only when<em> t</em> =<em> T</em>. Ideally, one might prefer feedback to be immediate and frequent (dense), and thus the training of the policy can be easier and more efficient. While several methods have been proposed to address sparse rewards, such as reward shaping, we will continue in our discussion to assume a sparse reward setup, where the reward is available only upon completing the prediction.</p></div>
<div class="para"><p>The model described in Eqs. (4.17-4.19) establishes a basic form of reinforcement learning,and many variants and improvements of this model have been developed. Before showing those more sophisticated models, let us take a moment to interpret the objective function<em> J</em>(<em>θ</em>) from the perspective of policy gradient. In gradient descent, we need to compute the gradient of<em> J</em>(<em>θ</em>) with respect to<em> θ</em>:</p></div>
<div class="para"><p><em>∂J</em>(<em>θ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=<em>∂</em>P <em>τ</em><em>∈D</em> Pr <em>θ</em>(<em>τ</em>)<em>R</em>(<em>τ</em>)</p></div>
<div class="para"><p><em>∂θ</em></p></div>
<div class="para"><p><em>∂</em>Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p>=X</p></div>
<div class="para"><p><em>∂θ</em> <em>R</em>(<em>τ</em>)</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p><em>τ</em><em>∈D</em> Pr <em>θ</em>(<em>τ</em>)<em>∂</em>Pr <em>θ</em>(<em>τ</em>)<em>/∂θ</em></p></div>
<div class="para"><p>=X</p></div>
<div class="para"><p>Pr <em>θ</em>(<em>τ</em>)<em>R</em>(<em>τ</em>)</p></div>
<div class="para"><p><em>τ</em><em>∈D</em> Pr <em>θ</em>(<em>τ</em>)<em>∂</em>log Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p>=X</p></div>
<div class="para"><p><em>∂θ</em> <em>R</em>(<em>τ</em>)(4.20)</p></div>
<div class="para"><p>In some cases, we will assume that every sequence in<em> D</em> is equally probable (i.e., Pr <em>θ</em>(<em>τ</em>) =1<em>/</em><em>|D|</em>). In this case we can simplify Eq. (4.20) and need only consider the terms<em> </em><em>∂</em>log Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p><em>∂θ</em> and <em>R</em>(<em>τ</em>):</p></div>
<div class="para"><p><em>∂J</em>(<em>θ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=1 <em>m</em></p></div>
<div class="para"><p><em>∂</em>log Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><em>∂θ</em> <em>R</em>(<em>τ</em>)(4.21)</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p>One advantage of this result is that<em> R</em>(<em>τ</em>) does not need to be differentiable, which means that we can use any type of reward function in reinforcement learning.</p></div>
<div class="para"><p>By treating the generation of the sequence<em> τ</em> as a Markov decision process, we can further</p></div>
<div class="para"><p><em>4.3 Human Preference Alignment: RLHF</em> 177</p></div>
<div class="para"><p>derive<em> </em><em>∂</em>log Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p><em>∂θ</em>, and obtain:</p></div>
<div class="para"><p><em>T</em> Y</p></div>
<div class="para"><p><em>∂</em>log Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=<em>∂</em><em>∂θ</em> log</p></div>
<div class="para"><p><em>t</em>=1 <em>π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>) Pr(<em>s</em> <em>t</em>+1<em>|</em><em>s</em> <em>t</em><em>, a</em> <em>t</em>)</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p>=<em>∂</em><em>∂θ</em></p></div>
<div class="para"><p>+<em> </em><em>∂</em></p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)|{z}policy</p></div>
<div class="para"><p><em>t</em>=1 log Pr(<em>s</em> <em>t</em>+1<em>|</em><em>s</em> <em>t</em><em>, a</em> <em>t</em>)|{z}dynamics</p></div>
<div class="para"><p>(4.22)</p></div>
<div class="para"><p><em>∂θ</em></p></div>
<div class="para"><p>where the gradient is decomposed into two parts: the policy gradient and the dynamics gradient.The policy component, log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>), determines the log-probability of taking action<em> a</em> <em>t</em> given state<em> s</em> <em>t</em>, and it is parameterized by<em> θ</em>. The dynamics component, log Pr(<em>s</em> <em>t</em>+1<em>|</em><em>s</em> <em>t</em><em>, a</em> <em>t</em>), represents the log-probability of transitioning to state<em> s</em> <em>t</em>+1 from state<em> s</em> <em>t</em> after taking action<em> a</em> <em>t</em>. In typical reinforcement learning settings, the dynamics are not directly influenced by the policy parameters <em>θ</em>, and thus, their derivatives are often zero. In this case, therefore, Eq. (4.22) can be simplified to:</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>∂</em>log Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=<em>∂</em><em>∂θ</em></p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)(4.23)</p></div>
<div class="para"><p>In other words, we only concentrate on optimizing the policy without concerning ourselves with the underlying dynamics.</p></div>
<div class="para"><p>Substituting Eq. (4.23) into Eq. (4.21), and expanding<em> R</em>(<em>τ</em>), we then obtain</p></div>
<div class="para"><p><em> </em><em>T</em> X</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>∂J</em>(<em>θ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=1<em>|D|</em></p></div>
<div class="para"><p><em>∂</em><em>∂θ</em></p></div>
<div class="para"><p><em>t</em>=1 <em>r</em> <em>t</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p>(4.24)</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p>While this policy gradient approach is straightforward, it suffers from the problem that the variance of the estimated gradients can be very high, making the learning process noisy and inefficient. One reason for this high variance problem is that rewards can vary greatly across different steps or scenarios. Imagine that in a sequence of action decisions, the reward model tends to assign small rewards to good actions (e.g.,<em> R</em> <em>t</em> = 2) and large penalties to poor actions (e.g.,<em> R</em> <em>t</em> =<em> −</em>50).Such varying reward scales for good and poor actions can result in a very low total reward for the entire sequence, even if it includes good actions.</p></div>
<div class="para"><p>One simple method for reducing the variance of the gradient is to set a baseline<em> b</em> and subtract it from P <em>T</em> <em>t</em>=1<em> </em><em>r</em> <em>t</em>, resulting in P <em>T</em> <em>t</em>=1<em> </em><em>r</em> <em>t</em><em> −</em><em>b</em>.2 Here, the baseline can be interpreted as a reference point. By centering the rewards around this baseline, we remove systematic biases in the reward signal, making the updates more stable and less sensitive to extreme fluctuations in individual rewards.</p></div>
<div class="para"><p>2 In fact, the use of a baseline<em> b</em> does not change the variance of the total rewards P <em>T</em> <em>t</em>=1<em> </em><em>r</em> <em>t</em>. However, it is important to note that while introducing a baseline does not alter the overall variance of the rewards, it helps reduce the variance of the gradient estimates. This is because subtracting the baseline from the total rewards effectively reduces fluctuations around their mean, which makes the gradient estimates more stable. In general, the operation P <em>T</em> <em>t</em>=1<em> </em><em>r</em> <em>t</em><em> −</em><em>b</em> centers the rewards around zero (e.g.,<em> b</em> is defined as the expected value of P <em>T</em> <em>t</em>=1<em> </em><em>r</em> <em>t</em>), which can lead to reduced variance in the product P <em>T</em> <em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)(P <em>T</em> <em>t</em>=1<em> </em><em>r</em> <em>t</em><em> −</em><em>b</em>).</p></div>
<div class="para"><p>178 <em>Alignment</em></p></div>
<div class="para"><p>This policy gradient model with a baseline can be given by</p></div>
<div class="para"><p><em> </em><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em> </em><em>T</em> X</p></div>
<div class="para"><p><em>∂J</em>(<em>θ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=1<em>|D|</em></p></div>
<div class="para"><p><em>∂</em><em>∂θ</em></p></div>
<div class="para"><p><em>t</em>=1 <em>r</em> <em>t</em><em> −</em><em>b</em></p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p>h<em> </em><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em>T</em> X</p></div>
<div class="para"><p>=1<em>|D|</em></p></div>
<div class="para"><p><em>∂</em><em>∂θ</em></p></div>
<div class="para"><p><em>k</em>=1 <em>r</em> <em>k</em><em> −</em><em>b</em>i</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em> </em><em>t</em><em>−</em>1 X</p></div>
<div class="para"><p>h<em> </em><em>T</em> X</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p>=1<em>|D|</em></p></div>
<div class="para"><p><em>∂</em><em>∂θ</em></p></div>
<div class="para"><p><em>k</em>=<em>t</em> <em>r</em> <em>k</em><em> −</em><em>b</em>i</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p><em>k</em>=1 <em>r</em> <em>k</em> +</p></div>
<div class="para"><p>(4.25)</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p>Here we write P <em>T</em> <em>k</em>=1<em> </em><em>r</em> <em>k</em> as the sum of two terms P <em>t</em><em>−</em>1 <em>k</em>=1<em> </em><em>r</em> <em>k</em> and P <em>T</em> <em>k</em>=<em>t</em><em> </em><em>r</em> <em>k</em> to distinguish between the rewards accrued before and after the action at time step<em> t</em>. Note that in Markov decision processes,the future is independent of the past given the present. Therefore, the action taken at time step<em> t</em> cannot influence the rewards received before<em> t</em>, or in other words, the rewards prior to<em> t</em> are already“fixed” by the time the action at<em> t</em> is chosen. The term P <em>t</em><em>−</em>1 <em>k</em>=1<em> </em><em>r</em> <em>k</em> does not contribute to the gradient and can be omitted, leading to a simplified version of Eq. (4.25)</p></div>
<div class="para"><p>h<em> </em><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em> </em><em>T</em> X</p></div>
<div class="para"><p><em>∂J</em>(<em>θ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=1<em>|D|</em></p></div>
<div class="para"><p><em>∂</em><em>∂θ</em></p></div>
<div class="para"><p><em>k</em>=<em>t</em> <em>r</em> <em>k</em><em> −</em><em>b</em>i</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(4.26)</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p>Also note that removing P <em>T</em> <em>k</em>=<em>t</em><em> </em><em>r</em> <em>k</em> can further reduce the variance of the gradient.</p></div>
<div class="para"><p>There are many ways to define the baseline<em> b</em>. Here we consider the value function of the state <em>s</em> <em>t</em>, that is, the estimated value of being in state<em> s</em> <em>t</em>:<em> V</em> (<em>s</em> <em>t</em>) = E(<em>r</em> <em>t</em> +<em> r</em> <em>t</em>+1 +<em> · · ·</em> +<em> r</em> <em>T</em> ). Hence we have</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>k</em>=<em>t</em> <em>r</em> <em>k</em><em> −</em><em>b</em></p></div>
<div class="para"><p><em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)=</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>k</em>=<em>t</em> <em>r</em> <em>k</em><em> −</em><em>V</em> (<em>s</em> <em>t</em>)(4.27)</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>where P <em>T</em> <em>k</em>=<em>t</em><em> </em><em>r</em> <em>k</em> represents the actual return received, and<em> V</em> (<em>s</em> <em>t</em>) represents the expected return.<em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) (or<em> A</em> <em>t</em> for short) is called the<strong> advantage</strong> at time step<em> t</em>, which quantifies the relative benefit of the action<em> a</em> <em>t</em> compared to the expected value of following the policy from the state<em> s</em> <em>t</em> onward.</p></div>
<div class="para"><p>By using the advantage function<em> A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>), the gradient of<em> J</em>(<em>θ</em>) can be written in the form</p></div>
<div class="para"><p><em> </em><em>T</em> X</p></div>
<div class="para"><p><em>∂J</em>(<em>θ</em>)</p></div>
<div class="para"><p><em>∂θ</em>=1<em>|D|</em></p></div>
<div class="para"><p><em>∂</em><em>∂θ</em></p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(4.28)</p></div>
<div class="para"><p><em>τ</em><em>∈D</em></p></div>
<div class="para"><p>This optimization objective corresponds to the<strong> advantage actor-critic</strong> (<strong>A2C</strong>) method in reinforcement learning [Mnih et al., 2016]. In this method, the actor aims at learning a policy. It updates the policy parameters using Eq. (4.28) to help focus more on actions that are likely to improve performance. The critic, on the other hand, updates its estimation of the value function,which is used to calculate the advantage function<em> A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>), thus serving as the evaluator of the</p></div>
<div class="para"><p><em>4.3 Human Preference Alignment: RLHF</em> 179</p></div>
<div class="para"><p>policy being learned by the actor.</p></div>
<div class="para"><p>In the A2C method,<em> A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) is typically expressed as the difference of the action-value function<em> Q</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) and the state-value function<em> V</em> (<em>s</em> <em>t</em>)</p></div>
<div class="para"><p><em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)=<em>Q</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)<em> −</em><em>V</em> (<em>s</em> <em>t</em>)(4.29)</p></div>
<div class="para"><p>At first glance, this model may seem challenging to develop because it requires two separate submodels to calculate<em> Q</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) and<em> V</em> (<em>s</em> <em>t</em>) respectively. Fortunately, considering that<em> Q</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) can be defined as the return<em> r</em> <em>t</em> +<em> V</em> (<em>s</em> <em>t</em>+1), we can rewrite Eq. (4.29) as</p></div>
<div class="para"><p><em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)=<em>r</em> <em>t</em> +<em> V</em> (<em>s</em> <em>t</em>+1)<em> −</em><em>V</em> (<em>s</em> <em>t</em>)(4.30)</p></div>
<div class="para"><p>or alternatively, introduce the discount factor<em> γ</em> to obtain a more general form</p></div>
<div class="para"><p><em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)=<em>r</em> <em>t</em> +<em> γV</em> (<em>s</em> <em>t</em>+1)<em> −</em><em>V</em> (<em>s</em> <em>t</em>)(4.31)</p></div>
<div class="para"><p><em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) =<em> r</em> <em>t</em> +<em> γV</em> (<em>s</em> <em>t</em>+1)<em> −</em><em>V</em> (<em>s</em> <em>t</em>) is also called the<strong> temporal difference</strong> (<strong>TD</strong>) error. What we need is to train a critic network for the value function<em> V</em> (<em>s</em> <em>t</em>), and then use it to compute the advantage function 3.</p></div>
<div class="para"><p>Up to this point, we have spent considerable space discussing the basics of reinforcement learning, especially on how to derive the optimization objective for the A2C method. However,reinforcement learning is a vast field, and many technical details cannot be covered here. The interested reader can refer to reinforcement learning books for more details [Sutton and Barto, 2018;Szepesvári, 2010]. Nevertheless, we now have the necessary knowledge to further discuss RLHF.In the subsequent subsections, we will return to the discussion on LLM alignment, demonstrating how to use the A2C method for aligning with human preferences.</p></div>
<div class="para"><p><strong>4.3.2</strong> <strong>Training Reward Models</strong></p></div>
<div class="para"><p>We have shown that reward models play a very important role in the general reinforcement learning framework and form the basis for computing value functions. We now consider the problem of training these reward models.</p></div>
<div class="para"><p>In RLHF, a reward model is a neural network that maps a pair of input and output token sequences to a scalar. Given an input<strong> x</strong> and an output<strong> y</strong>, the reward can be expressed as</p></div>
<div class="para"><p><em>r</em>=Reward(<strong>x</strong><em>,</em><strong> y</strong>)(4.33)</p></div>
<div class="para"><p>where Reward(<em>·</em>) is the reward model.<em> r</em> can be interpreted as a measure of how well the output<strong> y</strong> aligns with the desired behavior given the input<strong> x</strong>. As discussed in the previous subsection, both<strong> x</strong></p></div>
<div class="para"><p>3 The training loss for the value network (or critic network) in A2C is generally formulated as the mean squared error between the computed return<em> r</em> <em>t</em> +<em> γV</em> (<em>s</em> <em>t</em>+1) and the predicted state value<em> V</em> (<em>s</em> <em>t</em>). Suppose that the value network is parameterized by<em> ω</em>. The loss function is given by</p></div>
<div class="para"><p><em>L</em> <em>v</em>(<em>ω</em>)=1 <em>M</em></p></div>
<div class="para"><p>X  <em>r</em> <em>t</em> +<em> γV</em> <em>ω</em>(<em>s</em> <em>t</em>+1)<em> −</em><em>V</em> <em>ω</em>(<em>s</em> <em>t</em>)2(4.32)</p></div>
<div class="para"><p>where<em> M</em> is the number of training samples, for example, for a sequence of<em> T</em> tokens, we can set<em> M</em> =<em> T</em>.</p></div>
<div class="para"><p>180 <em>Alignment</em></p></div>
<div class="para"><p>Reward (Scalar)</p></div>
<div class="para"><p><strong>W</strong> <em>r</em> Linear Map</p></div>
<div class="para"><p><strong>h</strong> <em>x</em> 0 <strong>h</strong> <em>x</em> 1 <strong>h</strong> <em>x</em> 2<em>· · ·</em><strong>h</strong> <em>x</em> <em>m</em> <strong>h</strong> <em>y</em> 1 <strong>h</strong> <em>y</em> 2<em>· · ·</em><strong>h</strong> last</p></div>
<div class="para"><p>Representation</p></div>
<div class="para"><p>at Each Position</p></div>
<div class="para"><p>Transformer Decoder (LLM)</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1 <em>x</em> 2<em>· · ·</em><em>x</em> <em>m</em> <em>y</em> 1 <em>y</em> 2<em>· · ·</em><em>y</em> <em>n</em>(Last Token<em> ⟨</em>EOS<em>⟩</em>)</p></div>
<div class="para"><p><strong>Fig. 4.8:</strong> Architecture of the reward model based on Transformer. The main component of this model is still an LLM.We use the Transformer decoder as the sequence representation model. We extract the representation of the last position of the decoder as the representation of the entire sequence [<strong>x</strong><em>,</em><strong> y</strong>]. We then map this representation to a scalar through a linear transformation, which serves as the reward score for [<strong>x</strong><em>,</em><strong> y</strong>].</p></div>
<div class="para"><p>and<strong> y</strong> are assumed to complete texts. This means that the reward model evaluates the relationship between inputs and outputs that provide full semantic content. For example, when applying the reward model, it assigns a value of 0 (or another predetermined value) at each position<em> t</em> in the output sequence<strong> y</strong> =<em> y</em> 1<em>...y</em> <em>n</em>. Only at the final position, when<em> t</em> =<em> n</em>, does the reward model generate the actual reward score. To keep the notation uncluttered, we will use<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>) to denote the reward model Reward(<strong>x</strong><em>,</em><strong> y</strong>) from here on.</p></div>
<div class="para"><p>There are many ways to implement the reward model. One simple approach is to build the reward model based on a pre-trained LLM. More specifically, we can concatenate<strong> x</strong> and<strong> y</strong> to form a single token sequence seq <strong>x</strong><em>,</em><strong>y</strong> = [<strong>x</strong><em>,</em><strong> y</strong>]. We run a pre-trained LLM on this sequence, as usual,and at each position, we obtain a representation from the top-most Transformer layer. Then, we take the representation at the last position (denoted by<strong> h</strong> last) and map it to a scalar via linear transformation:</p></div>
<div class="para"><p><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)=<strong>h</strong> last <strong>W</strong> <em>r</em>(4.34)</p></div>
<div class="para"><p>where<strong> h</strong> last is a<em> d</em>-dimensional vector, and<strong> W</strong> <em>r</em> is a<em> d</em><em> ×</em> 1 linear mapping matrix. This architecture of the reward model is illustrated in Figure 4.8.</p></div>
<div class="para"><p>To train the reward model, the first step is to collect human feedback on a set of generated outputs. Given an input<strong> x</strong>, we use the LLM to produce multiple candidate outputs<em> {</em><strong>y</strong> 1<em>, ...,</em><strong> y</strong> <em>N</em><em>}</em>.Human feedback can be obtained in several ways:</p></div>
<div class="para"><p>•<strong> Pairwise Comparison</strong> (<strong>Pairwise Ranking</strong>). Given two different outputs, human experts select which one is better.</p></div>
<div class="para"><p>•<strong> Rating</strong>. Human experts provide a score or rating to each output. This score is often a continuous or discrete numerical value, such as a score on a scale (e.g., 1-5 stars, or 1-10 points). In some cases, the rating might be binary, indicating a “yes/no” or “positive/negative” preference.</p></div>
<div class="para"><p><em>4.3 Human Preference Alignment: RLHF</em> 181</p></div>
<div class="para"><p>•<strong> Listwise Ranking</strong>. Human experts are asked to rank or order the given set of possible outputs.</p></div>
<div class="para"><p>Here we consider pairwise comparison feedback as it is one of the simplest and most common forms of human feedback used in RLHF. In this setting, each time, two outputs (<strong>y</strong> <em>a</em><em>,</em><strong> y</strong> <em>b</em>) are randomly drawn from the candidate pool<em> {</em><strong>y</strong> 1<em>, ...,</em><strong> y</strong> <em>N</em><em>}</em>. Human experts are then presented with these pairs and asked to decide which output they prefer based on specific criteria, such as clarity,relevance, and accuracy. The human feedback can be encoded as a binary label,<strong> y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em> for a preference for<strong> y</strong> <em>a</em>, and<strong> y</strong> <em>b</em><em> ≻</em><strong>y</strong> <em>a</em> for a preference for<strong> y</strong> <em>b</em>.</p></div>
<div class="para"><p>One simple and widely used model for describing such pairwise comparisons is the<strong> Bradley-</strong><strong>Terry model</strong> [Bradley and Terry, 1952]. It is a probabilistic model that estimates the probability that one item is preferred over another. Adapting this model to the notation used here, we can write the probability that<strong> y</strong> <em>a</em> is preferred over<strong> y</strong> <em>b</em> in the form</p></div>
<div class="para"><p>Pr(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)=<em>e</em> <em>r</em>(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em>)</p></div>
<div class="para"><p><em>e</em> <em>r</em>(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em>) +<em> e</em> <em>r</em>(<strong>x</strong><em>,</em><strong>y</strong> <em>b</em>)</p></div>
<div class="para"><p>=<em>e</em> <em>r</em>(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em>)<em>−</em><em>r</em>(<strong>x</strong><em>,</em><strong>y</strong> <em>b</em>)</p></div>
<div class="para"><p><em>e</em> <em>r</em>(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em>)<em>−</em><em>r</em>(<strong>x</strong><em>,</em><strong>y</strong> <em>b</em>) + 1=Sigmoid(<em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>a</em>)<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>b</em>))(4.35)</p></div>
<div class="para"><p>When training the reward model, we want to maximize this preference probability. A loss function based on the Bradley-Terry model is given by</p></div>
<div class="para"><p><em>L</em> <em>r</em>(<em>ϕ</em>)=<em>−</em>E(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em><em>,</em><strong>y</strong> <em>b</em>)<em>∼D</em> <em>r</em> log Pr <em>ϕ</em>(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)(4.36)</p></div>
<div class="para"><p>where (<strong>x</strong><em>,</em><strong> y</strong> <em>a</em><em>,</em><strong> y</strong> <em>b</em>) is drawn from a human-annotated dataset<em> D</em> <em>r</em> consisting of preference pairs of outputs and their corresponding inputs.<em> ϕ</em> represents the parameters of the reward model, which includes both the parameters of the Transformer decoder and the linear mapping matrix<strong> W</strong> <em>r</em>. In practice, assuming (<strong>x</strong><em>,</em><strong> y</strong> <em>a</em><em>,</em><strong> y</strong> <em>b</em>) is uniformly sampled from<em> D</em> <em>r</em>, we can replace the expectation with a summation</p></div>
<div class="para"><p><em>L</em> <em>r</em>(<em>ϕ</em>)=<em>−</em>1</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em><em>,</em><strong>y</strong> <em>b</em>)<em>∈D</em> <em>r</em> log Pr <em>ϕ</em>(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)(4.37)</p></div>
<div class="para"><p><em>|D</em> <em>r</em><em>|</em></p></div>
<div class="para"><p>The goal of training the reward model is to find the optimal parameters ˆ <em>ϕ</em> that minimize this loss function, given by</p></div>
<div class="para"><p>ˆ <em>ϕ</em>=arg min <em>ϕ</em> <em>L</em> <em>r</em>(<em>ϕ</em>)(4.38)</p></div>
<div class="para"><p>Since the reward model itself is also an LLM, we can directly reuse the Transformer training procedure to optimize the reward model. The difference from training a standard LLM is that we only need to replace the cross-entropy loss with the pairwise comparison loss as described in Eq.(4.37). After the training of the reward model, we can apply the trained reward model<em> r</em> ˆ <em>ϕ</em>(<em>·</em>) to supervise the target LLM for alignment.</p></div>
<div class="para"><p>It is worth noting that although we train the reward model to perform pairwise ranking, we apply it to score each input-output pair independently during the alignment process. The pairwise</p></div>
<div class="para"><p>182 <em>Alignment</em></p></div>
<div class="para"><p>ranking objective ensures that the reward model is sensitive to subtle differences between outputs,but we rely on the continuous scores produced by the reward model to guide the optimization of the LLM. An advantage of this approach is that we can choose from or combine various ranking loss functions, and still apply the resulting reward models in the same way as we have done in this subsection. This consistency ensures a unified framework for aligning the LLM, regardless of the specific ranking loss used during reward model training.</p></div>
<div class="para"><p><strong>4.3.3</strong> <strong>Training LLMs</strong></p></div>
<div class="para"><p>Having obtained the reward model, we then train the policy (i.e., the LLM) via the A2C method.Recall from Section 4.3.1 that a state-action sequence or trajectory<em> τ</em> can be evaluated by the utility function</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)(4.39)</p></div>
<div class="para"><p><em>U</em>(<em>τ</em>;<em> θ</em>)=</p></div>
<div class="para"><p>where<em> A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>) is the advantage of taking the action<em> a</em> <em>t</em> given the state<em> s</em> <em>t</em>. An estimate of<em> A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)is defined as the TD error<em> r</em> <em>t</em> +<em> γV</em> (<em>s</em> <em>t</em>+1)<em> −</em><em>V</em> (<em>s</em> <em>t</em>), where the value function<em> V</em> (<em>s</em> <em>t</em>) is trained with the reward model.</p></div>
<div class="para"><p>Given this utility function, the A2C-based loss function can be written in the form</p></div>
<div class="para"><p><em>L</em>(<em>θ</em>)=<em>−</em>E <em>τ</em><em>∼D</em><em>U</em>(<em>τ</em>;<em> θ</em>)</p></div>
<div class="para"><p>=<em>−</em>E <em>τ</em><em>∼D</em><em> </em><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)(4.40)</p></div>
<div class="para"><p>where<em> D</em> is a space of state-action sequences. As usual, the goal of training the policy is to minimize this loss function</p></div>
<div class="para"><p>˜<em>θ</em>=arg min <em>θ</em> <em>L</em>(<em>θ</em>)(4.41)</p></div>
<div class="para"><p>If we map the problem back to the language modeling problem and adopt the notation from LLMs, the loss function can be written as:</p></div>
<div class="para"><p><em>L</em>(<em>θ</em>)=<em>−</em>E(<strong>x</strong><em>,</em><strong>y</strong>)<em>∼D</em><em>U</em>(<strong>x</strong><em>,</em><strong> y</strong>;<em> θ</em>)(4.42)</p></div>
<div class="para"><p>where</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)<em>A</em>(<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em><em>, y</em> <em>t</em>)(4.43)</p></div>
<div class="para"><p><em>U</em>(<strong>x</strong><em>,</em><strong> y</strong>;<em> θ</em>)=</p></div>
<div class="para"><p>Here<em> π</em> <em>θ</em>(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>) = Pr <em>θ</em>(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>) is the LLM parameterized by<em> θ</em>.</p></div>
<div class="para"><p>In general, we do not have a human annotated input-output dataset<em> D</em> in RLHF, but a dataset containing inputs only. The outputs, in this case, are typically the predictions made by the LLM.</p></div>
<h3>4.3 Human Preference Alignment: RLHF 183</h3>
<h3>The loss function is then defined as</h3>
<h3>L(θ) = −E x∼D E y∼π θ(·|x) U(x, y; θ)  (4.44)</h3>
<h3>where D denotes the input-only dataset, and y ∼π θ(·|x) denotes that the output y is sampled by the policy π θ(·|x).</h3>
<h3>The above formulation provides a basic form of the A2C method for LLMs. Improved versions of this model are more commonly used in RLHF. In the following discussion, we will still use the reinforcement learning notation to simplify the presentation and will get back the language modeling notation later.</h3>
<h3>One common improvement of policy gradient methods is to use importance sampling to refine the estimation of U(τ; θ). This can be written as</h3>
<div class="para"><p><em>T</em> X</p></div>
<h3>π θ(a t|s t) π θ ref(a t|s t)A(s t, a t) (4.45)</h3>
<h3>U(τ; θ) =</h3>
<div class="para"><p><em>t</em>=1</p></div>
<h3>Here we replace the log-probability log π θ(a t|s t) with the ratio π θ(a t|s t) π θ ref (a t|s t). θ ref denotes the pa- rameters of the previous policy (such as an initial model from which we start the training). So π θ(a t|s t) π θ ref (a t|s t), also called the ratio function, can be interpreted as the log-probability ratio between the current policy π θ and the previous policy π θ ref (call it the reference policy). By using the ratio function we reweight the observed rewards based on the likelihood of the actions under the current policy versus the reference policy. When π θ(a t|s t) π θ ref (a t|s t) &amp;gt; 1, the action a t is more favored by</h3>
<h3>the current policy compared to the reference policy. By contrast, when π θ(a t|s t) π θ ref (a t|s t) &amp;lt; 1, the action</h3>
<h3>a t is less favored by the current policy 4.</h3>
<div class="para"><p>4 Consider a more general case where we wish to evaluate the policy using its expected reward (also see Eq. (4.18))</p></div>
<div class="para"><p><em>J</em>(<em>θ</em>)=E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> h <em>R</em>(<em>τ</em>)i</p></div>
<div class="para"><p>(4.46)</p></div>
<div class="para"><p>Here<em> τ</em><em> ∼</em><em>π</em> <em>θ</em> means that the sequence<em> τ</em> is generated by the policy<em> π</em> <em>θ</em>. Alternatively, we can write<em> J</em>(<em>θ</em>) in another form</p></div>
<div class="para"><p><em>J</em>(<em>θ</em>)=E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> ref h Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p>Pr <em>θ</em> ref(<em>τ</em>)<em>R</em>(<em>τ</em>)i</p></div>
<div class="para"><p>(4.47)</p></div>
<div class="para"><p>It is not difficult to find that the right-hand sides of these equations are essentially the same since</p></div>
<div class="para"><p>E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> ref h Pr <em>θ</em>(<em>τ</em>)Pr <em>θ</em> ref (<em>τ</em>)<em>R</em>(<em>τ</em>)i= P</p></div>
<div class="para"><p><em>τ</em> Pr <em>θ</em>(<em>τ</em>)<em>R</em>(<em>τ</em>) = E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> h <em>R</em>(<em>τ</em>)i</p></div>
<div class="para"><p><em>τ</em> Pr <em>θ</em> ref(<em>τ</em>)Pr <em>θ</em>(<em>τ</em>)Pr <em>θ</em> ref (<em>τ</em>)<em>R</em>(<em>τ</em>) = P</p></div>
<div class="para"><p>Note that this equivalence holds only when the expectation is performed over the entire sequence space. In practice,however, we often only sample a relatively small number of sequences using a policy in policy learning. As a result,the sampling method itself matters. Eq. (4.47) offers an interesting manner to separate the sampling and reward computation processes: we first use a baseline policy (with<em> θ</em> ref) to sample a number of sequences, and then use the target policy (with<em> θ</em>) to compute the expected reward. In this way, we separate the policy used for collecting the data,and the policy used for computing the gradient. This approach avoids the need to directly sample from the policy we are evaluating, which can be beneficial in cases where generating sequences from the target policy is expensive or difficult.</p></div>
<div class="para"><p>In reinforcement learning, E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> ref h Pr <em>θ</em>(<em>τ</em>)Pr <em>θ</em> ref (<em>τ</em>)<em>R</em>(<em>τ</em>)i is often called a<strong> surrogate objective</strong>.</p></div>
<div class="para"><p>Eq. (4.47) can also be interpreted from a policy gradient perspective. For E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> ref h Pr <em>θ</em>(<em>τ</em>)Pr <em>θ</em> ref (<em>τ</em>)<em>R</em>(<em>τ</em>)i, the gradient at</p></div>
<div class="para"><p><em>θ</em> =<em> θ</em> ref is given by</p></div>
<div class="para"><p><em>∂</em><em>∂θ</em> E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> ref h Pr <em>θ</em>(<em>τ</em>)</p></div>
<div class="para"><p>Pr <em>θ</em> ref(<em>τ</em>)<em>R</em>(<em>τ</em>)i <em>θ</em>=<em>θ</em> ref=E <em>τ</em><em>∼</em><em>π</em> <em>θ</em> ref h<em>∂</em>Pr <em>θ</em>(<em>τ</em>)<em>|</em><em>θ</em>=<em>θ</em> ref</p></div>
<div class="para"><p><em>∂θ</em> <em>R</em>(<em>τ</em>)i</p></div>
<div class="para"><p>(4.48)</p></div>
<div class="para"><p>The right-hand side is a standard form used in policy gradient methods, meaning that we compute the direction of</p></div>
<div class="para"><p>184 <em>Alignment</em></p></div>
<div class="para"><p>A problem with the model presented in Eq. (4.47) (as well as in Eq. (4.39)) is that the variance in the gradient estimates is often high, making the learning process unstable. To mitigate this issue, techniques such as clipping are often employed to bound the importance weights and prevent large updates. A clipped version of the utility function (also called the clipped surrogate objective function) is given by</p></div>
<div class="para"><p><em>t</em>=1 Clip<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>A</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em>)(4.49)</p></div>
<div class="para"><p><em>U</em> clip(<em>τ</em>;<em> θ</em>)=</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p>Clip<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p>=min<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em>,</em> bound <em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em>,</em> 1<em> −</em><em>ϵ,</em> 1 +<em> ϵ</em></p></div>
<div class="para"><p>(4.50)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p>Here the function bound(<em> </em><em>π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref (<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em>,</em> 1<em> −</em><em>ϵ,</em> 1 +<em> ϵ</em>) constrains the ratio function to the range [1<em> −</em></p></div>
<div class="para"><p><em>ϵ,</em> 1 +<em> ϵ</em>].</p></div>
<div class="para"><p>A further improvement to the above model is to consider<strong> trust regions</strong> in optimization [Schulman et al., 2015]. In reinforcement learning, a large update to the policy can lead to instability,where the agent may start performing worse after an update. A reasonable idea is to optimize the model in the trust region, which refers to a region around the current parameter estimate where the model is well-behaved. One approach to incorporating trust regions is to impose a constraint on the size of the policy update, ensuring that the current policy does not deviate too significantly from the reference policy. This can be achieved by adding a penalty based on some form of divergence between the current and reference policies to the objective function. A simple form of such a penalty is given by the difference in the log-probability of the sequence<em> τ</em> under the current policy versus the reference policy:</p></div>
<div class="para"><p>Penalty=log<em> π</em> <em>θ</em>(<em>τ</em>)<em> −</em>log<em> π</em> <em>θ</em> ref(<em>τ</em>)(4.51)</p></div>
<div class="para"><p>In practice, this penalty can be approximated by considering only the policy probabilities and ignoring the dynamics. This gives</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em>(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)<em> −</em></p></div>
<div class="para"><p><em>t</em>=1 log<em> π</em> <em>θ</em> ref(<em>a</em> <em>t</em><em>|</em><em>s</em> <em>t</em>)(4.52)</p></div>
<div class="para"><p>Penalty=</p></div>
<div class="para"><p>By including this penalty in the optimization objective, we encourage the current policy to remain close to the reference policy, limiting very large updates that could destabilize learning.</p></div>
<div class="para"><p>We can incorporate this penalty into the clipped surrogate objective function, and obtain</p></div>
<div class="para"><p><em>U</em> ppo-clip(<em>τ</em>;<em> θ</em>)=<em>U</em> clip(<em>τ</em>;<em> θ</em>)<em> −</em><em>β</em> Penalty(4.53)</p></div>
<div class="para"><p>where<em> β</em> is the weight of the penalty. This training method is called<strong> proximal policy optimization</strong>(<strong>PPO</strong>), which is one of the most popular reinforcement learning methods used in LLMs and many other fields [Schulman et al., 2017].</p></div>
<div class="para"><p>Now we can write the objective of training LLMs in the form of PPO.</p></div>
<div class="para"><p>the parameter update at the point<em> θ</em> =<em> θ</em> ref on the optimization surface.</p></div>
<div class="para"><p><em>4.3 Human Preference Alignment: RLHF</em> 185</p></div>
<div class="para"><p><em>U</em>(<strong>x</strong><em>,</em><strong> y</strong>;<em> θ</em>)=<em>U</em> ppo-clip(<strong>x</strong><em>,</em><strong> y</strong>;<em> θ</em>)<em> −</em><em>β</em> Penalty(4.54)</p></div>
<div class="para"><p>where</p></div>
<div class="para"><p><em>t</em>=1 Clip<em> π</em> <em>θ</em>(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>A</em>(<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em><em>, y</em> <em>t</em>)(4.55)</p></div>
<div class="para"><p><em>U</em> ppo-clip(<strong>x</strong><em>,</em><strong> y</strong>;<em> θ</em>)=</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)</p></div>
<div class="para"><p>Penalty=log Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em>log Pr <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>T</em> X</p></div>
<div class="para"><p><em>t</em>=1 log Pr <em>θ</em>(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)<em> −</em></p></div>
<div class="para"><p><em>t</em>=1 log Pr <em>θ</em> ref(<em>y</em> <em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)(4.56)</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>Although the notation here appears a bit tedious, the idea of PPO is simple: we develop an objective by combining the clipped likelihood ratio of the target and reference policies with an advantage function, and then impose a penalty that ensures policy updates are not too large. The PPO-based RLHF is illustrated in Figure 4.9.</p></div>
<div class="para"><p>To summarize, implementing RLHF requires building four models, all based on the Transformer decoder architecture.</p></div>
<div class="para"><p>•<strong> Reward Model</strong> (<em>r</em> <em>ϕ</em>(<em>·</em>) where<em> ϕ</em> denotes the parameters). The reward model learns from human preference data to predict the reward for each pair of input and output token sequences. It is a Transformer decoder followed by a linear layer that maps a sequence (the concatenation of the input and output) to a real-valued reward score.</p></div>
<div class="para"><p>•<strong> Value Model</strong> or<strong> Value Function</strong> (<em>V</em> <em>ω</em>(<em>·</em>) where<em> ω</em> denotes the parameters). The value function receives reward scores from the reward model and is trained to predict the expected sum of rewards that can be obtained starting from a state. It is generally based on the same architecture as the reward model.</p></div>
<div class="para"><p>•<strong> Reference Model</strong> (<em>π</em> <em>θ</em> ref(<em>·</em>) = Pr <em>θ</em> ref(<em>·</em>) where<em> θ</em> ref denotes the parameters). The reference model is the baseline LLM that serves as a starting point for policy training. In RLHF, it represents the previous version of the model or a model trained without human feedback. It is used to perform sampling over the space of outputs and contribute to the loss computation for policy training.</p></div>
<div class="para"><p>•<strong> Target Model</strong> or<strong> Policy</strong> (<em>π</em> <em>θ</em>(<em>·</em>) = Pr <em>θ</em>(<em>·</em>) where<em> θ</em> denotes the parameters). This policy governs how the LLM decides the most appropriate next token given its context. It is trained under the supervision of both the reward model and the value model.</p></div>
<div class="para"><p>In practice, these models need to be trained in a certain order. First, we need to initialize them using some other models. For example, the reward model and the value model can be initialized with a pre-trained LLM, while the reference model and the target model can be initialized with a model that has been instruction fine-tuned. Note that, at this point, the reference model is ready for use and will not be further updated. Second, we need to collect human preference data and train the reward model on this data. Third, both the value model and the policy are trained simultaneously using the reward model. At each position in an output token sequence, we update the value model</p></div>
<h2>186 Alignment</h2>
<h2>Human preference data D r = {(x, y a, y b)}</h2>
<div class="para"><p>To Learn</p></div>
<h2>Minimizing the loss based on the Bradley-Terry model</h2>
<h2>Reward Model r ϕ(x, y)</h2>
<h2>Reward Model Training</h2>
<h2>min ϕ − 1 |D r| P</h2>
<div class="para"><p>(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em><em>,</em><strong>y</strong> <em>b</em>)<em>∈D</em> <em>r</em> log<em> σ</em>(<em>r</em> <em>ϕ</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>a</em>)<em> −</em><em>r</em> <em>ϕ</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>b</em>))</p></div>
<h2>To Learn Ref Model Pr θ old(y t|x, y&amp;lt;t)</h2>
<div class="para"><p>To Learn</p></div>
<div class="para"><p>Fixed</p></div>
<h2>LLM Policy Pr θ(y t|x, y&amp;lt;t)</h2>
<h2>Value Function V ω(x, y&amp;lt;t)</h2>
<div class="para"><p>Evaluate the state-action pair using the advantage function or the TD error (based on the reward model and the value function)</p></div>
<div class="para"><p><em>x</em> 1 <em>x</em> 2<em>· · ·</em><em>y</em> 1 <em>y</em> 2<em>· · ·</em><em>y</em> <em>t</em> Action<em> y</em> <em>t</em>(sampled with Pr <em>θ</em> old )</p></div>
<h2>LLM Policy</h2>
<div class="para"><p><em>x</em> 0 <em>x</em> 1<em>· · ·</em><em>x</em> <em>m</em> <em>y</em> 1<em>· · ·</em><em>y</em> <em>t</em><em>−</em>1</p></div>
<h2>Policy Training</h2>
<div class="para"><p>State (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)</p></div>
<h3>LLM Policy</h3>
<h3>Value Function</h3>
<div class="para"><p>Minimizing the clipped PPO loss with the penalty</p></div>
<div class="para"><p>Minimizing the MSE between the computed return and the predicted state value</p></div>
<h2>x∈D P T t=1  r t + γV ω(x, y&amp;lt;t+1) −V ω(x, y&amp;lt;t)2</h2>
<h2>x∈D,y∼Pr θ old (·|x) P T t=1 h Clip  Pr θ(y t|x,y&amp;lt;t) Pr θ old (y t|x,y&amp;lt;t)  A t−</h2>
<h2>1 M P</h2>
<h2>min θ −P</h2>
<div class="para"><p>min <em>ω</em></p></div>
<h2>β ·   log Pr θ(y t|x, y&amp;lt;t)−</h2>
<h2>log Pr θ old(y t|x, y&amp;lt;t)i</h2>
<div class="para"><p><em>∗</em><em>∗</em><em>r</em> <em>t</em> =<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>+1) denotes the reward received as step<em> t</em>.</p></div>
<div class="para"><p><em>∗∗</em><em>A</em> <em>t</em> denotes the advantage at step<em> t</em>, and can be defined as<em> r</em> <em>t</em> +<em> γV</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>+1)<em> −</em><em>V</em> <em>ω</em>(<strong>x</strong><em>,</em><strong> y</strong><em>&lt;t</em>)</p></div>
<h2>Input-only data D = {x}</h2>
<h3>Fig. 4.9: Illustration of RLHF. The first step is to collect human preference data and train the reward model using this data. Once the reward model is optimized, along with the reference model, we proceed to train both the policy and the value function. At each prediction step, we compute the sum of the PPO-based loss and update the parameters of the policy. This requires access to the reward model, the reference model, and the value function at hand. At the same time, we update the parameters of the value function by minimizing the MSE loss.</h3>
<h2>by minimizing the MSE error of value prediction, and the policy is updated by minimizing the PPO loss.</h2>
<div class="para"><p><em>4.4 Improved Human Preference Alignment</em> 187</p></div>
<h3>4.4 Improved Human Preference Alignment</h3>
<div class="para"><p>In the previous section, we reviewed the basic concepts of reinforcement learning and the general framework of RLHF. In this section, we will discuss some refinements of RLHF and alternative methods to achieve human preference alignment.</p></div>
<div class="para"><p><strong>4.4.1</strong> <strong>Better Reward Modeling</strong></p></div>
<div class="para"><p>In Section 4.3.2, we highlighted the task of learning from human preferences as well as the use of pairwise ranking loss for training reward models. Here we consider more methods for reward modeling. Our discussion will be relatively general, and since the reward model is widely used in many reinforcement learning problems, it will be easy for us to apply the methods discussed here to RLHF and related applications.</p></div>
<div class="para"><p><strong>4.4.1.1</strong> <strong>Supervision Signals</strong></p></div>
<div class="para"><p>The training of reward models can broadly be seen as a ranking problem, where the model learns to assign scores to outputs so that their order reflects the preferences indicated by humans. There are several methods to train a reward model from the perspective of ranking.</p></div>
<div class="para"><p>One approach is to extend pairwise ranking to listwise ranking. For each sample in a dataset,we can use the LLM to generate multiple outputs, and ask human experts to order these outputs.For example, given a set of four outputs<em> {</em><strong>y</strong> 1<em>,</em><strong> y</strong> 2<em>,</em><strong> y</strong> 3<em>,</em><strong> y</strong> 4<em>}</em>, one possible order of them can be <strong>y</strong> 2<em> ≻</em><strong>y</strong> 3<em> ≻</em><strong>y</strong> 1<em> ≻</em><strong>y</strong> 4. A very simple method to model the ordering of the list is to accumulate the pairwise comparison loss. For example, we can define the listwise loss by accumulating the loss over all pairs of outputs:</p></div>
<div class="para"><p><em>L</em> list=<em>−</em>E(<strong>x</strong><em>,Y</em> )<em>∼D</em> <em>r</em> h 1 <em>N</em>(<em>N</em><em> −</em>1)</p></div>
<div class="para"><p>log Pr(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)i</p></div>
<div class="para"><p>X</p></div>
<div class="para"><p>(4.57)</p></div>
<div class="para"><p><strong>y</strong> <em>a</em><em>∈</em><em>Y,</em><strong>y</strong> <em>b</em><em>∈</em><em>Y</em> <strong>y</strong> <em>a</em><em≯</em>=<strong>y</strong> <em>b</em></p></div>
<div class="para"><p>where<em> Y</em> is a list of outputs, and<em> N</em> is the number of outputs in the list. Pr(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>) can be defined using the Bradley-Terry model, that is, Pr(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>) = Sigmoid(<em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>a</em>)<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>b</em>)).Here we omit the<em> ϕ</em> superscript on the Pr(<em>·</em>) to keep the notation uncluttered.</p></div>
<div class="para"><p>An extension to the Bradley-Terry model for listwise ranking could involve a ranking mechanism that takes into account the entire list of outputs rather than just pairwise comparisons. One such model is the<strong> Plackett-Luce model</strong>, which generalizes the Bradley-Terry model to handle multiple items in a ranking [Plackett, 1975]. In the Plackett-Luce model, for each item in a list,we define a “worth” for this item that reflects its relative strength of being chosen over other items.For the reward modeling problem here, the worth of<strong> y</strong> in the list<em> Y</em> can be defined as</p></div>
<div class="para"><p><em>α</em>(<strong>y</strong>)=exp(<em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>))(4.58)</p></div>
<div class="para"><p>188 <em>Alignment</em></p></div>
<div class="para"><p>Then the probability of selecting<strong> y</strong> from<em> Y</em> is given by</p></div>
<div class="para"><p>Pr(<strong>y</strong> is selected<em>|</em><strong>x</strong><em>, Y</em> )=<em>α</em>(<strong>y</strong>)P <strong>y</strong><em>′</em><em>∈</em><em>Y</em><em> </em><em>α</em>(<strong>y</strong><em>′</em>)</p></div>
<div class="para"><p>=exp(<em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>))P <strong>y</strong><em>′</em><em>∈</em><em>Y</em> exp(<em>r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>′</em>))(4.59)</p></div>
<div class="para"><p>Suppose ˚<em>Y</em> is an ordered list<strong> y</strong> <em>j</em> 1<em> ≻</em><strong>y</strong> <em>j</em> 2<em> ≻· · · ≻</em><strong>y</strong> <em>j</em> <em>N</em> . The overall log-probability of this ordered list can be defined as the sum of the conditional log-probabilities at each stage of selection,given by</p></div>
<div class="para"><p>log Pr(˚<em>Y</em><em> |</em><strong>x</strong>)=log Pr(<strong>y</strong> <em>j</em> 1<em> ≻</em><strong>y</strong> <em>j</em> 2<em> ≻· · · ≻</em><strong>y</strong> <em>j</em> <em>N</em><em> |</em><strong>x</strong>)</p></div>
<div class="para"><p>=log Pr(<strong>y</strong> <em>j</em> 1<em>|</em><strong>x</strong><em>,</em><em> {</em><strong>y</strong> <em>j</em> 1<em>,</em><strong> y</strong> <em>j</em> 2<em>, ...,</em><strong> y</strong> <em>j</em> <em>N</em><em> }</em>) +</p></div>
<div class="para"><p>log Pr(<strong>y</strong> <em>j</em> 2<em>|</em><strong>x</strong><em>,</em><em> {</em><strong>y</strong> <em>j</em> 2<em>, ...,</em><strong> y</strong> <em>j</em> <em>N</em><em> }</em>) +</p></div>
<div class="para"><p><em>· · ·</em> +</p></div>
<div class="para"><p>log Pr(<strong>y</strong> <em>j</em> <em>N</em><em> |</em><strong>x</strong><em>,</em><em> {</em><strong>y</strong> <em>j</em> <em>N</em><em> }</em>)</p></div>
<div class="para"><p><em>N</em> X</p></div>
<div class="para"><p><em>k</em>=1 log Pr(<strong>y</strong> <em>j</em> <em>k</em><em>|</em><strong>x</strong><em>,</em> ˚<em>Y</em><em>≥</em><em>k</em>)(4.60)</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>where ˚<em>Y</em><em>≥</em><em>k</em> represents the subset of the list of outputs that remain unselected at the<em> k</em>-th stage, i.e.,˚<em>Y</em><em>≥</em><em>k</em> =<em> {</em><strong>y</strong> <em>j</em> <em>k</em><em>, ...,</em><strong> y</strong> <em>j</em> <em>N</em><em> }</em>. Given the log-probability log Pr(˚<em>Y</em><em> |</em><strong>x</strong>), we can define the loss function based on the Plackett-Luce model by</p></div>
<div class="para"><p><em>L</em> pl=<em>−</em>E(<strong>x</strong><em>,</em>˚<em>Y</em> )<em>∼D</em> <em>r</em> log Pr(˚<em>Y</em><em> |</em><strong>x</strong>)(4.61)</p></div>
<div class="para"><p>There are also many other pairwise and listwise methods for modeling rankings, such as RankNet [Burges et al., 2005] and ListNet [Cao et al., 2007]. All these methods can be categorized into a large family of learning-to-rank approaches, and most of them are applicable to the problem of modeling human preferences. However, discussing these methods is beyond the scope of this chapter. Interested readers can refer to books on this topic for more details [Liu, 2009; Li,2011].</p></div>
<div class="para"><p>In addition to pairwise and listwise ranking, using pointwise methods to train reward models offers an alternative way to capture human preferences. Unlike methods that focus on the relative rankings between different outputs, pointwise methods treat each output independently. For example, human experts might assign a score to an individual output, such as a rating on a five-point scale. The objective is to adjust the reward model so that its outputs align with these scores. A simple way to achieve pointwise training is through regression techniques where the reward of each output is treated as a target variable. Let<em> φ</em>(<strong>x</strong><em>,</em><strong> y</strong>) be the score assigned to<strong> y</strong> given<strong> x</strong> by humans. Pointwise reward models can be trained by minimizing a loss function, often based on mean squared error or other regression losses, between the predicted reward<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>) and the actual human feedback<em> φ</em>(<strong>x</strong><em>,</em><strong> y</strong>). For example, the loss function could be</p></div>
<div class="para"><p><em>L</em> point=<em>−</em>E<em>φ</em>(<strong>x</strong><em>,</em><strong> y</strong>)<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)2(4.62)</p></div>
<div class="para"><p>While pointwise methods are conceptually simpler and can directly guide the reward model to</p></div>
<div class="para"><p><em>4.4 Improved Human Preference Alignment</em> 189</p></div>
<div class="para"><p>predict scores, they might not always be the best choice in RLHF. A problem is that these methods may struggle with high variance in human feedback, especially when different experts provide inconsistent scores for similar outputs. Because they focus on fitting to absolute scores rather than relative differences, inconsistencies in scoring can lead to poor model performance. Moreover,fitting to specific scored outputs might discourage generalization, particularly given that training data is often very limited in RLHF. In contrast, methods that consider relative preferences can promote the learning of more generalized patterns of success and failure. Nevertheless, there are scenarios where pointwise methods might still be suitable. For example, in tasks where training data is abundant and the costs of obtaining accurate, consistent annotations are low, pointwise methods can prove effective.</p></div>
<div class="para"><p>In fact, to make the supervision signal for training the reward model more robust, we can also introduce additional regularization terms into training. For example, if we consider the first term <em>U</em> ppo-clip(<strong>x</strong><em>,</em><strong> y</strong>;<em> θ</em>) in Eq. (4.54) as a type of generalized reward, then the second term (i.e., the penalty term) can be viewed as a form of regularization for the reward model, except that here the goal is to train the policy rather than the reward model. Another example is that Eisenstein et al. [2023] develop a regularization term based on the squared sum of rewards, and add it to the pairwise comparison loss in RLHF:</p></div>
<div class="para"><p><em>L</em> reg=<em>L</em> pair + (<em>−</em>E(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em><em>,</em><strong>y</strong> <em>b</em>)<em>∼D</em> <em>r</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>a</em>) +<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>b</em>)2)</p></div>
<div class="para"><p>=<em>−</em>E(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em><em>,</em><strong>y</strong> <em>b</em>)<em>∼D</em> <em>r</em> log Pr <em>ϕ</em>(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p><em>−</em>E(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em><em>,</em><strong>y</strong> <em>b</em>)<em>∼D</em> <em>r</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>a</em>) +<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>b</em>)2(4.63)</p></div>
<div class="para"><p>Optimizing with this regularization term can help mitigate the underdetermination of reward models 5.</p></div>
<div class="para"><p><strong>4.4.1.2</strong> <strong>Sparse Rewards vs. Dense Rewards</strong></p></div>
<div class="para"><p>As discussed in Section 4.3, the rewards in RLHF are very sparse: they are observed only at the end of sequences, rather than continuously throughout the generation process. Dealing with sparse rewards has long been a concern in reinforcement learning, and has been one of the challenges in many practical applications. For example, in robotics, it often needs to shape the reward function to ease optimization rather than relying solely on end-of-sequence rewards. Various methods have been developed to address this issue. One common approach is reward shaping, where the original function is modified to include intermediate rewards, thereby providing more immediate feedback. Also, one can adopt curriculum learning to sequentially structure tasks in a way that the complexity gradually increases. This can help models to master simpler tasks first, which prepares them for more complex challenges as their skills develop. There are many such methods that can mitigate the impact of sparse rewards, such as Monte Carlo methods and intrinsic motivation. Most of these methods are general and the discussion of them can be found in the broader literature on reinforcement learning, such as Sutton and Barto [2018]’s book.</p></div>
<div class="para"><p>Although we do not discuss methods for mitigating sparse rewards in detail here, an interesting question arises: why are sparse rewards so successful in RLHF? Recall from Section 4.3.1 that the supervision signal received at each time step<em> t</em> is not the reward for the current action, but</p></div>
<div class="para"><p>5 A model is called underdetermined if there are multiple alternative sets of parameters that can achieve the same objective.</p></div>
<div class="para"><p>190 <em>Alignment</em></p></div>
<div class="para"><p>rather some form of the accumulated rewards from<em> t</em> until the last time step. Such supervision signals are dense over the sequence, because the reward obtained at the end of the sequence can be transferred back to that time step, regardless of which time step it is. In other words, the sparse rewards are transformed into the dense supervision signals. Furthermore, from the perspective of reward shaping, Ng et al. [1999] show that the reward at<em> t</em> can be defined as</p></div>
<div class="para"><p><em>r</em><em>′</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1)=<em>r</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1) +<em> f</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1)(4.64)</p></div>
<div class="para"><p>where<em> r</em><em>′</em>(<em>·</em>) is the transformed reward function,<em> r</em>(<em>·</em>) is the original reward function, and<em> f</em>(<em>·</em>) is the shaping reward function. To ensure the optimality of the policy under the transformed reward function, the shaping reward function can be given in the form</p></div>
<div class="para"><p><em>f</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1)=<em>γ</em> Φ(<em>s</em> <em>t</em>+1)<em> −</em>Φ(<em>s</em> <em>t</em>)(4.65)</p></div>
<div class="para"><p>where Φ(<em>s</em>) is called the potential value of the state<em> s</em>. If we define Φ(<em>s</em>) as the common value function as in Eq. (4.15) and substitute Eq. (4.65) into Eq. (4.64), we obtain</p></div>
<div class="para"><p><em>r</em><em>′</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1)=<em>r</em>(<em>s</em> <em>t</em><em>, a</em> <em>t</em><em>, s</em> <em>t</em>+1) +<em> γV</em> (<em>s</em> <em>t</em>+1)<em> −</em><em>V</em> (<em>s</em> <em>t</em>)(4.66)</p></div>
<div class="para"><p>It is interesting to see that this function is exactly the same as the advantage function used in PPO.This relates advantage-based methods to reward shaping: the advantage is essentially a shaped reward.</p></div>
<div class="para"><p>On the other hand, one of the reasons for adopting end-of-sequence rewards lies in the nature of the RLHF tasks. Unlike traditional reinforcement learning environments where the agent interacts with a dynamic environment, RLHF tasks often involve complex decision-making based on linguistic or other high-level cognitive processes. These processes do not lend themselves easily to frequent and meaningful intermediate rewards because the quality and appropriateness of the actions can only be fully evaluated after observing their impact in the larger context of the entire sequence or task. In this case, the reward signals based on human feedback, though very sparse, are typically very informative and accurate. Consequently, this sparsity, together with the high informativeness and accuracy of the human feedback, can make the learning both robust and efficient.</p></div>
<div class="para"><p><strong>4.4.1.3</strong> <strong>Fine-grained Rewards</strong></p></div>
<div class="para"><p>For many applications, our objective will be more complex than merely evaluating an entire text.For example, in sentiment analysis, we often do not just determine the sentiment of a text, but need to analyze the sentiment in more detail by associating it with specific aspects of a topic discussed in the text. Consider the sentence &quot;The camera of the phone is excellent, but the battery life is disappointing.&quot; In this example, we would need to separately analyze the sentiments expressed about the camera and the battery. Such analysis, known as aspect-based sentiment analysis, helps provide a finer-grained understanding of the customer review compared to general sentiment analysis.</p></div>
<div class="para"><p>For the problem of reward modeling, we often need to model different parts of a sequence as well. A simple and straightforward way to do this is to divide a sequence into different segments and then compute the reward for each segment [Wu et al., 2023b]. Suppose that an output token</p></div>
<div class="para"><p><em>4.4 Improved Human Preference Alignment</em> 191</p></div>
<div class="para"><p>sequence<strong> y</strong> can be divided into<em> n</em> <em>s</em> segments<em> {</em>¯<strong>y</strong> 1<em>, ...,</em> ¯<strong>y</strong> <em>n</em> <em>s</em><em>}</em> by some criterion. We can use the reward model to evaluate each of these segments. By taking<strong> x</strong>,<strong> y</strong> and ¯<strong>y</strong> <em>k</em> as input to the reward model, the reward score for the<em> k</em>-th segment is given by</p></div>
<div class="para"><p><em>r</em> <em>k</em>=<em>r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>,</em> ¯<strong>y</strong> <em>k</em>)(4.67)</p></div>
<div class="para"><p>Then the reward score for the entire output sequence is given by</p></div>
<div class="para"><p><em>n</em> <em>s</em> X</p></div>
<div class="para"><p><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)=</p></div>
<div class="para"><p><em>k</em>=1 <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>,</em> ¯<strong>y</strong> <em>k</em>)(4.68)</p></div>
<div class="para"><p>Here<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>) can be used to train the policy as usual.</p></div>
<div class="para"><p>A problem with this model is that training reward models at the segment level is not as straightforward as learning from human preferences on entire texts, as it is difficult to obtain segment-level human preference data. For rating-like problems (e.g., we rate a segment according to its level of misinformation), one simple approach is to assign a rating score to each segment and train the reward model using pointwise methods. For example, we can use a strong LLM to rate the sequences ¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em><em>−</em>1 and ¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em>, and obtain the scores<em> s</em>(¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em><em>−</em>1) and<em> s</em>(¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em>). We can then define the score of the segment ¯<strong>y</strong> <em>k</em> as the difference between<em> s</em>(¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em>) and<em> s</em>(¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em><em>−</em>1)</p></div>
<div class="para"><p><em>s</em>(¯<strong>y</strong> <em>k</em>)=<em>s</em>(¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em>)<em> −</em><em>s</em>(¯<strong>y</strong> 1<em>...</em>¯<strong>y</strong> <em>k</em><em>−</em>1)(4.69)</p></div>
<div class="para"><p>Using these segment-level scores, we can train the reward model with a regression loss function</p></div>
<div class="para"><p><em>L</em> rating=<em>−</em>E¯<strong>y</strong> <em>k</em><em>s</em>(¯<strong>y</strong> <em>k</em>)<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>,</em> ¯<strong>y</strong> <em>k</em>)2(4.70)</p></div>
<div class="para"><p>Sometimes, alignment can be treated as a classification problem, for example, we assess whether a segment has ethical issues. In this case, the segment can be labeled as ethical or unethical, either by humans or using additional classifiers. Given the label of the segment, we can train the reward model using some classification loss function. For example, suppose that <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>,</em> ¯<strong>y</strong> <em>k</em>) = 1 if the segment is classified as unethical, and<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>,</em> ¯<strong>y</strong> <em>k</em>) =<em> −</em>1 otherwise 6. The hinge loss of training binary classification models is given by</p></div>
<div class="para"><p><em>L</em> hinge=max(0<em>,</em> 1<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong><em>,</em> ¯<strong>y</strong> <em>k</em>)<em> ·</em> ˆ <em>r</em>)(4.71)</p></div>
<div class="para"><p>where ˆ <em>r</em><em> ∈{</em>1<em>,</em><em> −</em>1<em>}</em> denotes the ground truth label.</p></div>
<div class="para"><p>The remaining issue here is how to split<strong> y</strong> into segments. One approach is to define a fixedlength segmentation, where<strong> y</strong> is divided into equal-length chunks. However, this may not always be ideal, as the content of the sequence may not align well with fixed boundaries. An alternative approach is to segment<strong> y</strong> based on specific linguistic or semantic cues, such as sentence boundaries, topic shifts, or other meaningful structures in the text. Such a segmentation can be achieved by using linguistic segmentation systems or prompting LLMs to identify natural breaks in the sequence. Another approach is to use dynamic segmentation methods based on the complexity of</p></div>
<div class="para"><p>6 To allow the reward model to output categories, we can replace the linear layer described in Section 4.3.2 with a Softmax layer.</p></div>
<div class="para"><p>192 <em>Alignment</em></p></div>
<div class="para"><p>the sequence. For example, segments could be defined where there is a significant change in the reward score, which might correspond to shifts in the task being modeled.</p></div>
<div class="para"><p><strong>4.4.1.4</strong> <strong>Combination of Reward Models</strong></p></div>
<div class="para"><p>A reward model can be viewed as a proxy for the environment. Since the true environment is often too complex or unknown, developing a perfect proxy for the environment is generally not possible.As a result, over-aligning LLMs with this imperfect proxy might lead to decreased performance,known as the<strong> overoptimization problem</strong> [Stiennon et al., 2020; Gao et al., 2023a]7. We can also explain this through Goodhart’s law, which states:<em> when a measure becomes a target, it ceases to</em> <em>be a good measure</em> [Goodhart, 1984].</p></div>
<div class="para"><p>Addressing the overoptimization problem is not easy, and there is no mature solution yet. The ideal approach might be to develop an oracle reward model that can perfectly capture the true objectives of the task and prevent the agent from “tricking”. However, creating such a model is extremely difficult due to the complexity of the real-world environment, as well as the challenge of defining all the relevant factors that contribute to the desired outcome. Instead, a more practical approach is to combine multiple reward models, thereby alleviating the misalignment between the training objective and the true objective that arises from using a single, specific reward model[Coste et al., 2024].</p></div>
<div class="para"><p>Given a set of reward models, combining them is straightforward, and in some cases, we can simply treat this problem as an ensemble learning problem. A simple yet common approach is to average the outputs of these models to obtain a more precise reward estimation:</p></div>
<div class="para"><p><em>K</em> X</p></div>
<div class="para"><p><em>r</em> combine=1 <em>K</em></p></div>
<div class="para"><p><em>k</em>=1 <em>w</em> <em>k</em><em> ·</em><em> r</em> <em>k</em>(<strong>x</strong><em>,</em><strong> y</strong>)(4.72)</p></div>
<div class="para"><p>where<em> r</em> <em>k</em>(<em>·</em>) is the<em> k</em>-th reward model in the ensemble,<em> w</em> <em>k</em> is the weight of<em> r</em> <em>k</em>(<em>·</em>), and<em> K</em> is the number of reward models. This combined reward can then be used to supervise the training of a policy. In fact, there are many ways to combine different models, for example, one can make predictions using Bayesian model averaging or develop a fusion network to learn to combine the predictions from different models. Alternatively, one can frame this task as a multi-objective optimization problem, and use multiple reward models to train the policy simultaneously. These methods have been intensively discussed in the literature on optimization and machine learning[Miettinen, 1999; Bishop, 2006].</p></div>
<div class="para"><p>In addition to model combination methods, another important issue is how to collect or construct multiple different reward models. One of the simplest approaches is to employ ensemble learning techniques, such as developing diverse reward models from different subsets of a given dataset or from various data sources. For RLHF, it is also possible to construct reward models based on considerations of different aspects of alignment. For example, we can develop a reward model to evaluate the factual accuracy of the output and another reward model to evaluate the</p></div>
<div class="para"><p>7 This problem is also called<strong> reward hacking</strong> or<strong> reward gaming</strong> [Krakovna et al., 2020; Skalse et al., 2022; Pan et al., 2022], which refers to the phenomenon where the agent attempts to trick the reward model but fails to align its actions with the true intended objectives of the task. Imagine a student who is assigned homework and is rewarded with points or praise for completing it. The student might then find ways to finish the homework with minimal effort to maximize the reward, such as copying and pasting solutions from the internet or previous assignments, rather than solving the problems themselves.</p></div>
<div class="para"><p><em>4.4 Improved Human Preference Alignment</em> 193</p></div>
<div class="para"><p>Value Function</p></div>
<div class="para"><p>Preference Data</p></div>
<div class="para"><p>training with MLE Training with PPO</p></div>
<h3>y a ≻y b</h3>
<div class="para"><p>Reward Model</p></div>
<div class="para"><p>Policy</p></div>
<div class="para"><p>(a) RLHF (PPO)</p></div>
<div class="para"><p>Preference Data</p></div>
<div class="para"><p>Policy training with MLE</p></div>
<h3>y a ≻y b</h3>
<div class="para"><p>(b) DPO</p></div>
<div class="para"><p><strong>Fig. 4.10:</strong> Standard RLHF (PPO) vs. DPO. In RLHF, the human preference data is used to train a reward model, which is then employed in training the policy as well as the value function. In DPO, the use of human preference data is more direct, and the policy is trained on this data without the need for reward model training.</p></div>
<div class="para"><p>completeness of the output. These two models are complementary to each other, and can be combined to improve the overall evaluation of the output. Another approach is to employ different off-the-shelf LLMs as reward models. This approach is simple and practical, as there have been a lot of well-developed LLMs and we just need to use them with no or little modification. An interesting issue, though not closely related to the discussion here, arises: can an LLM that aligns with other LLMs outperform those LLMs? Probably not at first glance. In part, this is because the target LLM merely imitates other LLMs based on limited supervision and thus cannot capture well the nuances of the behaviors of these supervisors. However, given the strong generalization ability of LLMs, this approach can, in fact, be quite beneficial. For example, using open-sourced or commercial LLMs as reward models has demonstrated strong performance in aligning LLMs,even achieving state-of-the-art results on several popular tasks [Lambert et al., 2024].</p></div>
<div class="para"><p><strong>4.4.2</strong> <strong>Direct Preference Optimization</strong></p></div>
<div class="para"><p>Although learning reward models is a standard step in reinforcement learning, it makes the entire training process much more complex than supervised training. Training a reliable reward model is itself not an easy task, and a poorly trained reward model can greatly affect the outcome of policy learning. We now consider an alternative alignment method, called<strong> direct preference op-</strong><strong>timization</strong> (<strong>DPO</strong>), which simplifies the training framework by eliminating the need to explicitly model rewards [Rafailov et al., 2024]. This method directly optimizes the policy based on user preferences, rather than developing a separate reward model. As a result, we can achieve human preference alignment in a supervised learning-like fashion. Figure 4.10 shows a comparison of the standard RLHF method and the DPO method.</p></div>
<div class="para"><p>Before deriving the DPO objective, let us first review the objective of policy training used in RLHF. As discussed in Section 4.3.3, the policy is typically trained by optimizing a loss function</p></div>
<div class="para"><p>194 <em>Alignment</em></p></div>
<div class="para"><p>with a penalty term. The DPO method assumes a simple loss function where the quality of the output<strong> y</strong> given the input<strong> x</strong> is evaluated by the reward model<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>). The training objective is thus given by</p></div>
<div class="para"><p>˜<em>θ</em>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>)<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)|{z}loss</p></div>
<div class="para"><p>(4.73)</p></div>
<div class="para"><p>+<em>β</em> (log<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em>log<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>))|{z}penalty</p></div>
<div class="para"><p>Note that in this optimization problem, only the term<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>) depends on the target policy<em> π</em> <em>θ</em>(<em>·</em>).Both the reward model<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>) and the reference model<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) are assumed to be fixed given <strong>x</strong> and<strong> y</strong>. This is a strong assumption compared with PPO, but as will be shown later, it simplifies the problem and crucial for deriving the DPO objective.</p></div>
<div class="para"><p>Since<em> θ</em> is the variable we want to optimize, we rearrange the right-hand side of Eq. (4.73) to isolate<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>) as an independent term:</p></div>
<div class="para"><p>˜<em>θ</em>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>)<em>β</em> log<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em><em>β</em> log<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)</p></div>
<div class="para"><p>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>) log<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em>  log<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) + 1</p></div>
<div class="para"><p><em>β </em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)</p></div>
<div class="para"><p>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>) log<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)|{z}dependent on<em> θ</em></p></div>
<div class="para"><p><em>−</em>log<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) exp  1</p></div>
<div class="para"><p><em>β </em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)</p></div>
<div class="para"><p>(4.74)</p></div>
<div class="para"><p>|{z}not dependent on<em> θ</em></p></div>
<div class="para"><p>This equation defines the objective function as the difference between the log-probability distribution function of<em> y</em> and another function of<em> y</em>. This form of the objective function seems not“ideal”, as we usually prefer to see the difference between two distributions, so that we can interpret this difference as some kind of divergence between the distributions. A simple idea is to convert the second term (i.e., log<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) exp( 1</p></div>
<div class="para"><p><em>β</em> <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>))) into a log-probability distribution over the domain of<strong> y</strong>. If we treat<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) exp( 1</p></div>
<div class="para"><p><em>β</em> <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)) as an unnormalized probability of<em> y</em>,we can convert it into a normalized probability by dividing it by a normalization factor:</p></div>
<div class="para"><p><strong>y</strong> <em>π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) exp  1</p></div>
<div class="para"><p><em>Z</em>(<strong>x</strong>)=X</p></div>
<div class="para"><p><em>β </em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)(4.75)</p></div>
<div class="para"><p>Hence we can define a probability distribution by</p></div>
<div class="para"><p><em>π</em><em>∗</em>(<strong>y</strong><em>|</em><strong>x</strong>)=<em>π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) exp  1</p></div>
<div class="para"><p><em>β</em> <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)</p></div>
<div class="para"><p><em>Z</em>(<strong>x</strong>)(4.76)</p></div>
<div class="para"><p><em>4.4 Improved Human Preference Alignment</em> 195</p></div>
<div class="para"><p>We then rewrite Eq. (4.74) as</p></div>
<div class="para"><p>˜<em>θ</em>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>)h log<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em>log <em>π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) exp  1</p></div>
<div class="para"><p><em>β</em> <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)</p></div>
<div class="para"><p><em>Z</em>(<strong>x</strong></p></div>
<div class="para"><p><em>−</em>log<em> Z</em>(<strong>x</strong>)i</p></div>
<div class="para"><p>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>)h log<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em>log<em> π</em><em>∗</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em>log<em> Z</em>(<strong>x</strong>)i</p></div>
<div class="para"><p>E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>)h log<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em>log<em> π</em><em>∗</em>(<strong>y</strong><em>|</em><strong>x</strong>)i</p></div>
<div class="para"><p>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em></p></div>
<div class="para"><p><em>−</em>E <strong>y</strong><em>∼</em><em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>) log<em> Z</em>(<strong>x</strong>)</p></div>
<div class="para"><p>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> h KL <em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>)<em> ||</em><em> π</em><em>∗</em>(<em>·|</em><strong>x</strong>)</p></div>
<div class="para"><p>i</p></div>
<div class="para"><p><em>−</em>log<em> Z</em>(<strong>x</strong>)|{z}constant wrt.<em> θ</em></p></div>
<div class="para"><p>(4.77)</p></div>
<div class="para"><p>|{z}KL divergence</p></div>
<div class="para"><p>Since log<em> Z</em>(<strong>x</strong>) is independent of<em> θ</em>, it does not affect the result of the arg min <em>θ</em> operation,and can be removed from the objective. Now we obtain a new training objective which finds the optimal policy<em> π</em> <em>θ</em> by minimizing the KL divergence between<em> π</em> <em>θ</em>(<em>·|</em><strong>x</strong>) and<em> π</em><em>∗</em>(<em>·|</em><strong>x</strong>)</p></div>
<div class="para"><p>˜<em>θ</em>=arg min <em>θ</em> E <strong>x</strong><em>∼D</em> h KL <em>π</em> <em>θ</em>(<em>·|</em><strong>x</strong>)<em> ||</em><em> π</em><em>∗</em>(<em>·|</em><strong>x</strong>)i</p></div>
<div class="para"><p>(4.78)</p></div>
<div class="para"><p>Clearly, the solution to this optimization problem is given by</p></div>
<div class="para"><p><em>π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)=<em>π</em><em>∗</em>(<strong>y</strong><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p>=<em>π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) exp  1</p></div>
<div class="para"><p><em>β</em> <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>))</p></div>
<div class="para"><p><em>Z</em>(<strong>x</strong>(4.79)</p></div>
<div class="para"><p>Given this equation, we can express the reward<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>) using the target model<em> π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>), the reference model<em> π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>), and the normalization factor<em> Z</em>(<strong>x</strong>):</p></div>
<div class="para"><p>!</p></div>
<div class="para"><p>log<em> </em><em>π</em> <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)=<em>β</em></p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<strong>y</strong><em>|</em><strong>x</strong>) + log<em> Z</em>(<strong>x</strong>)</p></div>
<div class="para"><p>(4.80)</p></div>
<div class="para"><p>This is interesting because we initially seek to learn the policy<em> π</em> <em>θ</em>(<em>·</em>) using the reward model <em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>), but eventually obtain a representation of the reward model based on the policy. Given the reward model defined in Eq. (4.80), we can apply it to the Bradley-Terry model to calculate the preference probability (also see Section 4.3.2):</p></div>
<div class="para"><p>Pr <em>θ</em>(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)=Sigmoid(<em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>a</em>)<em> −</em><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong> <em>b</em>))</p></div>
<div class="para"><p>=Sigmoid<em>β</em>log<em> </em><em>π</em> <em>θ</em>(<strong>y</strong> <em>a</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<strong>y</strong> <em>a</em><em>|</em><strong>x</strong>) + log<em> Z</em>(<strong>x</strong>)<em>−</em></p></div>
<div class="para"><p><em>β</em>log<em> </em><em>π</em> <em>θ</em>(<strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<strong>y</strong> <em>b</em><em>|</em><strong>x</strong>) + log<em> Z</em>(<strong>x</strong>)</p></div>
<div class="para"><p>=Sigmoid<em>β</em> log<em> </em><em>π</em> <em>θ</em>(<strong>y</strong> <em>a</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<strong>y</strong> <em>a</em><em>|</em><strong>x</strong>)<em> </em><em>−</em><em>β</em> log<em> π</em> <em>θ</em>(<strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p></p></div>
<div class="para"><p>(4.81)</p></div>
<div class="para"><p><em>π</em> <em>θ</em> ref(<strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p>196 <em>Alignment</em></p></div>
<div class="para"><p>This formula is elegant because it converts the difference in rewards into the difference in ratio functions, and we do not need to calculate the value of<em> Z</em>(<strong>x</strong>). A direct result is that we no longer need a reward model, but only need the target policy and reference model to calculate the probability of preferences. Finally, we can train the target policy by minimizing the following DPO loss function</p></div>
<div class="para"><p><em>L</em> dpo(<em>θ</em>)=<em>−</em>E(<strong>x</strong><em>,</em><strong>y</strong> <em>a</em><em>,</em><strong>y</strong> <em>b</em>)<em>∼D</em> <em>r</em> log Pr <em>θ</em>(<strong>y</strong> <em>a</em><em> ≻</em><strong>y</strong> <em>b</em><em>|</em><strong>x</strong>)(4.82)</p></div>
<div class="para"><p>The form of this loss function is very similar to that used in training reward models in RLHF (see Eq. (4.36)). But it should be noted that the loss function here depends on the parameters of the policy (i.e.,<em> θ</em>) rather than the parameters of the reward model (i.e.,<em> ϕ</em>).</p></div>
<div class="para"><p>The main advantage of DPO lies in its simplicity and efficiency. The DPO objective is very straightforward — it directly optimizes for preference-based feedback, rather than relying on separately developed reward models. Moreover, DPO is generally more sample-efficient, as it learns from a fixed dataset without the need for the computationally expensive sampling process used in PPO. This makes DPO a popular method for human preference alignment, especially when developing and applying reward models via reinforcement learning is challenging.</p></div>
<div class="para"><p>DPO can broadly be viewed as an<strong> offline reinforcement learning</strong> method, where the training data is pre-collected and fixed, and there is no exploration. In contrast, online reinforcement learning methods like PPO, which require exploring new states through interaction with the environment (using the reward model as a proxy), also have their unique advantages. One of the benefits of online reinforcement learning is that it allows the agent to continuously adapt to changes in the environment by learning from real-time feedback. This means that, unlike offline methods,online methods are not constrained by the static nature of pre-collected data and can discover new problem-solving strategies. In addition, exploration can help the agent cover a wider range of state-action pairs, thus improving generalization. This could be an important advantage for LLMs,as generalization is considered a critical aspect in applying such large models.</p></div>
<div class="para"><p><strong>4.4.3</strong> <strong>Automatic Preference Data Generation</strong></p></div>
<div class="para"><p>Although learning from human preferences is an effective and popular method for aligning LLMs,annotating preference data is costly. Using human feedback does not only faces the problem of limited scalability, but it may also introduce bias because human feedback is inherently subjective.As a result, one can turn to AI feedback methods to address these scalability and consistency issues without the limitations associated with human annotators.</p></div>
<div class="para"><p>As with data generation for instruction fine-tuning, generating preference data using LLMs is straightforward. Given a set of inputs, we first use an LLM to generate pairs of outputs. Then, we prompt the LLM to label the preference between each pair of outputs, along with its corresponding input. Below is an example of prompting the LLM to generate a preference label for a pair of consumer service responses.</p></div>
<div class="para"><p><em>4.4 Improved Human Preference Alignment</em> 197</p></div>
<div class="para"><p>Consider a customer service scenario where a customer poses a request. You will review two responses to this request. Please indicate which response is preferred. Note that a good response should be courteous, clear, and concise. It should address the customer’s concern directly, provide helpful information or a solution, and maintain a positive tone.</p></div>
<div class="para"><p>Request:</p></div>
<div class="para"><p><em>Hello, I noticed that my order hasn’t arrived yet, though it was scheduled to</em> <em>arrive several days ago. Could you please update me on its status? Thank you!</em></p></div>
<div class="para"><p>Response A:</p></div>
<div class="para"><p><em>I’m very sorry for the delay and understand how disappointing this can be. We’re</em> <em>doing our best to sort this out quickly for you.</em></p></div>
<div class="para"><p>Response B:</p></div>
<div class="para"><p><em>Hey, stuff happens! Your package will get there when it gets there, no need to</em> <em>stress.</em></p></div>
<div class="para"><p>Response A is preferred.</p></div>
<div class="para"><p>Once we collect such preference labels, we can use them, along with the output pair and input,to train the reward model. Of course, we can consider demonstrating a few examples or using advanced prompting techniques, such as CoT, to improve labeling performance. For example, we can include in the prompt an example showing how and why one of the two responses is preferred based on a CoT rationale.</p></div>
<div class="para"><p>In addition to preference labels, we can also obtain the probability associated with each label[Lee et al., 2023]. A simple method is to extract the probabilities for the label tokens, such as “A”and “B”, from the probabilities output by the LLM. We can then use the Softmax function or other normalization techniques to re-normalize these probabilities into a distribution over the labels.These probabilities of preferred labels can serve as pointwise supervision signals for training the reward model, as discussed in Section 4.4.1.</p></div>
<div class="para"><p>For data generation, although it is easy to scale up, it is often necessary to ensure the data is accurate and diverse. Here, the data quality and diversity issues involve not only the labeling of preferences but also the inputs and outputs of the model. Therefore, we often need to use a variety of techniques to obtain large-scale, high-quality data. For example, one can generate diverse model outputs and annotations by using different LLMs, prompts, in-context demonstrations, and so on [Cui et al., 2024]. Dubois et al. [2024] report that the variability in pairwise preference data is important for training LLMs from either human or AI feedback.</p></div>
<div class="para"><p>While learning from AI feedback is highly scalable and generally objective, this method is more suited to well-defined tasks where objective performance metrics are available. By contrast,learning from human feedback is more advantageous when aligning AI systems with human values, preferences, and complex real-world tasks that require understanding of subtle or subjective context. These methods can be combined to train LLMs that benefit from both human insights and the scalability of AI feedback.</p></div>
<div class="para"><p>198 <em>Alignment</em></p></div>
<div class="para"><p><strong>4.4.4</strong> <strong>Step-by-step Alignment</strong></p></div>
<div class="para"><p>So far, our discussion of alignment has primarily focused on the use of reward models for evaluating entire input-output sequence pairs. These methods can be easily adapted to scenarios where the correctness of an output can be examined by checking whether the desired result is included.For example, in the task of calculating a mathematical expression, a reward model can provide positive feedback if the answer is correct, and negative feedback if the answer is wrong. However, in many problems that require complex reasoning, simply examining the correctness of the final result is insufficient for learning. Imagine a student who is only given the final answer to a challenging math problem. Knowing whether the final answer is right or wrong does not help the student figure out where they went wrong and how to calculate the correct answer. A better approach would be to guide the student with a step-by-step breakdown of the problem-solving process and encourage understanding of the underlying concepts and logic behind these steps.</p></div>
<div class="para"><p>In Chapter 3, we studied CoT methods to prompt LLMs to explicitly write out intermediate steps or the reasoning process needed to reach a conclusion or solve a problem. We saw that breaking down a problem into smaller parts could make it easier to understand the solution path and increase the accuracy of the output. These methods can be naturally extended to the alignment of LLMs, that is, we supervise the model during the intermediate steps of reasoning. Consider a reasoning task where an LLM produces a sequence of reasoning steps<strong> y</strong> =<em> {</em>¯<strong>y</strong> 1<em>, ...,</em> ¯<strong>y</strong> <em>n</em> <em>s</em><em>}</em> for the given input. The result of the reasoning is assumed to be included in the last step ¯<strong>y</strong> <em>n</em> <em>s</em>, and can be easily verified. For this reasoning problem, Uesato et al. [2022] categorize LLM fine-tuning approaches into two classes:</p></div>
<div class="para"><p>•<strong> Outcome-based Approaches</strong>. Supervision occurs only when the end result is verified. This is a standard method for learning from human feedback we have discussed in this chapter.For example, the LLM is optimized to maximize some form of the reward<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>).</p></div>
<div class="para"><p>•<strong> Process-based Approaches</strong>. Supervision is involved in all intermediate steps in addition to the last step. To do this, we need to develop a model to give a supervision signal at each step, and develop loss functions that can make use of such supervision signals.</p></div>
<div class="para"><p>Figure 4.11 shows two LLM outputs for an example math problem. Although the LLM gives the correct final answer in both cases, it makes mistakes during the problem-solving process in the second output. Outcome-based approaches overlook these mistakes and give positive feedback for the entire solution. By contrast, process-based approaches can take these mistakes into account and provide additional guidance on the detailed reasoning steps.</p></div>
<div class="para"><p>An important issue for process-based approaches is that we need to get step-level feedback during a (potentially) long reasoning path. We can collect or generate reasoning paths corresponding to problems from existing datasets. Human experts then annotate each step in these paths for correctness. These annotations can be used to directly train LLMs or as rewards in reward modeling. However, in practice, richer annotations are often introduced [Lightman et al., 2024]. In addition to the<em> correct</em> and<em> incorrect</em> labels, a step can also be labeled as<em> neutral</em> to indicate that while the step may be technically correct, it might still be problematic within the overall reasoning process. Furthermore, to improve the efficiency of data annotation, techniques such as active learning can be employed. Identifying obvious errors usually does not significantly contribute to</p></div>
<div class="para"><p><em>4.4 Improved Human Preference Alignment</em> 199</p></div>
<div class="para"><p><strong>Question</strong> (from The International Junior Math Olympiad):Jessica has a lot of apps on her phone. The number of apps she has is divisible by 5. If she downloaded 10 more apps, she would still have less than 50 apps. What is the maximum original number of apps Jessica could have?</p></div>
<div class="para"><p><strong>Response 1</strong>:</p></div>
<div class="para"><p><strong>Response 2</strong>:</p></div>
<div class="para"><p><em>x</em> must be a multiple of 5, i.e.,<em>x</em><em> ≡</em>0 (mod 5).x¯<strong>y</strong> 1:</p></div>
<div class="para"><p>The number of apps is divisible by 5,so<em> x</em> must be a multiple of 5.x¯<strong>y</strong> 1:</p></div>
<div class="para"><p>¯<strong>y</strong> 2:</p></div>
<div class="para"><p>¯<strong>y</strong> 2:</p></div>
<div class="para"><p>If she downloads 10 more apps, her new total will be<em> x</em> + 10, and this must be less than 50. Therefore,<em> x</em> + 10<em> &lt;</em> 50.Simplifying this gives<em> x &lt;</em> 40.</p></div>
<div class="para"><p>After downloading 10 apps, the total total number of apps should still be less than 50:<em> x</em> + 10<em> &lt;</em> 50. Solving this inequality:<em> x &lt;</em> 40.</p></div>
<div class="para"><p>x</p></div>
<div class="para"><p>x</p></div>
<div class="para"><p>problematic</p></div>
<div class="para"><p><em>x</em> must be less than or equal to the largest multiple of 5, which is 40. x¯<strong>y</strong> 3:</p></div>
<div class="para"><p>Since<em> x</em> must be a multiple of 5 and less than 40, the largest value of<em> x</em> is 35.x¯<strong>y</strong> 3:</p></div>
<div class="para"><p>problematic</p></div>
<div class="para"><p>¯<strong>y</strong> 4:</p></div>
<div class="para"><p>¯<strong>y</strong> 4:</p></div>
<div class="para"><p>But<em> x</em> + 10 should not be more than or equal to 50. So we need to subtract 5 from 40.</p></div>
<div class="para"><p>Verify the result. If<em> x</em> = 35, then after downloading 10 apps, Jessica would have: 35 + 10 = 45 which satisfy 45<em> &lt;</em> 50.</p></div>
<div class="para"><p>x</p></div>
<div class="para"><p>x</p></div>
<div class="para"><p>Therefore, the final result is 35.x¯<strong>y</strong> 5:</p></div>
<div class="para"><p>The maximum original number of apps is 35.x¯<strong>y</strong> 5:</p></div>
<div class="para"><p>correct</p></div>
<div class="para"><p>correct</p></div>
<div class="para"><p><strong>Fig. 4.11:</strong> Two LLM responses to a math problem. In response 1, both the final result and all the reasoning steps are correct. In response 2, the final result is correct, but there are mistakes in the reasoning process (highlighted in red).For outcome-based approaches, both responses are considered correct. For process-based approaches, the mistakes in response 2 can be considered in reward modeling.</p></div>
<div class="para"><p>learning from reasoning mistakes. Instead, annotating steps that the model confidently considers correct but are actually problematic is often more effective.</p></div>
<div class="para"><p>Given a set of step-level annotated reasoning paths and corresponding inputs, we can train a reward model to provide feedback for supervising policy learning. The reward model can be treated as a classification model, and so its architecture can be a Transformer decoder with a Softmax layer stacked on top. At step<em> k</em>, the reward model takes both the problem description(denoted by<strong> x</strong>) and the reasoning steps generated so far (denoted by ¯<strong>y</strong><em>≤</em><em>k</em>) as input and outputs a probability distribution over the label set<em> {</em><em>correct</em><em>,</em><em> incorrect</em><em>}</em> or<em> {</em><em>correct</em><em>,</em><em> incorrect</em><em>,</em><em> neutral</em><em>}</em>.Then the learned reward model is used to evaluate reasoning paths by assessing the correctness of each step. A simple method to model correctness is to count the number of steps that are classified as<em> correct</em>, given by</p></div>
<div class="para"><p><em>n</em> <em>s</em> X</p></div>
<div class="para"><p><em>k</em>=1 <em>δ</em>(<em>correct</em><em>, C</em>(<strong>x</strong><em>,</em> ¯<strong>y</strong><em>≤</em><em>k</em>))(4.83)</p></div>
<div class="para"><p><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)=</p></div>
<div class="para"><p>where<em> C</em>(<strong>x</strong><em>,</em> ¯<strong>y</strong><em>≤</em><em>k</em>) denotes the label with the maximum probability. We can also use log-probabilities</p></div>
<div class="para"><p>200 <em>Alignment</em></p></div>
<div class="para"><p>of classification to define the reward of the entire path</p></div>
<div class="para"><p><em>n</em> <em>s</em> X</p></div>
<div class="para"><p><em>k</em>=1 log Pr(<em>correct</em><em>|</em><strong>x</strong><em>,</em> ¯<strong>y</strong><em>≤</em><em>k</em>)(4.84)</p></div>
<div class="para"><p><em>r</em>(<strong>x</strong><em>,</em><strong> y</strong>)=</p></div>
<div class="para"><p>where Pr(<em>correct</em><em>|</em><strong>x</strong><em>,</em> ¯<strong>y</strong><em>≤</em><em>k</em>) denotes the probability of the<em> correct</em> label generated by the reward model. The reward score<em> r</em>(<strong>x</strong><em>,</em><strong> y</strong>) can then be used to train the policy in RLHF as usual.</p></div>
<div class="para"><p>While we restrict our discussion to math problems, the approaches described here are general and can be applied to a wide variety of tasks that involve multi-step reasoning and decisionmaking. Moreover, we can consider various aspects when assessing the quality of a step, rather than just its correctness. For example, in dialogue systems, responses must not only be accurate but also contextually appropriate across multiple turns of conversation. If a model provides a correct response but fails to maintain coherence in the context of the ongoing dialogue, steplevel feedback could help the model identify and correct such discrepancies. Also note that the process-based approaches are related to the fine-grained reward modeling approaches discussed in Section 4.4.1.3. All these approaches essentially aim to provide more detailed supervision to LLMs by breaking their outputs into smaller, more manageable steps. However, process-based feedback focuses more on evaluating the correctness of a step based on its preceding steps, while the approaches in Section 4.4.1.3 emphasize evaluating each step independently.</p></div>
<div class="para"><p>The idea of aligning LLMs step by step has great application potential, especially considering the recent shift towards more complex reasoning tasks in the use of LLMs. For example, both the GPT-o1 and GPT-o3 models are designed with more advanced reasoning techniques (such as long internal CoT) to solve challenging problems like scientific and mathematical reasoning[OpenAI, 2024]. These tasks often rely on long and complex reasoning paths, and therefore, it seems essential to introduce detailed supervision signals in the reasoning process. Moreover, from a practical perspective, effective supervision on long reasoning paths not only improves reasoning performance, but it also helps the model eliminate redundant or unnecessary reasoning steps,thereby reducing reasoning complexity and improving efficiency.</p></div>
<div class="para"><p><strong>4.4.5</strong> <strong>Inference-time Alignment</strong></p></div>
<div class="para"><p>In this section we explored a variety of methods to align models with human preferences and annotations. However, one of the significant limitations of many such methods is that LLMs must be fine-tuned. For RLHF and its variants, training LLMs with reward models can be computationally expensive and unstable, leading to increased complexity and costs when applying these approaches. In this case, we can consider aligning models at inference time, thus avoiding the additional complexity and effort involved.</p></div>
<div class="para"><p>One simple way to achieve inference-time alignment is to use the reward model to select the best one from<em> N</em> alternative outputs generated by the LLM, a method known as<strong> Best-of-</strong><em>N</em> <strong>sampling</strong> (<strong>BoN sampling</strong>). We can consider BoN sampling as a form of reranking. In fact,reranking methods have been widely used in NLP tasks, such as machine translation, for a long time. They are typically applied in situations where training complex models is costly. In such cases, directly reranking the outputs allows for the incorporation of these complex models at a very low cost 8.</p></div>
<div class="para"><p>8 Reranking methods can also help us explore what are known as model errors and search errors, although these</p></div>
<div class="para"><p><em>4.5 Summary</em> 201</p></div>
<div class="para"><p>In the BoN sampling process, the LLM takes the input sequence<strong> x</strong> and generates<em> N</em> different output sequences<em> {</em>ˆ <strong>y</strong> 1<em>, ...,</em> ˆ <strong>y</strong> <em>N</em><em>}</em>:</p></div>
<div class="para"><p><em>{</em>ˆ <strong>y</strong> 1<em>, ...,</em> ˆ <strong>y</strong> <em>N</em><em>}</em>=argTopN <strong>y</strong>[Pr(<strong>y</strong><em>|</em><strong>x</strong>)](4.85)</p></div>
<div class="para"><p>where the argTopN operation returns the top-<em>N</em> outputs that maximize the function Pr(<strong>y</strong><em>|</em><strong>x</strong>).These outputs can be generated in a variety of ways, depending on the search algorithm used by the model (e.g., sampling or beam search). Once the<em> N</em>-best output candidates are generated, the reward model is used to evaluate and select the best one:</p></div>
<div class="para"><p>ˆ <strong>y</strong> best=max<em>{</em><em>r</em>(<strong>x</strong><em>,</em> ˆ <strong>y</strong> 1)<em>, ..., r</em>(<strong>x</strong><em>,</em> ˆ <strong>y</strong> <em>N</em>)<em>}</em>(4.86)</p></div>
<div class="para"><p>It is worth noting that the result of BoN sampling is also influenced by the diversity of the <em>N</em>-best list. This is a common issue with most reranking methods. Typically, we wish the<em> N</em>-best output candidates to have relatively high quality but be sufficiently different from each other. In many text generation systems, the<em> N</em>-best outputs are very similar, often differing by just one or two words. The diversity issue is even more challenging in LLMs, as the<em> N</em>-best outputs generated by an LLM can be different in their wordings, yet their semantic meanings are often quite similar. In practice, one can adjust the model hyperparameters and/or adopt different LLMs to generate more diverse output candidates for reranking. Nevertheless, as with many practical systems, we need to make a trade-off between selecting high-quality candidates and ensuring sufficient variation in the generated outputs.</p></div>
<div class="para"><p>BoN sampling can be used for training LLMs as well. A closely related method is<strong> rejection</strong> <strong>sampling</strong>. In this method, we first select the “best” outputs from the<em> N</em>-best lists via the reward model, and then take these selected outputs to fine-tune the LLM. In this way, we can introduce human preferences into the training of LLMs via a much simpler approach compared to RLHF.Many LLMs have adopted rejection sampling for fine-tuning [Nakano et al., 2021; Touvron et al.,2023b].</p></div>
<h3>4.5 Summary</h3>
<div class="para"><p>In this chapter, we have explored a range of techniques for aligning LLMs. In particular, we have discussed fine-tuning methods that enable LLMs to follow instructions and align them with human preferences. One of the benefits of fine-tuning LLMs is computation efficiency. Unlike pre-training based on large-scale neural network optimization, fine-tuning is a post-training step and so is less computationally expensive. Moreover, it is better suited to address problems that are not easily solved in pre-training, such as human value alignment. The widespread attention to the alignment issue has also led to a surge of research papers on this topic, which has posed challenges in writing this chapter, as it is difficult to cover all the latest techniques. However, we have tried to provide a relatively detailed introduction to the fundamental approaches to alignment, such as</p></div>
<div class="para"><p>issues are not often discussed in the context of LLMs. For example, suppose we have an old model and a new, more powerful model. We can use the new model to select the best output from the<em> N</em>-best list of the old model as the oracle output. The performance difference between the oracle output and the top-1 output of the original<em> N</em>-best list reflects the performance gain brought by the new model. If the performance gain is significant, we can say that the old model has more model errors. If the gain is small, it may indicate that the issue lies in search errors, as the best candidates were not found.</p></div>
<div class="para"><p>202 <em>Alignment</em></p></div>
<div class="para"><p>instruction fine-tuning and RLHF.</p></div>
<div class="para"><p>While we have focused on LLM alignment techniques in this chapter, the term<em> AI alignment</em> is a wide-ranging concept. It generally refers to the process of ensuring that the behavior of an AI system aligns with human values, goals, and expectations. The idea of AI alignment can be traced back to the early days of AI. A widely cited description of AI alignment comes from an article by the mathematician and computer scientist Norbert Wiener [Wiener, 1960]. The quote is as follows</p></div>
<div class="para"><p><em>If we use, to achieve our purposes, a mechanical agency with whose operation</em> <em>we cannot efficiently interfere ... we had better be quite sure that the purpose</em> <em>put into the machine is the purpose which we really desire.</em></p></div>
<div class="para"><p>At that time, AI alignment was a distant concern for researchers. But today, it greatly influences the design of various AI systems. For example, in robotics, alignment is critical to ensuring that autonomous robots safely interact with humans and their environments. In autonomous driving, cars must not only follow traffic laws but also make complex, real-time decisions that prioritize human safety, avoid accidents, and navigate ethical dilemmas.</p></div>
<div class="para"><p>In current AI research, alignment is usually achieved by developing a surrogate objective that is analogous to the real goal and steering the AI system towards this objective. However, designing the objective of AI alignment is very difficult. One reason is that human values are diverse and often context-dependent, making it difficult to distill them into a single, universally applicable objective function. Also, the complexity of real-world environments, where values and goals often conflict or evolve over time, further complicates alignment efforts. Even if we could define an appropriate objective, AI systems may find unintended ways to achieve it, leading to “misaligned”outcomes that still technically satisfy the objective but in a harmful or counterproductive way.</p></div>
<div class="para"><p>These challenges have motivated and are motivating AI research towards more aligned systems, either through developing new mechanisms for perceiving the world or more efficient and generalizable methods to adapt these systems to given tasks. More importantly, as AI systems become more powerful and intelligent, especially given that recent advances in LLMs have shown remarkable capabilities in dealing with many challenging problems, the need for AI alignment has become more urgent. Researchers have started to be concerned with AI safety and warn the community that they need to develop and release AI systems with great caution to prevent these systems from being misaligned [Russell, 2019; Bengio et al., 2024].</p></div>
<div class="para"><p>https://github.com/NiuTrans/NLPBook</p></div>
<div class="para"><p>https://niutrans.github.io/NLPBook</p></div>
<h3>C HAPTER 5</h3>
<h1>Inference</h1>
<div class="para"><p>Once we have pre-trained and fine-tuned an LLM, we can apply it to make predictions on new data. This process is called inference, in which the LLM computes the probabilities of different possible outputs given an input, and selects the output that maximizes the probability. The inference problem is generally expressed in the following form:</p></div>
<div class="para"><p>ˆ <strong>y</strong>=arg max <strong>y</strong> Pr(<strong>y</strong><em>|</em><strong>x</strong>)(5.1)</p></div>
<div class="para"><p>where<strong> x</strong> is the input sequence,<strong> y</strong> is a possible output sequence, and ˆ <strong>y</strong> is the best output sequence.</p></div>
<div class="para"><p>This is perhaps one of the most widely adopted formulas in NLP, and dates back to the early days of speech recognition and machine translation systems based on probabilistic models. Although for some applications, such as predicting a token using a very small language model, solving this optimization problem seems trivial, for most situations the computational challenges arise from both calculating Pr(<strong>y</strong><em>|</em><strong>x</strong>) and performing the arg max operation. The problems we therefore wish to address in this chapter involve: 1) computing the prediction probability efficiently given a trained LLM, and 2) devising an efficient (suboptimal) search for ˆ <strong>y</strong>.</p></div>
<div class="para"><p>At a high level, these are fundamental issues in artificial intelligence, which have been extensively studied. So many well-established techniques can be directly applied, for example, one can use greedy search algorithms to implement an efficient inference system. On the other hand,model-specific optimizations, such as efficient attention models for Transformers, can be considered to further improve efficiency. But, in many practical applications, we still need to make a trade-off between accuracy and efficiency, by carefully combining various techniques.</p></div>
<div class="para"><p>The importance of the inference problem in LLMs also lies in the fact that many application scenarios require processing extremely long sequences. Recent studies have found that injecting additional prompts and contextual information, such as long chain-of-thought prompts, during inference can significantly improve the performance of LLMs. This provides a new approach to scaling LLMs: better results can be achieved by increasing the compute at inference time. For instance, through inference-time scaling, OpenAI [2024]’s o1 and Deepseek [2025]’s R1 systems have demonstrated impressive performance on complex reasoning and programming tasks. This,in turn, has encouraged the NLP field to focus more on the issue of efficient inference.</p></div>
<div class="para"><p>In this chapter, we will introduce basic concepts and algorithms of LLM inference, including prefilling-decoding frameworks, search (decoding) algorithms, and evaluation metrics of inference performance. We will then present methods for improving the efficiency of LLM inference,covering a range of techniques for speeding up the system and compressing the model. Finally,we will discuss inference-time scaling, which is considered an important application of inference optimization.</p></div>
<div class="para"><p>204 <em>Inference</em></p></div>
<h3>5.1 Prefilling and Decoding</h3>
<div class="para"><p>In this section, we present the prefilling-decoding framework, which is the most commonly used for interpreting and implementing LLM inference processes. We first introduce the notation and background knowledge, and then describe the details of the framework, such as the decoding algorithms for LLM inference.</p></div>
<div class="para"><p><strong>5.1.1</strong> <strong>Preliminaries</strong></p></div>
<div class="para"><p>Although we have described LLMs many times in this book, we begin by briefly defining the notation to facilitate the subsequent discussion, and to make this chapter self-contained.</p></div>
<div class="para"><p><strong>x</strong>:The input token sequence. It is conceptually equivalent to a “prompt”,which includes instructions, user inputs, and any additional context intended as input to the LLM.<strong> x</strong> comprises<em> m</em> + 1 tokens, denoted by <em>x</em> 0<em>...x</em> <em>m</em>, where<em> x</em> 0 is the start symbol<em> ⟨</em>SOS<em>⟩</em>.</p></div>
<div class="para"><p><strong>y</strong>:The output token sequence, also called the response to the input.<strong> y</strong> comprises<em> n</em> tokens, denoted by<em> y</em> 1<em>...y</em> <em>n</em>.</p></div>
<div class="para"><p><strong>y</strong><em>&lt;i</em>:The output tokens that precede position<em> i</em>, that is,<strong> y</strong><em>&lt;i</em> =<em> y</em> 1<em>...y</em> <em>i</em><em>−</em>1.</p></div>
<div class="para"><p>Pr(<strong>y</strong><em>|</em><strong>x</strong>):The probability of generating<strong> y</strong> given<strong> x</strong> using the LLM. If the LLM is parameterized by<em> θ</em>, we can write it as Pr <em>θ</em>(<strong>y</strong><em>|</em><strong>x</strong>).</p></div>
<div class="para"><p>[<strong>x</strong><em>,</em><strong> y</strong>]:The concatenated token sequence of<strong> x</strong> and<strong> y</strong>.That is, [<strong>x</strong><em>,</em><strong> y</strong>]=<em>x</em> 0<em>...x</em> <em>m</em> <em>y</em> 1<em>...y</em> <em>n</em>. Occasionally, we use the notation seq <strong>x</strong><em>,</em><strong>y</strong> to represent[<strong>x</strong><em>,</em><strong> y</strong>].</p></div>
<div class="para"><p>Pr([<strong>x</strong><em>,</em><strong> y</strong>]):The probability of generating the token sequence [<strong>x</strong><em>,</em><strong> y</strong>] using the LLM.</p></div>
<div class="para"><p>As described in Eq. (5.1), the goal of LLM inference is to maximize Pr(<strong>y</strong><em>|</em><strong>x</strong>). Modeling this conditional probability is common in NLP. At first glance, it seems to be a sequence-to-sequence problem, where we transform a sequence into another using encoding-decoding models. However, we are not discussing sequence-to-sequence problems or encoding-decoding architectures.Instead, as discussed in earlier chapters, this modeling problem can be addressed by using decoderonly models. To do this, we can interpret the log-scale probability log Pr(<strong>y</strong><em>|</em><strong>x</strong>) as the difference between log Pr([<strong>x</strong><em>,</em><strong> y</strong>]) and log Pr(<strong>x</strong>)</p></div>
<div class="para"><p>log Pr(<strong>y</strong><em>|</em><strong>x</strong>)=log Pr([<strong>x</strong><em>,</em><strong> y</strong>])<em> −</em>log Pr(<strong>x</strong>)(5.2)</p></div>
<div class="para"><p>where log Pr([<strong>x</strong><em>,</em><strong> y</strong>]) and log Pr(<strong>x</strong>) can be obtained by running the LLM on the sequences [<strong>x</strong><em>,</em><strong> y</strong>]and<strong> x</strong>, respectively. For example, we can calculate the probability of generating<strong> x</strong> using the chain</p></div>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 205</p></div>
<div class="para"><p>rule</p></div>
<div class="para"><p>log Pr(<strong>x</strong>)=log Pr(<em>x</em> 0<em>...x</em> <em>m</em>)</p></div>
<div class="para"><p>=log Pr(<em>x</em> 0) Pr(<em>x</em> 1<em>|</em><em>x</em> 0)<em> · · ·</em> Pr(<em>x</em> <em>m</em><em>|</em><em>x</em> 0<em>...x</em> <em>m</em><em>−</em>1)</p></div>
<div class="para"><p><em>m</em> X</p></div>
<div class="para"><p><em>j</em>=1 log Pr(<em>x</em> <em>j</em><em>|</em><strong>x</strong><em>&lt;j</em>)</p></div>
<div class="para"><p>=log Pr(<em>x</em> 0)|{z}=0</p></div>
<div class="para"><p>+</p></div>
<div class="para"><p><em>m</em> X</p></div>
<div class="para"><p><em>j</em>=1 log Pr(<em>x</em> <em>j</em><em>|</em><strong>x</strong><em>&lt;j</em>)(5.3)</p></div>
<div class="para"><p>=</p></div>
<div class="para"><p>In other words, we calculate the token prediction log-probability at each position of<strong> x</strong>, and sum all these log-probabilities.</p></div>
<div class="para"><p>In common implementations of LLMs, however, we do not need to compute the log-probability of the input sequence, but use the LLM to directly compute the log-probability of the output sequence in the following form</p></div>
<div class="para"><p><em>n</em> X</p></div>
<div class="para"><p>log Pr(<strong>y</strong><em>|</em><strong>x</strong>)=</p></div>
<div class="para"><p><em>i</em>=1 log Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)(5.4)</p></div>
<div class="para"><p>where [<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>] represents the context for predicting<em> y</em> <em>i</em>. We use Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>) to denote Pr(<em>y</em> <em>i</em><em>|</em>[<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>]),following the commonly used notation in the literature.</p></div>
<div class="para"><p>Now, we have two sub-problems in addressing the inference issue described in Eq. (5.1):</p></div>
<div class="para"><p>•<strong> Model Computation</strong>: we model Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>) and compute it in an efficient manner.</p></div>
<div class="para"><p>•<strong> Search</strong>: we find the optimal (or sub-optimal) output sequence in terms of log Pr(<strong>y</strong><em>|</em><strong>x</strong>).</p></div>
<div class="para"><p>The second sub-problem is a classic issue in NLP. We will show in Section 5.1.3 that there are several well-studied algorithms that can be applied to efficiently search the space of possible output sequences. The first sub-problem requires a language model to produce a distribution over a vocabulary<em> V</em> given a sequence of context tokens. We can do this by training a Transformer decoder, which outputs the distribution</p></div>
<div class="para"><p>Pr(<em>·|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)=Softmax(<strong>HW</strong> <em>o</em>)<em>m</em>+<em>i</em>(5.5)</p></div>
<div class="para"><p><strong>H</strong>=Dec([<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>])(5.6)</p></div>
<div class="para"><p>Here Dec(<em>·</em>) produces a sequence of representations, each corresponding to a position of the input sequence. So, if we input [<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>] to the LLM,<strong> H</strong> is an<em> i</em><em>′</em><em> </em><em>×</em><em>d</em> matrix, where<em> d</em> is the dimensionality of each representation, and<em> i</em><em>′</em> =<em> m</em>+<em>i</em> is the number of context tokens. We can then use a Softmax layer to transform these representations into distributions of tokens.<strong> W</strong> <em>o</em><em> </em><em>∈</em>R <em>d</em><em>×|</em><em>V</em><em> |</em> is the linear mapping matrix of the Softmax layer, and<strong> HW</strong> <em>o</em> transforms the<em> d</em>-dimensional representations in<strong> H</strong> into the<em> |</em><em>V</em><em> |</em>-dimensional representations. The use of the subscript<em> m</em> +<em> i</em> indicates that the Softmax function is performed only on the representation at position<em> m</em> +<em> i</em>. See Figure 5.1 for an illustration of this architecture.</p></div>
<div class="para"><p>206 <em>Inference</em></p></div>
<div class="para"><p>Pr(<em>·|</em><strong>x</strong>)Pr(<em>·|</em><strong>x</strong><em>, y</em> 1)<em>· · ·</em>Pr(<em>·|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)</p></div>
<div class="para"><p>Softmax Layer</p></div>
<ul><li>· ·</li><li>· ·</li></ul>
<div class="para"><p>decoder output</p></div>
<div class="para"><p>FFN</p></div>
<ul><li>· ·</li><li>· ·</li></ul>
<div class="para"><p>Self-attention <em>L</em> layers</p></div>
<ul><li>· ·</li><li>· ·</li></ul>
<div class="para"><p>Linear Mapping</p></div>
<ul><li>· ·</li><li>· ·</li></ul>
<div class="para"><p>Embedding Layer</p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1<em>· · ·</em><em>x</em> <em>m</em> <em>y</em> 1<em>· · ·</em><em>y</em> <em>i</em><em>−</em>1</p></div>
<div class="para"><p><strong>Fig. 5.1:</strong> The decoder-only architecture for LLMs. The decoder consists of an embedding layer and a stack of Transformer layers. In each Transformer layer, the input passes through a linear mapping, a self-attention network, and an FFN. The output of the decoder is a sequence of representations that are taken as input to a Softmax layer, which generates a distribution of tokens for each position.</p></div>
<div class="para"><p>Dec(<em>·</em>) is a Transformer decoding network that consists of an embedding network and a number of stacked self-attention and FFN networks. We will not discuss Transformers in detail here,as readers can easily learn about these models from the literature. However, it is worth pointing out that the difficulty of inference is in part from the use of the self-attention mechanism in Transformers. Recall that a general form of single-head self-attention is given by</p></div>
<div class="para"><p>Att qkv(<strong>q</strong> <em>i</em><em>′</em><em>,</em><strong> K</strong><em>,</em><strong> V</strong>)=Softmax(<strong>q</strong> <em>i</em><em>′</em><strong>K</strong> T</p></div>
<div class="para"><p><em>√</em></p></div>
<div class="para"><p><em>d</em>)<strong>V</strong>(5.7)</p></div>
<div class="para"><p>where<strong> q</strong> <em>i</em><em>′</em><em> ∈</em>R <em>d</em> is the query at the position<em> i</em><em>′</em> (i.e., position of<em> y</em> <em>i</em> ), and<strong> K</strong> and<strong> V</strong><em> ∈</em>R <em>i</em><em>′</em><em>×</em><em>d</em> are the keys and values up to<em> i</em><em>′</em>, respectively.</p></div>
<div class="para"><p>At each step during inference, we call the self-attention function Att qkv(<em>·</em>), followed by an FFN, to generate a<em> d</em>-dimensional representation that integrates information from both the current token and its left context. This process is repeated through<em> L</em> layers of self-attention and FFN,forming a stack of Transformer layers. The output of the<em> L</em>-th layer in this stack is the final representation.</p></div>
<div class="para"><p>Each time, the model attends position<em> i</em><em>′</em> to all previous positions, which results in 2 <em>i</em><em>′</em> vector products (<em>i</em><em>′</em> times for<strong> q</strong> <em>i</em><em>′</em><strong>K</strong> T and<em> i</em><em>′</em> times for the product of Softmax(<strong>q</strong> <em>i</em><em>′</em><strong>K</strong> T</p></div>
<div class="para"><p><em>d</em> ) and<strong> V</strong>). Hence,generating a sequence of length<em> len</em> has a time complexity of<em> O</em>(<em>L</em><em> ×</em><em> len</em> 2) for the self-attention network. Clearly, the inference of this model is slow for long sequences due to its quadratic time complexity with respect to sequence length. Therefore, many improvements to Transformers and alternative models have focused on efficient methods that are faster than this quadratic time complexity, such as sparse attention mechanisms and linear-time models. A detailed discussion of efficient Transformers can be found in the previous chapters, and this section will focus on the</p></div>
<div class="para"><p><em>√</em></p></div>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 207</p></div>
<div class="para"><p>standard Transformer architecture.</p></div>
<div class="para"><p>Note that in self-attention, the queries, keys, and values of a layer are linear mappings from the same input (i.e., the output of the previous layer). Once a new key-value pair is generated, it is repeatedly used in subsequent inference steps. Rather than regenerating these key-value pairs during inference, a more desirable way is to store them in a structure, called the<strong> key-value cache</strong>,or the<strong> KV cache</strong>. Thus, (<strong>K</strong><em>,</em><strong> V</strong>) can straightforwardly be considered a KV cache. This cache is updated as follows</p></div>
<div class="para"><p><strong>K</strong>=Append(<strong>K</strong><em>,</em><strong> k</strong> <em>i</em><em>′</em>)(5.8)</p></div>
<div class="para"><p><strong>V</strong>=Append(<strong>V</strong><em>,</em><strong> v</strong> <em>i</em><em>′</em>)(5.9)</p></div>
<div class="para"><p>where (<strong>k</strong> <em>i</em><em>′</em><em>,</em><strong> v</strong> <em>i</em><em>′</em>) is the newly generated key-value pair at position<em> i</em><em>′</em>, and Append(<strong>a</strong><em>,</em><strong> b</strong>) denotes a function that appends a row vector<strong> b</strong> to a matrix<strong> a</strong>. Figure 5.2 shows how a Transformer decoder works with a KV cache.</p></div>
<div class="para"><p>Finally, the process of computing log Pr(<strong>y</strong><em>|</em><strong>x</strong>) is summarized as follows:</p></div>
<div class="para"><p>1. We concatenate<strong> x</strong> and<strong> y</strong> into a sequence [<strong>x</strong><em>,</em><strong> y</strong>]. For each position<em> i</em><em>′</em> of this sequence, we perform the following steps.</p></div>
<div class="para"><p>(a) We compute the embedding of the token at position<em> i</em><em>′</em>, and feed the resulting embedding as an initial representation into the stack of Transformer layers.</p></div>
<div class="para"><p>(b) In each Transformer layer, we pass the input representation through the self-attention network first and then through an FFN. In the self-attention network, the input representation is transformed into<strong> q</strong> <em>i</em><em>′</em>,<strong> k</strong> <em>i</em><em>′</em>, and<strong> v</strong> <em>i</em><em>′</em>. Then, we update the KV cache (<strong>K</strong><em>,</em><strong> V</strong>)using<strong> k</strong> <em>i</em><em>′</em> and<strong> v</strong> <em>i</em><em>′</em> (see Eqs. (5.8-5.9)). Then, we compute the output of the attention model by attending<strong> q</strong> <em>i</em><em>′</em> to (<strong>K</strong><em>,</em><strong> V</strong>) (see Eq. (5.7)).</p></div>
<div class="para"><p>(c) If<em> i</em><em>′</em><em> </em><em>&gt; m</em> (i.e.,<em> i</em> =<em> i</em><em>′</em><em> </em><em>−</em><em>m</em><em> ≥</em>0), we take the output of the Transformer stack and compute the token prediction probability Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>) via the Softmax layer (see Eq.(5.5)).</p></div>
<div class="para"><p>2. When reaching the end of the sequence, we obtain log Pr(<strong>y</strong><em>|</em><strong>x</strong>) by summing log Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)over<em> i</em><em> ∈</em>[1<em>, n</em>] (see Eq. (5.4)).</p></div>
<div class="para"><p><strong>5.1.2</strong> <strong>A Two-phase Framework</strong></p></div>
<div class="para"><p>As we have seen, language modeling is a standard autoregressive process, where each token is generated one at a time, conditioned on the previous tokens. For Transformers, this requires the model to maintain a KV cache that stores past representations, and attend the newly generated representation to this KV cache. If we think of the model Pr(<strong>y</strong><em>|</em><strong>x</strong>) from the perspective of computing the KV cache, it is natural to divide inference into two phases:</p></div>
<div class="para"><p>•<strong> Prefilling</strong>. The prefilling phase computes the KV cache for the input sequence<strong> x</strong>. It is called prefilling because the model prepares and stores the key-value pairs for each token</p></div>
<h2>208 Inference</h2>
<div class="para"><p>attention</p></div>
<div class="para"><p><strong>q</strong> <em>i</em><em>′</em>query</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><strong>k</strong> <em>i</em><em>′</em><em>−</em>1</p></div>
<div class="para"><p><strong>k</strong> 1</p></div>
<div class="para"><p><strong>k</strong> 2</p></div>
<div class="para"><p><strong>k</strong> <em>i</em><em>′</em></p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>′</em><em>−</em>1</p></div>
<div class="para"><p><strong>v</strong> 1</p></div>
<div class="para"><p><strong>v</strong> 2</p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>′</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p>KV Cache (positions 1 to<em> i</em><em>′</em><em> </em><em>−</em>1)</p></div>
<div class="para"><p>key value</p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>′</em><strong>k</strong> <em>i</em><em>′</em></p></div>
<div class="para"><p>Linear maps</p></div>
<div class="para"><p>Input</p></div>
<div class="para"><p>Input at position<em> i</em><em>′</em></p></div>
<h3>(a) Updating the KV Cache at Position i′</h3>
<div class="para"><p>attention</p></div>
<div class="para"><p><strong>q</strong> <em>i</em><em>′</em>+1 query</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><strong>k</strong> <em>i</em><em>′</em><em>−</em>1</p></div>
<div class="para"><p><strong>k</strong> <em>i</em><em>′</em>+1</p></div>
<div class="para"><p><strong>k</strong> 1</p></div>
<div class="para"><p><strong>k</strong> 2</p></div>
<div class="para"><p><strong>k</strong> <em>i</em><em>′</em></p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>′</em><em>−</em>1</p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>′</em>+1</p></div>
<div class="para"><p><strong>v</strong> 1</p></div>
<div class="para"><p><strong>v</strong> 2</p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>′</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p>KV Cache (positions 1 to<em> i</em><em>′</em>)</p></div>
<div class="para"><p>key value</p></div>
<div class="para"><p><strong>v</strong> <em>i</em><em>′</em>+1 <strong>k</strong> <em>i</em><em>′</em>+1</p></div>
<div class="para"><p>Linear maps</p></div>
<div class="para"><p>Input</p></div>
<div class="para"><p>Input at position<em> i</em><em>′</em> + 1</p></div>
<h3>(b) Updating the KV Cache at Position i′ + 1</h3>
<div class="para"><p><strong>Fig. 5.2:</strong> Illustration of the KV cache. We update the KV cache at a position, perform the attention operation, and then move to the next position to repeat the process.</p></div>
<h2>in the input before the actual inference begins. The process of prefilling in an LLM can be expressed as</h2>
<h2>cache = Dec kv(x) (5.10)</h2>
<h2>where Dec kv(·) is the decoding network (i.e., the same as Dec(·)), but it returns the KV</h2>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 209</p></div>
<div class="para"><p>cache in self-attention instead of the output representations. cache is a list, given by</p></div>
<div class="para"><p>cache=<em>{</em>cache 1<em>, ...,</em> cache <em>L</em><em>}</em>(5.11)</p></div>
<div class="para"><p>where cache <em>l</em> represents the key-value pairs for the<em> l</em>-th layer.</p></div>
<div class="para"><p>•<strong> Decoding</strong>. The decoding phase continues generating tokens based on the KV cache, as illustrated in Figure 5.2. When a new token is input into the decoder, we update the KV cache in each layer by adding the new key-value pair. The updated cache is then used for self-attention computation. The token generation stops when some stopping criterion is met,such as when the generated token is the end symbol. The goal of decoding is to find the best predicted sequence, which is given by</p></div>
<div class="para"><p>ˆ <strong>y</strong>=arg max <strong>y</strong> Pr(<strong>y</strong><em>|</em>cache)(5.12)</p></div>
<div class="para"><p>Here we use Pr(<strong>y</strong><em>|</em>cache) instead of Pr(<strong>y</strong><em>|</em><strong>x</strong>) to emphasize that the decoding process actually relies on the KV cache rather than<strong> x</strong>.</p></div>
<div class="para"><p>The prefilling and decoding processes are illustrated in Figure 5.3. Note that both these processes are autoregressive. However, as shown in Table 5.1, they differ in several aspects, which lead to very different implementations in practice.</p></div>
<div class="para"><p>In essence, while the underlying model of prefilling is based on token prediction, it can be considered an encoding process. This is because our goal is not to generate tokens, but to build a context representation (i.e., the KV cache) for the subsequent steps in the decoding phase. In this sense, it is similar to BERT, where we encode the input sequence into a sequence of contextualized token representations. On the other hand, unlike BERT which generates bidirectional sequence representations, prefilling is based on standard language modeling tasks, and is thus unidirectional.Note that, since the entire sequence<strong> x</strong> is input into the model all at once, all queries can be packed together and the self-attention operation is performed on<strong> x</strong> in parallel. Let<strong> Q</strong> be the queries that are packed into one matrix. The self-attention model in prefilling can be defined as</p></div>
<div class="para"><p>Att qkv(<strong>Q</strong><em>,</em><strong> K</strong><em>,</em><strong> V</strong>)=Softmax(<strong>QK</strong> T</p></div>
<div class="para"><p><em>√</em></p></div>
<div class="para"><p><em>d</em>+<strong> Mask</strong>)<strong>V</strong>(5.13)</p></div>
<div class="para"><p>where<strong> Q</strong><em>,</em><strong> K</strong><em>,</em><strong> V</strong><em> ∈</em>R <em>d</em><em>×</em>(<em>m</em>+1).<strong> Mask</strong><em> ∈</em>R(<em>m</em>+1)<em>×</em>(<em>m</em>+1) is a mask that ensures that each token only attends to itself and the tokens that precede it in the sequence. It is represented by setting the values in the mask corresponding to future tokens to a large negative number, for example, for the query<strong> q</strong> <em>i</em> and the key<strong> k</strong> <em>j</em>, we set the value of the entry (<em>i, j</em>) to<em> −∞</em>if<em> i &lt; j</em>. One advantage of processing the sequence with a single self-attention computation is that we can make better use of the parallel computing capabilities of modern GPUs, and so speed up prefilling. In general, the prefilling process is considered compute-bound. This is because merging multiple computational operations into one operation reduces the number of data transfers and the performance bottleneck usually comes from the computational capacity rather than memory bandwidth.</p></div>
<div class="para"><p>Decoding is a standard left-to-right text generation process. The token sequence is generated autoregressively by predicting one token at a time based on the KV cache. Each time a new token is generated, we need to attend it to previous tokens, following Eq. (5.7). Therefore, the decoding</p></div>
<h1>210 Inference</h1>
<h2>Transformer Decoder</h2>
<h2>· ·</h2>
<div class="para"><p><em>· · ·</em></p></div>
<h3>Self-attention in a Layer</h3>
<div class="para"><p><strong>q</strong> 0 <strong>q</strong> 1<em>· · ·</em><strong>q</strong> <em>m</em><em>−</em>1</p></div>
<h3>queries</h3>
<h3>keys values</h3>
<div class="para"><p><strong>k</strong> <em>m</em><em>−</em>1 <strong>v</strong> <em>m</em><em>−</em>1</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><strong>k</strong> 0 <strong>v</strong> 0</p></div>
<div class="para"><p><strong>k</strong> 1 <strong>v</strong> 1</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<h2>· ·</h2>
<h2>· ·</h2>
<h2>Embedding Layer</h2>
<h2>x 0 x 1 · · x m−1</h2>
<h2>Processed all at once</h2>
<h2>(a) Prefilling</h2>
<h3>Pr(y 1|x)Pr(y 2|x, y 1) · · · Pr(y n|x, y&amp;lt;n)</h3>
<h2>Softmax Layer</h2>
<h2>Transformer Decoder</h2>
<h2>· ·</h2>
<div class="para"><p><em>· · ·</em></p></div>
<h3>Self-attention in a Layer</h3>
<div class="para"><p><strong>q</strong> <em>m</em> <strong>q</strong> <em>m</em>+1<em>· · ·</em><strong>q</strong> <em>m</em>+<em>n</em></p></div>
<h3>queries</h3>
<h3>keys values</h3>
<div class="para"><p><strong>k</strong> <em>m</em><em>−</em>1 <strong>v</strong> <em>m</em><em>−</em>1</p></div>
<div class="para"><p><strong>k</strong> <em>m</em>+1 <strong>v</strong> <em>m</em>+1</p></div>
<div class="para"><p><strong>k</strong> <em>m</em>+<em>n</em> <strong>v</strong> <em>m</em>+<em>n</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><strong>k</strong> 0 <strong>v</strong> 0</p></div>
<div class="para"><p><strong>k</strong> 1 <strong>v</strong> 1</p></div>
<div class="para"><p><strong>k</strong> <em>m</em> <strong>v</strong> <em>m</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><em>· · ·</em></p></div>
<h2>· ·</h2>
<h2>· ·</h2>
<h2>Embedding Layer</h2>
<h2>x m y 1 · · y n−1</h2>
<h2>Processed step by step (n steps)</h2>
<h2>(b) Decoding (at the n-th step)</h2>
<h2>Fig. 5.3: Illustration of the prefilling and decoding processes. In prefilling, the entire input sequence is processed together and the KV cache is filled. In decoding, the LLM generates the output sequence step by step based on the prefilled KV cache.</h2>
<h1>process is memory-bound due to its frequent access to the KV cache. The cost of decoding grows significantly as more tokens are generated. In most cases, decoding is computationally more expensive than prefilling. Note that this is not just because, in decoding, the LLM generates tokens one by one and repeatedly updates the KV cache. As we will see in the following subsection,</h1>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 211</p></div>
<div class="para"><p>Prefilling Decoding</p></div>
<div class="para"><p>Goal Set up initial context<strong> x</strong>.Continue generating tokens <strong>y</strong> after the initial input.</p></div>
<div class="para"><p>All-at-once Visibility Tokens in<strong> x</strong> are presented all at once.</p></div>
<div class="para"><p>Tokens in<strong> y</strong> are presented sequentially, that is,predicting a token requires waiting for the previous tokens to be predicted first.</p></div>
<div class="para"><p>Context Use Build the context or encoded representation of the input.</p></div>
<div class="para"><p>Use the cached key-value pairs (from prefilling) to generate further tokens.</p></div>
<div class="para"><p>Resource Limitation Compute-bound Memory-bound</p></div>
<div class="para"><p>Computational Cost High Very High</p></div>
<div class="para"><p><strong>Table 5.1:</strong> Prefilling vs Decoding.</p></div>
<div class="para"><p>we may need to explore multiple different token sequences during decoding, which makes the problem more complex and increases its cost further.</p></div>
<div class="para"><p><strong>5.1.3</strong> <strong>Decoding Algorithms</strong></p></div>
<div class="para"><p>So far our discussion of LLM inference has primarily focused on the model computation problem,that is, how to compute Pr(<strong>y</strong><em>|</em><strong>x</strong>). Now we turn to the discussion of the search problem. The problem can be stated as: given an LLM Pr(<strong>y</strong><em>|</em><strong>x</strong>), how do we efficiently search for the best output sequence ˆ <strong>y</strong> given the input sequence<strong> x</strong> (or the generated KV cache)? Naively, we can consider all of the output sequences, compute the prediction probability for each, and then select the output sequence having the highest probability. This method can guarantee the globally optimal solution,but direct exhaustive search is impractical for LLMs as the number of possible output sequences grows exponentially with the length of<strong> y</strong>.</p></div>
<div class="para"><p>In practice, various heuristic search algorithms, such as greedy search and sampling-based search, are commonly employed to approximate the solution. Each of these methods offers tradeoffs between search quality and computational efficiency. The search problem, therefore, becomes a balancing act between exploration and exploitation, where the goal is to find an efficient strategy that produces high-quality outputs without exploring the entire space.</p></div>
<div class="para"><p>Before giving a more detailed discussion of these methods, let us first informally define what a search space is and how it is represented. In LLM inference, we define a hypothesis as a tuple of input and output sequences. Since<strong> x</strong> is fixed during inference, we can simply consider each hypothesis as an output sequence. The search space, denoted by<em> Y</em>, is then the set of all possible hypotheses (i.e., output sequences) that the model can generate. The search problem for LLM inference can be re-expressed as</p></div>
<div class="para"><p>Pr(<strong>y</strong><em>|</em><strong>x</strong>)(5.14)</p></div>
<div class="para"><p>ˆ <strong>y</strong>=arg max <strong>y</strong><em>∈</em><em>Y</em></p></div>
<div class="para"><p>In NLP,<em> Y</em> is commonly represented in a tree data structure to facilitate search. Figure 5.4 shows an example of the search tree resulting from a small vocabulary. In this example, a node</p></div>
<div class="para"><p>212 <em>Inference</em></p></div>
<div class="para"><p>Path: node 0<em> →</em>node 3<em> →</em>node 9<em> →</em>node 11<em> →</em>node 17 Output: cats are playful.</p></div>
<div class="para"><p>Probability:node 0<em> →</em>0 node 3<em> →</em>log Pr(&quot;cats&quot;<em>|</em><strong>x</strong>)node 9<em> →</em>log Pr(&quot;are&quot;<em>|</em><strong>x</strong><em>,</em> &quot;cats&quot;)node 11<em> →</em>log Pr(&quot;playful&quot;<em>|</em><strong>x</strong><em>,</em> &quot;cats are&quot;)node 17<em> →</em>log Pr(&quot;.&quot;<em>|</em><strong>x</strong><em>,</em> &quot;cats are playful&quot;)</p></div>
<div class="para"><p>16</p></div>
<div class="para"><p>playful</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>17</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>playful</p></div>
<div class="para"><p>playful</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>11</p></div>
<div class="para"><p>18</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>playful</p></div>
<div class="para"><p>cats</p></div>
<div class="para"><p>root 0</p></div>
<div class="para"><p>3</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>12</p></div>
<div class="para"><p>19</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>cats</p></div>
<div class="para"><p>cats</p></div>
<div class="para"><p>4</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>13</p></div>
<div class="para"><p>20</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>cats</p></div>
<div class="para"><p>is</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>10</p></div>
<div class="para"><p>14</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>is</p></div>
<div class="para"><p>is</p></div>
<div class="para"><p>15</p></div>
<div class="para"><p>is</p></div>
<div class="para"><p><strong>Fig. 5.4:</strong> A search tree for decoding. At each node, we expand the tree by considering all possible tokens, each leading to a new node representing a potential continuation of the text. Here we highlight a path through nodes 0, 3,9, 11, and 17. The path represents the output sequence “<em>cats are playful.</em>”, whose log-probability can be computed by accumulating the log-probabilities of these nodes.</p></div>
<div class="para"><p>represents a prefix subsequence that can be shared by many sequences. The search starts with the root of the tree, which can be regarded as the beginning of all sequences that can be generated 1.Each child node extends the prefix of its parent node by adding one token from the vocabulary to the sequence, along with the probability of predicting the token given the prefix. This process continues as each node further branches out into additional child nodes, each representing a new possible extension of the sequence with another token. The search tree thus grows deeper and wider, representing an ever-increasing number of potential sequences as more tokens are appended. This structure allows us to efficiently traverse through possible sequences, evaluating each in terms of the log-probability accumulated over the path from the root to that node. For example, in Figure 5.4, the path from the root to the node 17 corresponds to the output sequence“<em>Cats are playful.</em>”. The prediction log-probability log Pr(<strong>y</strong><em>|</em><strong>x</strong>) is the sum of the log-probabilities of all the nodes on this path.</p></div>
<div class="para"><p>In general, the search tree is organized as levels, where each level consists of all nodes that are the same distance from the root node. Thus, a breadth-first search over the tree essentially performs left-to-right generation of tokens. Nodes in the same level correspond to sequences of the same length. As the search progresses, new tokens are appended to these sequences, expanding them incrementally.</p></div>
<div class="para"><p>Let<em> Y</em> <em>i</em> be the set of the sequences that the LLM generates at step<em> i</em>.<em> Y</em> <em>i</em> can be obtained by expanding each sequence in<em> Y</em> <em>i</em><em>−</em>1 with all possible next tokens in the vocabulary<em> V</em> , given in the</p></div>
<div class="para"><p>1 Here, since the predictions in LLMs are based on<strong> x</strong>, we can think of the root as a representation of<strong> x</strong>.</p></div>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 213</p></div>
<div class="para"><p>following recursive form</p></div>
<div class="para"><p><em>Y</em> <em>i</em>=<em>Y</em> <em>i</em><em>−</em>1<em> ×</em><em> V</em>(5.15)</p></div>
<div class="para"><p>where<em> Y</em> <em>i</em><em>−</em>1<em> ×</em><em> V</em> denotes the Cartesian product of<em> Y</em> <em>i</em><em>−</em>1 and<em> V</em> (i.e., each sequence in<em> Y</em> <em>i</em><em>−</em>1 is concatenated with each token in<em> V</em> ). Note that if a sequence in<em> Y</em> <em>i</em><em>−</em>1 is complete (e.g., ending with the<em> ⟨</em>EOS<em>⟩</em>token), it will not be expanded any further. Let Ψ(<em>Y</em> <em>i</em>) be the set of all complete sequences in<em> Y</em> <em>i</em>. Then, the search space can be expressed as</p></div>
<div class="para"><p><em>Y</em>=Ψ(<em>Y</em> 1)[Ψ(<em>Y</em> 2)[<em>· · ·</em>[Ψ(<em>Y</em> <em>n</em> max)(5.16)</p></div>
<div class="para"><p>where<em> n</em> max is the maximum length of a sequence.</p></div>
<div class="para"><p>Most decoding algorithms follow this level-by-level search process. However,<em> Y</em> consists of an exponentially large number of sequences, and a direct search in such a vast space is computationally infeasible. Therefore, practical decoding algorithms often rely on strategies to prune the search space and avoid exploring low-quality sequences. For example, at each decoding step,<em> Y</em> <em>i</em> can be obtained in the following way</p></div>
<div class="para"><p><em>Y</em> <em>i</em>=Prune(<em>Y</em> <em>i</em><em>−</em>1<em> ×</em><em> V</em> )(5.17)</p></div>
<div class="para"><p>where Prune(<em>·</em>) is a function that selectively removes sequences less likely to result in high-quality outcomes. In general, we expect that<em> |</em><em>Y</em> <em>i</em><em>|</em><em> &lt;&lt;</em><em> |</em><em>Y</em> <em>i</em><em>−</em>1<em>| · |</em><em>V</em><em> |</em>. Thus we can drastically reduce the number of sequences under consideration at each step, ensuring that the computational load does not grow exponentially with the sequence length.</p></div>
<div class="para"><p>Next, we will introduce these decoding algorithms. Some of them have already been discussed in sequence-to-sequence models, while others are more commonly used in LLMs.</p></div>
<div class="para"><p><strong>5.1.3.1</strong> <strong>Greedy Decoding</strong></p></div>
<div class="para"><p>Greedy search (or greedy decoding) is one of the most widely used decoding methods in NLP,particularly in text generation tasks like machine translation. The idea behind greedy search is straightforward: at each step in generation, it selects the next token that has the highest prediction probability. For each sequence<strong> y</strong> =<em> y</em> 1<em>...y</em> <em>i</em><em> ∈</em><em>Y</em> <em>i</em><em>−</em>1<em> ×</em><em> V</em> , we can evaluate it using log Pr(<strong>y</strong><em>|</em><strong>x</strong>).This log-probability can be easily computed by noting that</p></div>
<div class="para"><p>log Pr(<strong>y</strong><em>|</em><strong>x</strong>)=log Pr(<em>y</em> 1<em>...y</em> <em>i</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p>=log Pr(<strong>y</strong><em>&lt;i</em><em>|</em><strong>x</strong>)|{z}accumulated up to the parent node</p></div>
<div class="para"><p>+log Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)|{z}newly computed for the current node</p></div>
<div class="para"><p>(5.18)</p></div>
<div class="para"><p>Here the first term is the sum of the log-probabilities of the path from the root to the parent node,which has been computed in the previous decoding steps. At step<em> i</em>, we only need to compute the second term which is the standard token prediction log-probability produced by the LLM.</p></div>
<div class="para"><p>214 <em>Inference</em></p></div>
<div class="para"><p>The “best” token at step<em> i</em> is then chosen as</p></div>
<div class="para"><p><em>y</em> top1 <em>i</em>=arg max <em>y</em> <em>i</em><em>∈</em><em>V</em> log Pr(<em>y</em> 1<em>...y</em> <em>i</em><em>|</em><strong>x</strong>)</p></div>
<div class="para"><p> log Pr(<strong>y</strong><em>&lt;i</em><em>|</em><strong>x</strong>)|{z}fixed wrt.<em> y</em> <em>i</em></p></div>
<div class="para"><p>+ log Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)</p></div>
<div class="para"><p>=arg max <em>y</em> <em>i</em><em>∈</em><em>V</em></p></div>
<div class="para"><p>=arg max <em>y</em> <em>i</em><em>∈</em><em>V</em> log Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)(5.19)</p></div>
<div class="para"><p>Thus, the “best” sequence generated up to step<em> i</em> is given by</p></div>
<div class="para"><p><strong>y</strong> top1=<em>y</em> 1<em>...y</em> <em>i</em><em>−</em>1 <em>y</em> top1 <em>i</em>(5.20)</p></div>
<div class="para"><p>Finally,<em> Y</em> <em>i</em> contains only this sequence</p></div>
<div class="para"><p><em>Y</em> <em>i</em>=<em>{</em><strong>y</strong> top1<em>}</em>(5.21)</p></div>
<div class="para"><p>The greedy choice in one decoding step is illustrated in Figure 5.5 (a). Greedy search offers computational efficiency and simplicity in implementation for LLM inference. Its primary disadvantage, however, lies in its suboptimal nature — high-quality sequences are likely pruned at early stages of decoding. Therefore, greedy search is appealing for tasks that demand speed and simplicity. For tasks that require better search results, alternative strategies such as beam search,which explores multiple potential paths simultaneously, are preferable.</p></div>
<div class="para"><p><strong>5.1.3.2</strong> <strong>Beam Decoding</strong></p></div>
<div class="para"><p>Beam search (or beam decoding) is a natural extension of greedy search. Instead of selecting the single most probable token at each step, beam search maintains a fixed number of the best candidates at each step, known as the “beam width”. See Figure 5.5 (b) for an illustration of beam search.</p></div>
<div class="para"><p>Let<em> K</em> be the beam width. Given a parent node, which corresponds to the prefix<em> y</em> 1<em>...y</em> <em>i</em><em>−</em>1, we can select the top-<em>K</em> next tokens by</p></div>
<div class="para"><p><em>{</em><em>y</em> top1 <em>i</em><em>, ..., y</em> top <em>K</em> <em>i</em><em>}</em>=argTopK <em>y</em> <em>i</em><em>∈</em><em>V</em> Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)(5.22)</p></div>
<div class="para"><p>where argTopK is a function that ranks the prediction probabilities of all possible next tokens and selects the top<em> K</em> candidates. Given these tokens, the top-<em>K</em> sequences for step<em> i</em> are given by</p></div>
<div class="para"><p><strong>y</strong> top1=<em>y</em> 1<em>...y</em> <em>i</em><em>−</em>1 <em>y</em> top1 <em>i</em>(5.23)...</p></div>
<div class="para"><p><strong>y</strong> top <em>K</em>=<em>y</em> 1<em>...y</em> <em>i</em><em>−</em>1 <em>y</em> top <em>K</em> <em>i</em>(5.24)</p></div>
<div class="para"><p>Then, we can define<em> Y</em> <em>i</em> as</p></div>
<div class="para"><p><em>Y</em> <em>i</em>=<em>{</em><strong>y</strong> top1<em>, ...,</em><strong> y</strong> top <em>K</em><em>}</em>(5.25)</p></div>
<h2>5.1 Prefilling and Decoding 215</h2>
<div class="para"><p>① Expansion</p></div>
<div class="para"><p>② Ranking</p></div>
<div class="para"><p>③ Output (1-best)</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>6</p></div>
<h3>cute 6 =⇒</h3>
<div class="para"><p>.</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>Pr =<em> .</em>34</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>on</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>Pr =<em> .</em>32</p></div>
<div class="para"><p>is 4 on</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>21</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>Pr =<em> .</em>12</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>01</p></div>
<h3>(a) Greedy search</h3>
<div class="para"><p>① Expansion</p></div>
<div class="para"><p>② Ranking</p></div>
<div class="para"><p>③ Output (<em>K</em>-best)</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>6</p></div>
<h3>=⇒</h3>
<div class="para"><p>.</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>Pr =<em> .</em>34</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>7</p></div>
<h3>=⇒</h3>
<div class="para"><p>beam width (<em>K</em>) = 3</p></div>
<div class="para"><p>on</p></div>
<div class="para"><p>on</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>Pr =<em> .</em>32</p></div>
<div class="para"><p>is 4 on</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>9</p></div>
<h3>=⇒</h3>
<div class="para"><p>ok</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>21</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>Pr =<em> .</em>12</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>01</p></div>
<h3>(b) Beam search</h3>
<div class="para"><p>① Expansion</p></div>
<div class="para"><p>② Ranking</p></div>
<div class="para"><p>③ Selection &amp; Sampling</p></div>
<div class="para"><p>④ Output</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>6</p></div>
<h3>=⇒</h3>
<div class="para"><p>.</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>Pr =<em> .</em>34</p></div>
<div class="para"><p>Pr =<em> .</em>39</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>7</p></div>
<h3>on 7 =⇒</h3>
<h3>=⇒</h3>
<div class="para"><p>on</p></div>
<div class="para"><p>on</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>Pr =<em> .</em>32</p></div>
<div class="para"><p>Pr =<em> .</em>36</p></div>
<div class="para"><p>is 4 on</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>9</p></div>
<h3>=⇒</h3>
<div class="para"><p>pruned</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>21</p></div>
<div class="para"><p>Pr =<em> .</em>25</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>Pr =<em> .</em>12</p></div>
<div class="para"><p>select top-<em>k</em> hypotheses (<em>k</em> = 3),renormalize their proababilities,and select one via sampling.</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>01</p></div>
<h3>(c) Top-k Sampling</h3>
<div class="para"><p>① Expansion</p></div>
<div class="para"><p>② Ranking</p></div>
<div class="para"><p>③ Selection &amp; Sampling</p></div>
<div class="para"><p>④ Output</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>6</p></div>
<h3>=⇒</h3>
<div class="para"><p>.</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>Pr =<em> .</em>34</p></div>
<div class="para"><p>Pr =<em> .</em>51</p></div>
<div class="para"><p>6</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>7</p></div>
<h3>on 7 =⇒</h3>
<h3>=⇒</h3>
<div class="para"><p>on</p></div>
<div class="para"><p>on</p></div>
<div class="para"><p>cute</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>ok</p></div>
<div class="para"><p>Pr =<em> .</em>32</p></div>
<div class="para"><p>Pr =<em> .</em>49</p></div>
<div class="para"><p>is 4 on</p></div>
<div class="para"><p>7</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>21</p></div>
<div class="para"><p>select top-ranked hypotheses whose probability sum<em> ≥</em><em>p</em> = 0<em>.</em>6,renormalize their proababilities,and select one via sampling.</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>8</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>are</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>Pr =<em> .</em>12</p></div>
<div class="para"><p>9</p></div>
<div class="para"><p>5</p></div>
<div class="para"><p>.</p></div>
<div class="para"><p>pruned</p></div>
<div class="para"><p>sick</p></div>
<div class="para"><p>Pr =<em> .</em>01</p></div>
<h3>(d) Top-p Sampling</h3>
<div class="para"><p><strong>Fig. 5.5:</strong> Illustrations of greedy decoding, beam decoding, top-<em>k</em> decoding and top-<em>p</em> decoding methods (in one decoding step).</p></div>
<h2>We can adjust the beam width K to balance search efficiency and accuracy. But a very large beam width might not be helpful. In many practical applications, selecting a relatively small number for K, such as K = 2 or K = 4, is often sufficient to achieve satisfactory performance in LLM inference.</h2>
<div class="para"><p>216 <em>Inference</em></p></div>
<div class="para"><p><strong>5.1.3.3</strong> <strong>Sampling-based Decoding</strong></p></div>
<div class="para"><p>Both greedy and beam search generate deterministic outputs, that is, given an LLM, the output of the model will always be the same every time it processes the same input. The deterministic nature of greedy and beam search ensures predictability and reliability in applications where consistent outcomes are critical, such as in formal document generation, where varying outputs could cause confusion or errors. On the other hand, one disadvantage of these methods is the lack of diversity and flexibility. For example, in creative tasks like story generation or conversational agents,generic or repetitive outputs generated by deterministic systems are often less engaging.</p></div>
<div class="para"><p>To add variation into LLM outputs, we can use sampling-based decoding methods. There are two commonly used methods.</p></div>
<div class="para"><p>•<strong> Top-</strong><em>k</em><strong> Sampling</strong>. This method selects the next token from the top-<em>k</em> most likely candidates at each step of the generation process [Fan et al., 2018]. Let<em> V</em><em> i</em> be the selection pool for top-<em>k</em> sampling. We can define it as</p></div>
<div class="para"><p><em>V</em><em> i</em>=<em>{</em><em>y</em> top1 <em>i</em><em>, ..., y</em> top <em>k</em> <em>i</em><em>}</em>(5.26)</p></div>
<div class="para"><p>where<em> {</em><em>y</em> top1 <em>i</em><em>, ..., y</em> top <em>k</em> <em>i</em><em>}</em> are the top-<em>k</em> tokens selected based on their prediction probabilities (see Eq. (5.22)). Once the selection pool is determined, we recompute the prediction probability distribution over<em> V</em><em> i</em>. One of the simplest ways to do this is to renormalize their probabilities:</p></div>
<div class="para"><p>Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)=Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)P <em>y</em> <em>j</em><em>∈</em><em>V</em><em> i</em> Pr(<em>y</em> <em>j</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)(5.27)</p></div>
<div class="para"><p>Alternatively, we can calculate the distribution by using the Softmax function:</p></div>
<div class="para"><p>Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)=exp(<em>u</em> <em>y</em> <em>i</em>)P <em>y</em> <em>j</em><em>∈</em><em>V</em><em> i</em> exp(<em>u</em> <em>y</em> <em>j</em>)(5.28)</p></div>
<div class="para"><p>where<em> u</em> <em>y</em> <em>i</em> is the logit for token<em> y</em> <em>i</em>. Then, we sample a token ¯<em>y</em> <em>i</em> from this distribution:</p></div>
<div class="para"><p>¯<em>y</em> <em>i</em><em>∼</em>Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)(5.29)</p></div>
<div class="para"><p>The corresponding sequence is ¯<strong>y</strong> =<em> y</em> 1<em>...y</em> <em>i</em><em>−</em>1¯<em>y</em> <em>i</em>, and<em> Y</em> <em>i</em> is given by</p></div>
<div class="para"><p><em>Y</em> <em>i</em>=<em>{</em>¯<strong>y</strong><em>}</em>(5.30)</p></div>
<div class="para"><p>•<strong> Top-</strong><em>p</em><strong> Sampling</strong>. This sampling method, also known as<strong> nucleus sampling</strong>, follows a procedure similar to that of top-<em>k</em> sampling. Instead of drawing from a fixed size candidate pool, it selects the next token from the smallest set of tokens that together have a cumulative probability higher than a predefined threshold<em> p</em> [Holtzman et al., 2020]. In this way we prevent the prediction from choosing from low-probability tokens in the long tail that could lead to incoherent or nonsensical outputs. To obtain the candidate pool in the top-<em>p</em> sampling method, we can sort all tokens by their predicted probabilities. Then, starting with the token with the highest probability, we continue to add tokens to the candidate pool until the</p></div>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 217</p></div>
<div class="para"><p>(a)<em> β</em> = 0<em>.</em>1(b)<em> β</em> = 0<em>.</em>8(c)<em> β</em> = 2<em>.</em>0</p></div>
<div class="para"><p><strong>Fig. 5.6:</strong> Histogram estimates of the distributions generated by the Softmax function with different values of the temperature parameter<em> β</em>.</p></div>
<div class="para"><p>cumulative probability of the tokens in the pool reaches or exceeds<em> p</em> (we denote the size of the candidate pool at this point as<em> k</em> <em>p</em>). The candidate pool can then be expressed as</p></div>
<div class="para"><p><em>V</em><em> i</em>=<em>{</em><em>y</em> top1 <em>i</em><em>, ..., y</em> top <em>k</em> <em>p</em> <em>i</em><em>}</em>(5.31)</p></div>
<div class="para"><p>The subsequent steps, such as the renormalization of the distribution and sampling, are the same as in the top-<em>k</em> sampling method (see Eqs.(5.27-5.30)).</p></div>
<div class="para"><p>See Figure 5.5 (c-d) for illustrations of the top-<em>k</em> and top-<em>p</em> sampling methods. By limiting the choices to a smaller set of high-probability tokens, these methods strike a balance between randomness and coherence. They allow for more diverse outputs while still maintaining a reasonable level of relevance and fluency. However, the value of<em> k</em> or<em> p</em> must be carefully chosen: if<em> k</em> or<em> p</em> is too small, the output may still be overly deterministic (more like greedy decoding), and if<em> k</em> or<em> p</em> is too large, the LLM might produce degenerate outputs.</p></div>
<div class="para"><p>In order to further control the randomness of the token selection process, the renormalized distribution Pr(<em>·</em>) is typically obtained by using the Softmax function with the temperature parameter,given by</p></div>
<div class="para"><p>Pr(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)=exp(<em>u</em> <em>y</em> <em>i</em><em>/β</em>)P <em>y</em> <em>j</em><em>∈</em><em>V</em><em> i</em> exp(<em>u</em> <em>y</em> <em>j</em><em>/β</em>)(5.32)</p></div>
<div class="para"><p>Here<em> β</em> is a temperature parameter<em> β</em> that controls the sharpness of the probability distribution derived from logits. In Figure 5.6, we show simple examples involving distributions generated by the above function with different temperatures. When the temperature is set to a higher value, the resulting probability distribution becomes more uniform, as the differences between the logits are diminished. This means that each token in the candidate pool has a more equal chance of being selected, leading to greater diversity in the generated output. By contrast, when the temperature is set to a lower value, the distribution becomes sharper, making the high-probability tokens even more likely to be chosen, which often results in more deterministic outputs. For example, if we set <em>p</em> to 1 and<em> β</em> to a very small number (approaching zero), the top-<em>p</em> sampling method will become equivalent to the greedy search method.</p></div>
<div class="para"><p><strong>5.1.3.4</strong> <strong>Decoding with Penalty Terms</strong></p></div>
<div class="para"><p>One common improvement to decoding methods in text generation is to modify the search objective. For example, one can replace maximum a posteriori (MAP) decoding with minimum Bayes risk (MBR) decoding [Kumar and Byrne, 2004], where the focus shifts from selecting the single</p></div>
<div class="para"><p>218 <em>Inference</em></p></div>
<div class="para"><p>most probable output to choosing an output that minimizes the expected risk over a distribution of possible outputs. Here we explore methods that incorporate penalty terms into decoding. These methods offer a simple but effective way to make decoding more controllable.</p></div>
<div class="para"><p>Recall from Eq. (5.14) that the goal of decoding is to maximize the likelihood of the output sequence. With penalty terms, the objective is extended to include additional factors that penalize or reward certain behaviors in the generated text. A general form of the new objective is given by</p></div>
<div class="para"><p> Pr(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em><em>λ</em><em> ·</em> Penalty(<strong>x</strong><em>,</em><strong> y</strong>)(5.33)</p></div>
<div class="para"><p>ˆ <strong>y</strong>=arg max <strong>y</strong><em>∈Y</em></p></div>
<div class="para"><p>where Penalty(<strong>x</strong><em>,</em><strong> y</strong>) is a function that quantifies the degree to which the generated sequence<strong> y</strong> violates certain constraints or exhibits undesirable behaviors given the input<strong> x</strong>. The design of Penalty(<em>·</em>) is very flexible, thus allowing us to incorporate a wide range of constraints or prior knowledge into it. Below, we present some common types of penalty functions.</p></div>
<div class="para"><p>•<strong> Repetition Penalty</strong>. A repetition penalty discourages the model from generating repetitive or redundant text. The penalty function might measure the frequency of repeated tokens or phrases in the generated sequence and impose a penalty proportional to their occurrence.</p></div>
<div class="para"><p>•<strong> Length Penalty</strong>. A length penalty ensures that the generated sequence adheres to a desired length. For example, in text summarization tasks, the penalty function could penalize outputs that are too short or too long.</p></div>
<div class="para"><p>•<strong> Diversity Penalty</strong>. A diversity penalty promotes variation in the generated text. For example, in beam search, we can measure the similarity between generated hypotheses, and encourage the model to explore different hypotheses.</p></div>
<div class="para"><p>•<strong> Constraint-based Penalty</strong>. A constraint-based penalty enforces specific constraints related to the content or style of the generated text. For example, in machine translation, the penalty function could penalize outputs that deviate from a desired tone or terminology.</p></div>
<div class="para"><p>In general, we can consider Penalty(<strong>x</strong><em>,</em><strong> y</strong>) as a function that defines the cost of generating the surface form of the output sequence<strong> y</strong> given the input sequence<strong> x</strong>. Alternatively, this function can be defined to assess the hidden states of an LLM when generating<strong> y</strong>. For example, Su et al. [2022]develop a penalty term that calculates the maximum distance between the representation of the predicted token and the representations of the previously generated tokens. Therefore, the search objective will penalize degenerated outputs, such as texts with many repetitions.</p></div>
<div class="para"><p>The method described in Eq. (5.33) is general and can be easily adapted to different search algorithms. For example, in greedy search, we can keep the single sequence that maximizes Pr(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em><em>λ</em><em> ·</em> Penalty(<strong>x</strong><em>,</em><strong> y</strong>) at each decoding step; in sampling-based search, we can rank and select the top-ranked sequences based on Pr(<strong>y</strong><em>|</em><strong>x</strong>)<em> −</em><em>λ</em><em> ·</em> Penalty(<strong>x</strong><em>,</em><strong> y</strong>) to form the candidate pool.</p></div>
<div class="para"><p><strong>5.1.3.5</strong> <strong>Speculative Decoding</strong></p></div>
<div class="para"><p><strong>Speculative decoding</strong> stems from the concept of<strong> speculative execution</strong>, where a system makes educated guesses about future actions and performs them in advance. If the guess is correct, the</p></div>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 219</p></div>
<div class="para"><p>results are immediately available, which speeds up processing. In the case of LLM inference,suppose we have two models. One is a smaller, faster model (called draft model), and the other is the full, more accurate model (called verification model). These two models represent two baselines in LLM inference: the draft model is efficient but not very accurate; the verification model is usually the one we want to run, but it is very slow. Given a prefix, we first use the draft model to speculatively predict a sequence of likely future tokens. This is a standard autoregressive decoding process, but it is still fast in practice due to the high efficiency of the draft model. Then,the verification model evaluates the speculated tokens in parallel. It checks whether the predicted tokens are correct or need to be adjusted. Note that, since we can deal with these tokens all at once, the verification can be done in a single step for all the tokens simultaneously, rather than in a token-by-token manner. If the speculated tokens are correct, they are accepted, and the process continues with the next set of tokens. If they are incorrect, the incorrect speculations are discarded,and the verification model is used to generate the correct tokens.</p></div>
<div class="para"><p>To be more specific, let us see the speculative decoding method presented in Leviathan et al.[2023]’s work. In this method, the draft model is a small language model, denoted by Pr <em>q</em>(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>),while the verification model is a normal LLM, denoted by Pr <em>p</em>(<em>y</em> <em>i</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>). The goal is that, given a prefix, we use the draft model to autoregressively predict up to<em> τ</em> tokens. The verification model is then employed to generate the last token at the point where errors begin to occur in the speculative predictions. Figure 5.7 illustrates one step in this decoding process.</p></div>
<div class="para"><p>The speculative decoding algorithm can be summarized as follows.</p></div>
<div class="para"><p>• Given the prefix [<strong>x</strong><em>,</em><strong> y</strong><em>≤</em><em>i</em>], we use the draft model to predict the next<em> τ</em> consecutive tokens,denoted by<em> {</em>ˆ <em>y</em> <em>i</em>+1<em>, ...,</em> ˆ <em>y</em> <em>i</em>+<em>τ</em><em>}</em>. This is a token-by-token generation process, given by</p></div>
<div class="para"><p>ˆ <em>y</em> <em>i</em>+<em>t</em>=arg max <em>y</em> <em>i</em>+<em>t</em> Pr <em>q</em>(<em>y</em> <em>i</em>+<em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>≤</em><em>i</em><em>,</em> ˆ <em>y</em> <em>i</em>+1<em>...</em>ˆ <em>y</em> <em>i</em>+<em>t</em><em>−</em>1)(5.34)</p></div>
<div class="para"><p>• We evaluate<em> {</em>ˆ <em>y</em> <em>i</em>+1<em>, ...,</em> ˆ <em>y</em> <em>i</em>+<em>τ</em><em>}</em> using the verification model, that is, we compute<em> {</em>Pr <em>p</em>(ˆ <em>y</em> <em>i</em>+1<em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>≤</em><em>i</em>)<em>, ...,</em> Pr <em>p</em>(ˆ <em>y</em> <em>i</em>+<em>τ</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>≤</em><em>i</em><em>,</em> ˆ <em>y</em> <em>i</em>+1<em>...</em>ˆ <em>y</em> <em>i</em>+<em>τ</em><em>−</em>1)<em>}</em>. Note that we can compute these probabilities in parallel, and so this verification step is efficient.</p></div>
<div class="para"><p>• We determine the maximum number of accepted speculated tokens. In order to keep the notation uncluttered, we denote Pr <em>q</em>(ˆ <em>y</em> <em>i</em>+<em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>≤</em><em>i</em><em>,</em> ˆ <em>y</em> <em>i</em>+1<em>...</em>ˆ <em>y</em> <em>i</em>+<em>t</em><em>−</em>1) and Pr <em>p</em>(ˆ <em>y</em> <em>i</em>+<em>t</em><em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>≤</em><em>i</em><em>,</em> ˆ <em>y</em> <em>i</em>+1<em>...</em>ˆ <em>y</em> <em>i</em>+<em>t</em><em>−</em>1)simply by<em> q</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>) and<em> p</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>), respectively. We then define that, if<em> q</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>)<em> ≤</em><em>p</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>), then we accept this speculation. By contrast, if<em> q</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>)<em> &gt; p</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>), we reject this speculation with probability 1<em> −</em><em>p</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>)</p></div>
<div class="para"><p><em>q</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>). Starting from ˆ <em>y</em> <em>i</em>+1, the maximum number of accepted consecutive speculated tokens is defined as</p></div>
<div class="para"><p><em>n</em> <em>a</em>=min<em>t</em><em> −</em>1<em>|</em>1<em> ≤</em><em>t</em><em> ≤</em><em>τ, r</em> <em>t</em><em> &gt; </em><em>p</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>)</p></div>
<div class="para"><p></p></div>
<div class="para"><p>(5.35)</p></div>
<div class="para"><p><em>q</em>(ˆ <em>y</em> <em>i</em>+<em>t</em>)</p></div>
<div class="para"><p>where<em> r</em> <em>t</em> is a variable drawn from the uniform distribution<em> U</em>(0<em>,</em> 1).</p></div>
<div class="para"><p>• Given<em> n</em> <em>a</em>, we keep the speculated tokens<em> {</em>ˆ <em>y</em> <em>i</em>+1<em>, ...,</em> ˆ <em>y</em> <em>i</em>+<em>n</em> <em>a</em><em>}</em>. We then use the verification model to make a new prediction at<em> i</em> +<em> n</em> <em>a</em> + 1</p></div>
<div class="para"><p>¯<em>y</em> <em>i</em>+<em>n</em> <em>a</em>+1=arg max <em>y</em> <em>i</em>+<em>ns</em>+1 Pr <em>p</em>(<em>y</em> <em>i</em>+<em>n</em> <em>s</em>+1<em>|</em><strong>x</strong><em>,</em><strong> y</strong><em>≤</em><em>i</em><em>,</em> ˆ <em>y</em> <em>i</em>+1<em>...</em>ˆ <em>y</em> <em>i</em>+<em>n</em> <em>s</em>)(5.36)</p></div>
<div class="para"><p>220 <em>Inference</em></p></div>
<div class="para"><p>Context (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)ˆ <em>y</em> <em>i</em>+1 ˆ <em>y</em> <em>i</em>+2 ˆ <em>y</em> <em>i</em>+3 ˆ <em>y</em> <em>i</em>+4 ˆ <em>y</em> <em>i</em>+5</p></div>
<div class="para"><p>predict</p></div>
<div class="para"><p>Draft Model Pr <em>q</em>(<em>·</em>)</p></div>
<div class="para"><p>(a) Predict the next<em> τ</em> tokens given the context using the draft model (<em>τ</em> = 5)</p></div>
<div class="para"><p>Evaluation Model Pr <em>p</em>(<em>·</em>)</p></div>
<div class="para"><p>evaluate</p></div>
<div class="para"><p>Context (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)ˆ <em>y</em> <em>i</em>+1 ˆ <em>y</em> <em>i</em>+2 ˆ <em>y</em> <em>i</em>+3 ˆ <em>y</em> <em>i</em>+4 ˆ <em>y</em> <em>i</em>+5</p></div>
<div class="para"><p>Draft Model Pr <em>q</em>(<em>·</em>)</p></div>
<div class="para"><p>(b) Evaluate the predicted tokens using the evaluation model</p></div>
<div class="para"><p>Evaluation Model Pr <em>p</em>(<em>·</em>)</p></div>
<div class="para"><p>accepted rejected</p></div>
<div class="para"><p>Context (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)ˆ <em>y</em> <em>i</em>+1 ˆ <em>y</em> <em>i</em>+2 ˆ <em>y</em> <em>i</em>+3 ˆ <em>y</em> <em>i</em>+4 ˆ <em>y</em> <em>i</em>+5</p></div>
<div class="para"><p>Draft Model Pr <em>q</em>(<em>·</em>)</p></div>
<div class="para"><p>(c) Determine the number of accepted tokens</p></div>
<div class="para"><p>Evaluation Model Pr <em>p</em>(<em>·</em>)</p></div>
<div class="para"><p>Context (<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>)ˆ <em>y</em> <em>i</em>+1 ˆ <em>y</em> <em>i</em>+2 ˆ <em>y</em> <em>i</em>+3¯<em>y</em> <em>i</em>+4</p></div>
<div class="para"><p>Draft Model Pr <em>q</em>(<em>·</em>)</p></div>
<div class="para"><p>(d) Predict a new token following the accepted tokens using the evaluation model</p></div>
<div class="para"><p><strong>Fig. 5.7:</strong> Illustration of one step of speculative decoding. The goal is to predict as many next tokens as possible using the draft model. There are four sub-steps. Given the context, we first use the draft model to predict the next<em> τ</em> tokens.Then, we evaluate these predictions in parallel using the evaluation model. Next, we determine the maximum number of predicted tokens that can be accepted. Finally, we use the evaluation model to predict a new token following these accepted tokens.</p></div>
<div class="para"><p>• Above, we have described one step of speculative decoding. The result sequence (including both the context and predicted tokens) is illustrated as follows</p></div>
<div class="para"><p><em>5.1 Prefilling and Decoding</em> 221</p></div>
<div class="para"><p>[<strong>x</strong><em>,</em><strong> y</strong><em>&lt;i</em>]ˆ <em>y</em> <em>i</em>+1<em>...</em>ˆ <em>y</em> <em>i</em>+<em>n</em> <em>a</em>¯<em>y</em> <em>i</em>+<em>n</em> <em>a</em>+1</p></div>
<div class="para"><p>Context <em>n</em> <em>a</em> tokens predicted using the draft model</p></div>
<div class="para"><p>One token predicted using the verification model</p></div>
<div class="para"><p>Once we have finished this step, we add the predicted tokens<em> {</em>ˆ <em>y</em> <em>i</em>+1<em>, ...,</em> ˆ <em>y</em> <em>i</em>+<em>n</em> <em>a</em><em>,</em> ¯<em>y</em> <em>i</em>+<em>n</em> <em>a</em>+1<em>}</em> to the context, and repeat the above process.</p></div>
<div class="para"><p>In practice, we usually wish to use a smaller draft model so that predicting<em> {</em>ˆ <em>y</em> <em>i</em>+1<em>, ...,</em> ˆ <em>y</em> <em>i</em>+<em>n</em> <em>a</em><em>}</em>would be computationally cheaper. But a very small draft model is less accurate and can result in smaller<em> n</em> <em>a</em>. We therefore need to carefully select the draft model to make the trade-off between the computational efficiency and accuracy.</p></div>
<div class="para"><p><strong>5.1.3.6</strong> <strong>Stopping Criteria</strong></p></div>
<div class="para"><p>Stopping criteria are a critical component of LLM inference. They typically involve rules or conditions that specify when the model should stop generating text during decoding. Most LLMs are trained to generate an end-of-sequence token (e.g.,<em> ⟨</em>EOS<em>⟩</em>or<em> ⟨</em><em>/s</em><em>⟩</em>) to signal the end of the generated text. So one of the simplest strategies is that the generation process stops when this token is produced. For beam search, which explores multiple hypotheses simultaneously, the process can continue until a given number of complete sequences have been generated.</p></div>
<div class="para"><p>In practical applications, it will generally be undesirable to generate very long sequences,and so we need to reduce the decoding cost and unnecessary verbosity. One commonly-used stopping criterion is the maximum length of the output. The model stops generating text once it has produced a predetermined number of tokens. Alternatively, we can stop the decoding based on the real cost, such as the computational resources or time constraints. For example, in realtime applications like chatbots, decoding may need to stop after a certain time limit to ensure responsiveness.</p></div>
<div class="para"><p>Another approach is to design stopping criteria based on the behavior of LLMs. For example,decoding can be stopped if the probability of predicting the next token falls below a certain threshold. In addition to probability-based stopping, a repetition detection module can be implemented to trigger the model to stop if it begins repeating tokens or phrases beyond a predefined limit. This helps prevent redundant or incoherent outputs.</p></div>
<div class="para"><p><strong>5.1.4</strong> <strong>Evaluation Metrics for LLM Inference</strong></p></div>
<div class="para"><p>Evaluating the performance of LLMs during inference involves a variety of metrics to assess how well these models meet desired standards, such as accuracy, robustness, usability, and efficiency.As with most NLP systems, we can evaluate LLMs using accuracy-based metrics, such as perplexity and F1 score. We can also examine their robustness by testing how well they handle ambiguous or challenging inputs, including adversarial, perturbed, or out-of-distribution data. Additionally,usability can be assessed by measuring how well the generated outputs align with user expectations in terms of fluency, coherence, relevance, and diversity. Human evaluators can rate the naturalness of the text or assess whether the responses are contextually appropriate and logically consistent. Ethical and fairness metrics can also be included to ensure LLMs avoid perpetuating biases or generating harmful content.</p></div>
<div class="para"><p>222 <em>Inference</em></p></div>
<div class="para"><p>All of the evaluation metrics mentioned above essentially focus on assessing the quality of the outputs. Given the high cost of deploying and applying LLMs, efficiency metrics are also very important for practitioners. Below are some commonly used efficiency metrics [Nvidia, 2025]:</p></div>
<div class="para"><p>•<strong> Request Latency</strong>. This metric measures the total time taken from when a request is sent to the LLM until the complete response is received. This includes the time taken for data transmission, processing by the model, and the return of the output to the user.</p></div>
<div class="para"><p>•<strong> Throughput</strong>. It refers to the number of tokens or requests the model can process per second.</p></div>
<div class="para"><p>•<strong> Time to First Token</strong> (TTFT). This metric measures the time it takes from the beginning of a request being sent to the generation of the first token of the response. If data transmission does not consume too much time, then TTFT is mainly the time for prefilling and predicting the first token.</p></div>
<div class="para"><p>•<strong> Inter-token Latency</strong> (ITL). This metric refers to the time taken to generate each subsequent token after the first one. It reflects the efficiency of the decoding process.</p></div>
<div class="para"><p>•<strong> Tokens Per Second</strong> (TPS). This metric quantifies the number of tokens that the model can generate per second.</p></div>
<div class="para"><p>•<strong> Resource Utilization</strong>. This involves measuring the computational resource usage (e.g.,CPU and GPU utilization) and memory consumption of the model during inference.</p></div>
<div class="para"><p>In addition to these metrics, energy efficiency and cost efficiency are practical considerations for deploying LLMs at scale. Energy efficiency measures the amount of electrical power consumed by the model during inference. Cost efficiency, on the other hand, evaluates the total expenses related to deploying and maintaining the model.</p></div>
<div class="para"><p>In general, choosing the right evaluation metrics depends on the specific task and application.While quality-focused metrics are essential for assessing LLMs, efficiency metrics are equally crucial for their effective deployment in real-world applications. A comprehensive evaluation framework should include both sets of metrics to accurately estimate an LLM’s performance and practicality.</p></div>
<h3>5.2 Efficient Inference Techniques</h3>
<div class="para"><p>In practical applications, we often wish a system to be as efficient as possible. For LLM inference,this typically involves two types of improvements: reducing memory requirements and accelerating the system. For example, we can modify the Transformer architecture to avoid memory explosion when processing very long input sequences. Another example is that we can compress input sequences to reduce computational overhead while preserving their semantic information. In addition, techniques like quantization and pruning can be employed to further optimize memory usage and inference speed.</p></div>
<div class="para"><p>Efficient inference is a wide-ranging topic that overlaps with several sub-fields of LLMs, such as architecture design and model compression. Most of these topics have been covered in previous</p></div>
<div class="para"><p><em>5.2 Efficient Inference Techniques</em> 223</p></div>
<div class="para"><p>chapters. For example, in Chapter 2, we discussed efficient Transformer architectures and longcontext LLMs; and in Chapter 3, we discussed prompt compression methods for reducing prompt length. In this section, we focus on techniques that are commonly used in LLM deployment and serving.</p></div>
<div class="para"><p><strong>5.2.1</strong> <strong>More Caching</strong></p></div>
<div class="para"><p>In real-world applications, it is common practice to store frequent requests and their corresponding responses in a cache. When a new request hits the cache, the system can retrieve the response directly from the cache instead of recomputing the result. One straightforward implementation is a key-value datastore (e.g., a hash table) that maps input sequences to their LLM-generated output sequences. In the simplest case, we can collect frequent queries, generate their responses using the LLM, and store these query-response pairs in the datastore. This creates a basic sequence-level caching mechanism that allows the system to bypass LLM computation when the input sequence exactly matches a cached query.</p></div>
<div class="para"><p>A straightforward extension of the caching mechanism is to cache prefixes and their corresponding hidden states. Given an input sequence<strong> x</strong> in a dataset<em> D</em>, we can process it as in the standard prefilling phase. Thus, we obtain a sequence of prefixes and their corresponding KV cache states:</p></div>
<div class="para"><p><em>x</em> 0 (<strong>x</strong><em>&lt;</em>1)<em>⇒</em>cache<em>&lt;</em>1 <em>x</em> 0 <em>x</em> 1 (<strong>x</strong><em>&lt;</em>2)<em>⇒</em>cache<em>&lt;</em>2<em>...</em></p></div>
<div class="para"><p><em>x</em> 0 <em>x</em> 1<em>...x</em> <em>m</em><em>−</em>1 (<strong>x</strong><em>&lt;m</em>)<em>⇒</em>cache<em>&lt;m</em></p></div>
<div class="para"><p>where cache<em>&lt;i</em> denotes the KV cache for the prefix<strong> x</strong><em>&lt;i</em> (see also Eq. (5.10)). All these mappings can be stored in the prefix cache for efficient reuse.</p></div>
<div class="para"><p>When processing a new sequence that shares a common prefix with a previously seen sequence in<em> D</em>, we can load the corresponding cached hidden states instead of recomputing them. Specifically, if a new input<strong> x</strong><em>′</em> has<strong> x</strong><em>&lt;k</em> (i.e.,<strong> x</strong><em>′</em><em>&lt;k</em> =<strong> x</strong><em>&lt;k</em> for some<em> k</em><em> ≤</em><em>m</em>), we can initialize the KV cache with cache<em>&lt;k</em> and only compute the hidden states for the remaining tokens<strong> x</strong><em>′</em><em>≥</em><em>k</em>.</p></div>
<div class="para"><p>As usual, we can maintain a key-value datastore that maps frequently encountered prefixes to their precomputed KV caches. The lookup can be performed using a hash of the prefix tokens,allowing constant-time access to the cached states. Care must be taken to manage memory usage,as storing all possible prefixes may be infeasible for large datasets. Practical systems often employ least recently used (LRU) caching methods or other strategies to balance between computational savings and memory constraints.</p></div>
<div class="para"><p><strong>5.2.2</strong> <strong>Batching</strong></p></div>
<div class="para"><p>Batching in LLM inference refers to the process of processing multiple input sequences simultaneously as a group (called a batch) rather than one at a time. Because modern GPUs excel at parallel processing, batching allows them to compute multiple sequences in a single forward pass, keeping the hardware fully occupied. Therefore, when serving LLMs at scale, batching is important for</p></div>
<h2>224 Inference</h2>
<div class="para"><p>prefilling decoding</p></div>
<div class="para"><p>prefilling decoding</p></div>
<div class="para"><p>1 1 1 1 1 pad 1 1 1 1</p></div>
<div class="para"><p>2 2 2 2 pad pad 2 2 2</p></div>
<div class="para"><p>3 3 3 pad pad pad 3 3</p></div>
<div class="para"><p>4 4 4 4 4 4 4 4 4 4 4 4</p></div>
<div class="para"><p>(a) batch size = 1</p></div>
<div class="para"><p>(b) batch size = 4</p></div>
<div class="para"><p>transfer the KV cache</p></div>
<div class="para"><p>engine 1 engine 2</p></div>
<div class="para"><p>prefilling decoding</p></div>
<div class="para"><p>prefilling decoding</p></div>
<div class="para"><p>1 1 1 1 1 pad 1 1 1 1 1 1</p></div>
<div class="para"><p>1 1 1 1 1 pad 1 1 1 1</p></div>
<div class="para"><p>2 2 2 2 2 pad 2 2 2 2 2</p></div>
<div class="para"><p>2 2 2 3 3 3 2 2 2</p></div>
<div class="para"><p>3 3 3 3 3 3 3 3 3 3 3</p></div>
<div class="para"><p>5 5 5 5 4 4 3 3 3 3</p></div>
<div class="para"><p>4 4 4 4 4 4 4 4 4 4 4 4</p></div>
<div class="para"><p>6 6 6 6 6 6 4 4 4</p></div>
<div class="para"><p>5 5 5 5 5</p></div>
<div class="para"><p>6 6 6 6 6 6</p></div>
<div class="para"><p>(c) batch size = 4 (similar sequence lengths)</p></div>
<div class="para"><p>(d) disaggregation of prefilling and decoding</p></div>
<div class="para"><p><strong>Fig. 5.8:</strong> Illustrations of basic batching methods. We use a 2D layout to illustrate the batch, where each square represents a token. Red squares indicate tokens in the prefilling stage, blue squares represent tokens in the decoding stage, green squares denote padding tokens, and gray squares correspond to meaningless tokens. Subfigures (a) and (b)compare the cases where the batch size is 1 and 4, respectively. Subfigure (c) shows the strategy of grouping sequences with similar lengths into the same batch. Subfigure (d) illustrates the disaggregation of prefilling and decoding. In this approach, we can make better use of the parallelism of GPUs by concatenating multiple short sequences into a single long sequence for joint processing. This allows us to maximize the number of tokens processed in a batch while minimizing the number of padding tokens. However, as a trade-off, we need to copy the KV cache to the decoding engine and reorganize it after the prefilling phase, which introduces additional data transfer overhead.</p></div>
<h2>improving computational efficiency and maximizing hardware utilization 2.</h2>
<h2>To illustrate the idea of batching, Figure 5.8 (a-b) show simple examples with batch sizes of 1 and 4, respectively. When using a batch size of 1 (i.e., without batching), the GPU processes one input sequence at a time. Thus, the processing is sequential: the next sequence must wait for the current computation to finish. By contrast, when using a batch size of 4, the GPU can process four sequences simultaneously in a single forward pass. As the input sequences vary in length, we need to standardize their length using padding techniques. Here we use left padding, which adds dummy tokens to the beginnings of short sequences, so all the sequences in the batch would have the same length for prefilling. For decoding, tokens are generated simultaneously for all these sequences, and the generation process continues until the longest sequence reaches completion.</h2>
<h2>The above examples imply a trade-off between throughput and latency, which is a very impor- tant consideration in designing and implementing LLM inference systems. If we choose a smaller batch size, the latency would be lower, as fewer tokens need to be processed in a single run of inference. Imagine that we have only one sequence. The result becomes available immediately after generation completes, with no additional computational overhead. However, this low-latency advantage comes at the cost of underutilizing parallel computing resources, as the parallelism of</h2>
<div class="para"><p>2 See https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#understand-perf for a simple evaluation.</p></div>
<div class="para"><p><em>5.2 Efficient Inference Techniques</em> 225</p></div>
<div class="para"><p>GPUs remains largely idle during sequential processing. On the other hand, if we use a larger batch, we can make better use of the parallelism, as GPUs can be occupied by large-scale matrix computations. As a result, we can process more tokens in the same period of time and the throughput is improved. However, since the result is obtained only when the last token in the batch is predicted, the latency would be higher.</p></div>
<div class="para"><p>In practice, we usually prefer to use a slightly larger batch, but try to fill the batch with sequences of similar lengths to reduce the number of padding tokens and improve device utilization.For example, we can group the incoming user requests in a short period of time into buckets, each of which contains sequences with similar lengths. Then, we can fill the batch with sequences in the same bucket, so that we can minimize wasted computational resources, as illustrated in Figure 5.8 (c).</p></div>
<div class="para"><p>Another approach to implementing batching in LLMs is to disaggregate the prefilling and decoding processes [Wu et al., 2023a; Patel et al., 2024; Zhong et al., 2024]. For example, we can perform prefilling on one GPU, and perform decoding on another GPU. One advantage of disaggregation is that we can rearrange the input sequences in the batch to better fill it, because there is no interference between prefilling and decoding. For example, we can concatenate multiple short sequences into a longer one, thus ensuring that the lengths of sequences in the batch are as consistent as possible, as illustrated in Figure 5.8 (d). In this way, we can maximize the throughput of the prefilling phase. However, as a trade-off, we need to transfer the KV cache to the devices performing decoding, which also incurs extra communication overhead. Typically, this method requires a high-bandwidth, low-latency network to achieve optimal performance.</p></div>
<div class="para"><p>In this section, we will discuss several improvements to the above basic batching strategies.Most of them are based on an aggregated architecture, that is, decoding and prefilling can be considered as different stages of a model executed on the same device.</p></div>
<div class="para"><p><strong>5.2.2.1</strong> <strong>Scheduling</strong></p></div>
<div class="para"><p>A practical LLM inference system typically consists of two components:</p></div>
<div class="para"><p>•<strong> Scheduler</strong>. Its primary role is to efficiently queue and dispatch tasks (i.e., input sequences)to the inference engine based on the current system load and task priorities. This often involves a variety of batching strategies that group certain requests together to maximize processing efficiency in some way.</p></div>
<div class="para"><p>•<strong> Inference Engine</strong>. It is responsible for the actual execution of the LLMs, processing the queued requests as they come in. As discussed previously, this engine involves both prefilling and decoding processes.</p></div>
<div class="para"><p>This architecture is illustrated in Figure 5.9. Incorporating scheduling into batch processing provides a flexible way to optimize both the system’s throughput and latency, thereby achieving a better balance between them. For example, the batching methods shown in Figure 5.8 (a) and (b)can be considered one of the simplest scheduling strategies, called<strong> request-level scheduling</strong>. In this strategy, once a batch is filled and sent to the engine, the processing of the entire batch cannot</p></div>
<div class="para"><p>226 <em>Inference</em></p></div>
<div class="para"><p>Request Pool <strong>x</strong> 1,<strong> x</strong> 2,<strong> x</strong> 3, ...</p></div>
<div class="para"><p>batch</p></div>
<div class="para"><p>Scheduler Inference Engine</p></div>
<div class="para"><p>batch (after processing)</p></div>
<div class="para"><p>Predictions <strong>y</strong> 2,<strong> y</strong> 1,<strong> y</strong> 3, ...</p></div>
<div class="para"><p><strong>Fig. 5.9:</strong> Illustration of the LLM inference architecture involving a scheduler and an inference engine. Each time, the scheduler selects a number of user requests to form a batch and sends it to the inference engine. The scheduler can interact with the inference engine and adjust the batch at certain points during inference, such as at the beginning of batch processing and at the start of each token prediction.</p></div>
<div class="para"><p>be interrupted. The scheduler waits for this batch to be processed before handling the next batch[Timonin et al., 2022].</p></div>
<div class="para"><p>A more sophisticated scheduling strategy, called<strong> iteration-based scheduling</strong>, interacts with the inference engine at each token prediction step rather than at the sequence level. This approach allows dynamic batch adjustment during inference, as illustrated in Figure 5.10. Such fine-grained control lets the system prioritize critical tokens or sequences in real-time. For instance, if an urgent request arrives at some decoding step, the scheduler can add this request into the batch so that it can be processed as early as possible. In the following subsections, we will discuss batching methods based on iteration-based scheduling.</p></div>
<div class="para"><p><strong>5.2.2.2</strong> <strong>Continuous Batching</strong></p></div>
<div class="para"><p><strong>Continuous batching</strong> is an iteration-based scheduling method used in the Orca system [Yu et al.,2022]. In this method, an iteration refers to either the entire prefilling procedure or a single decoding step. For example, given an input sequence<strong> x</strong> =<em> x</em> 0<em>...x</em> <em>m</em> and an output sequence<strong> y</strong> =<em>y</em> 1<em>...y</em> <em>n</em>, there are<em> n</em>+1 iterations in total: one for prefilling, and<em> n</em> for generating the output tokens(one per token). During scheduling, the batch can be adjusted between iterations. For example,we can either add a new input sequence to the batch, or remove a complete sequence from the batch at some iteration, even if the batch processing is not yet finished.</p></div>
<div class="para"><p>The general process of continuous batching includes the following steps:</p></div>
<div class="para"><p>• Initially, a batch is created with one or more input sequences, based on both the inference engine’s processing capacity and the current user requests. The batch is then fed into the inference engine.</p></div>
<h1>5.2 Efficient Inference Techniques 227</h1>
<h3>Requests x 1, x 2, x 3 arrived</h3>
<h3>Requests x 1, x 2, x 3 arrived</h3>
<h3>Begin</h3>
<h3>Begin</h3>
<div class="para"><p>1 1</p></div>
<div class="para"><p>1 1</p></div>
<h3>Iteration 1 (prefilling)</h3>
<h3>Iteration 1 (prefilling)</h3>
<div class="para"><p>2</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>3 3 3</p></div>
<div class="para"><p>3 3 3</p></div>
<div class="para"><p>1 1</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>1 1</p></div>
<div class="para"><p>1</p></div>
<h3>Iteration 2</h3>
<h3>Iteration 2</h3>
<div class="para"><p>2</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>3 3 3 3</p></div>
<div class="para"><p>3 3 3 3</p></div>
<h3>Request x 4 arrived</h3>
<h3>Request x 4 arrived</h3>
<div class="para"><p>one decoding step for<strong> x</strong> 1,<strong> x</strong> 2,<strong> x</strong> 3</p></div>
<div class="para"><p>1 1</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>1 1</p></div>
<div class="para"><p>1</p></div>
<h3>Iteration 3</h3>
<div class="para"><p>2</p></div>
<div class="para"><p>2 2</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>2 2</p></div>
<h3>Iteration 3</h3>
<div class="para"><p>3 3 3 3 3</p></div>
<div class="para"><p>3 3 3</p></div>
<div class="para"><p>3 3</p></div>
<div class="para"><p>4 4 4</p></div>
<div class="para"><p>prefilling for<strong> x</strong> 4</p></div>
<div class="para"><p>1 1</p></div>
<div class="para"><p>1</p></div>
<div class="para"><p>1 1</p></div>
<div class="para"><p>1</p></div>
<h3>Iteration 4</h3>
<div class="para"><p>2</p></div>
<div class="para"><p>2 2</p></div>
<div class="para"><p>2</p></div>
<div class="para"><p>2 2</p></div>
<h3>Iteration 4</h3>
<div class="para"><p>3 3 3 3 3 3</p></div>
<div class="para"><p>3 3 3</p></div>
<div class="para"><p>3 3 3</p></div>
<div class="para"><p>4 4 4</p></div>
<div class="para"><p>4</p></div>
<h3>End</h3>
<h3>More iterations</h3>
<h3>x 4 is added to the next batch</h3>
<h3>End</h3>
<h2>(a) Request-level Scheduling</h2>
<h2>(b) Iteration-level Scheduling</h2>
<h2>Fig. 5.10: Illustrations of request-level scheduling and iteration-based scheduling. In request-level scheduling, once a batch is created and sent to the inference engine, we cannot adjust the batch. In other words, scheduling only occurs after the processing of a batch finishes. In iteration-level scheduling, we can perform scheduling during batch processing. For example, if a new request arrives at some point during inference, we can add it to the batch and continue processing.</h2>
<h1>The inference engine processes the batch iteration by iteration. After each iteration, the scheduler may adjust the batch in one of the following ways:</h1>
<h1>If a sequence in the batch completes generation (i.e., generates the end-of-sequence symbol), that sequence is removed from the batch.</h1>
<h1>If a new user request arrives and the inference engine has additional processing capac- ity, it is added to the batch.</h1>
<h1>If no sequences are added to or removed from the batch, the batch remains unchanged.</h1>
<h1>The processing terminates only when all sequences have been completed and no new user requests arrive.</h1>
<h1>See Figure 5.11 for an example of continuous batching. In this example, we start with two user</h1>
<h2>228 Inference</h2>
<div class="para"><p>input</p></div>
<div class="para"><p>input</p></div>
<div class="para"><p><strong>x</strong> 1,<strong> x</strong> 2 arrived</p></div>
<div class="para"><p>batch</p></div>
<div class="para"><p>batch</p></div>
<div class="para"><p>1 1 1 2 2 Scheduler</p></div>
<div class="para"><p>1 1 1 2 2 2 1 Scheduler</p></div>
<div class="para"><p>prefilling for<strong> x</strong> 1 and<strong> x</strong> 2.one decoding step for <strong>x</strong> 1 and<strong> x</strong> 2.</p></div>
<div class="para"><p>output</p></div>
<div class="para"><p>output</p></div>
<h3>(a) Iteration 1</h3>
<h3>(b) Iteration 2</h3>
<div class="para"><p>input</p></div>
<div class="para"><p>input</p></div>
<div class="para"><p><strong>x</strong> 3 arrived</p></div>
<div class="para"><p>batch</p></div>
<div class="para"><p>batch</p></div>
<div class="para"><p>1 1 1 2 2 2 2 1 1 Scheduler</p></div>
<div class="para"><p>2 2 2 1 1 1 Scheduler</p></div>
<div class="para"><p>1 1 1 2 2 3 3</p></div>
<div class="para"><p>one decoding step for <strong>x</strong> 1 and<strong> x</strong> 2.</p></div>
<div class="para"><p>one decoding step for <strong>x</strong> 1 and<strong> x</strong> 2, and prefilling for<strong> x</strong> 3.</p></div>
<div class="para"><p>output</p></div>
<div class="para"><p>output</p></div>
<h3>(c) Iteration 3</h3>
<h3>(d) Iteration 4</h3>
<div class="para"><p>input</p></div>
<div class="para"><p>input</p></div>
<div class="para"><p><strong>x</strong> 4,<strong> x</strong> 5 arrived</p></div>
<div class="para"><p>continue the second sequence in the batch with<strong> x</strong> 4</p></div>
<div class="para"><p>complete</p></div>
<div class="para"><p>batch</p></div>
<div class="para"><p>batch</p></div>
<div class="para"><p>1 1 1 2 2 3 3 3 2 2 2 2 1 1 1 1</p></div>
<div class="para"><p>1 1 1 4 4 4 3 3 3 3</p></div>
<div class="para"><p>1 1 1 1 1</p></div>
<div class="para"><p>Scheduler</p></div>
<div class="para"><p>Scheduler</p></div>
<div class="para"><p>one decoding step for <strong>x</strong> 1,<strong> x</strong> 2 and<strong> x</strong> 3.<strong>x</strong> 2’s prediction completes.</p></div>
<div class="para"><p>one decoding step for <strong>x</strong> 1 and<strong> x</strong> 3, and prefilling for<strong> x</strong> 4.</p></div>
<div class="para"><p>output</p></div>
<div class="para"><p>output</p></div>
<div class="para"><p><strong>y</strong> 2</p></div>
<h3>(e) Iteration 5</h3>
<h3>(f) Iteration 6</h3>
<div class="para"><p><strong>Fig. 5.11:</strong> Illustration of batch adjustment in continuous batching. Instead of fixing a batch of input sequences and processing them to completion (as in request-level batching), continuous batching dynamically updates the batch during inference. The system continuously accepts and adds new requests (e.g.,<strong> x</strong> 3 and<strong> x</strong> 4) into the current batch as long as there is available compute capacity.</p></div>
<h2>requests, x 1 and x 2. These two sequences are packed into a batch and sent to the inference engine for processing. After the engine completes two iterations, a new user request, x 3, arrives. At this point, the scheduler adjusts the batch by adding x 3 to it. The inference engine then continues processing the updated batch. Note that the inference engine now processes different sequences in different ways: x 1 and x 2 proceed with the decoding process (i.e., predicting the next tokens), while x 3 undergoes the prefilling process. After some time, the generation for x 2 completes. As it happens, two more user requests, x 4 and x 5, arrive. The scheduler removes the completed sequence x 2 from the batch and, considering the current load of the inference engine, adds x 4 to the batch. However, x 5 must wait until another sequence in the batch finishes before it can be added.</h2>
<div class="para"><p><em>5.2 Efficient Inference Techniques</em> 229</p></div>
<div class="para"><p>The idea behind continuous batching is to keep the inference engine fully utilized by processing as many sequences as possible, thereby maximizing computational resource usage. A key difference between continuous batching and standard batching (see Figure 5.8) lies in the fact that, in continuous batching, prefilling and decoding can occur simultaneously across different sequences,whereas in standard batching, these two phases are performed sequentially for the entire batch. As discussed in Section 5.1.2, prefilling is considered a compute-bound process, while decoding is considered a memory-bound process. The intuition behind overlapping prefilling and decoding is to reduce idle times for both computation and data transfer. Consider two mini-batches: one for prefilling and one for decoding. While the prefilling mini-batch keeps the GPUs occupied, the decoding mini-batch can perform memory transfers concurrently.</p></div>
<div class="para"><p>Another difference between continuous batching and standard batching is that continuous batching is prefilling-prioritized, while standard batching is decoding-prioritized [Agrawal et al.,2024]. In continuous batching, once the inference engine has spare computational resources, the scheduler will add new requests to the batch. In other words, these newly added requests will be processed for prefilling as early as possible. This approach improves system throughput, but at the cost of increased latency, as the newly added requests extend the processing time of earlier ones.In contrast, in standard batching, once the batch is created, we must wait for the last sequence in the batch to complete before processing new requests. This ensures relatively low latency, but results in lower device utilization and system throughput.</p></div>
<div class="para"><p>It is important to note that the cost of continuous batching is that we need to continuously reorganize the batches, which involves rearranging the data in memory. Each time a new request is added, the scheduler needs to reassess and optimize the current batch structure. This dynamic adjustment can incur additional memory and computational overhead, especially when the batches are frequently adjusted. Therefore, while this method can improve throughput, it may also lead to increased memory fragmentation and, in some cases, introduce additional latency.</p></div>
<div class="para"><p><strong>5.2.2.3</strong> <strong>PagedAttention</strong></p></div>
<div class="para"><p>PagedAttention (or paged KV caching) is a technique used in the vLLM system [Kwon et al.,2023]. Inspired by operating system paging, it optimizes memory usage during LLM inference— particularly for the KV cache — by addressing fragmented memory allocation in dynamic batching scenarios with variable-length sequences. The idea behind PagedAttention is to break down large memory requirements for KV caching into more manageable &quot;pages&quot; or chunks of memory. In this way, we do not need to store the KV cache of the full sequence in a continuous memory. Instead, the KV cache is divided into fixed-size blocks (analogous to memory pages in an operating system), which can be non-contiguously allocated in physical memory. One advantage of PagedAttention is that it enables flexible memory management, supporting dynamic sequence growth without requiring expensive reallocation or copying of large contiguous memory regions. Note that PagedAttention is not specifically designed for batching. But it indeed helps improve memory efficiency in batched inference scenarios, where memory management is more demanding and complicated.</p></div>
<div class="para"><p>Consider a simple example of memory allocation in Figure 5.12 in which self-attention is performed for a batch consisting of two sequences. For each sequence, we need to attend the current token to the key-value pairs in the KV cache of this sequence, as required by self-attention. In the</p></div>
<h2>230 Inference</h2>
<div class="para"><p>attend</p></div>
<div class="para"><p>KV Cache</p></div>
<div class="para"><p><em>⟨</em>SOS<em>⟩</em>I think this moive is better than</p></div>
<div class="para"><p>Sequence 1</p></div>
<div class="para"><p><em>⟨</em>pad<em>⟩</em><em>⟨</em>pad<em>⟩</em><em>⟨</em>pad<em>⟩</em><em>⟨</em>SOS<em>⟩</em>I really like reading</p></div>
<div class="para"><p>Sequence 2</p></div>
<h3>(a) Two sequences in a batch</h3>
<div class="para"><p>Physical Memory Blocks used</p></div>
<div class="para"><p>than</p></div>
<div class="para"><p>fragmented memory</p></div>
<div class="para"><p>reading</p></div>
<div class="para"><p><em>⟨</em>SOS<em>⟩</em>I think this</p></div>
<div class="para"><p>movie is better</p></div>
<div class="para"><p><em>⟨</em>pad<em>⟩</em><em>⟨</em>pad<em>⟩</em><em>⟨</em>pad<em>⟩</em><em>⟨</em>SOS<em>⟩</em></p></div>
<div class="para"><p>I really like</p></div>
<h3>(b) Memory allocation for KV caching in standard self-attention</h3>
<div class="para"><p>Physical Memory Blocks used</p></div>
<div class="para"><p>than</p></div>
<div class="para"><p>reading<em>⟨</em>SOS<em>⟩</em>I think this</p></div>
<div class="para"><p><em>⟨</em>pad<em>⟩</em><em>⟨</em>pad<em>⟩</em><em>⟨</em>pad<em>⟩</em><em>⟨</em>SOS<em>⟩</em></p></div>
<div class="para"><p>movie is better</p></div>
<div class="para"><p>I really like</p></div>
<h3>(c) Memory allocation for KV caching in PagedAttention</h3>
<div class="para"><p><strong>Fig. 5.12:</strong> Illustration of memory allocation in PagedAttention. There are two sequences in the batch, as illustrated in sub-figure (a). Since the memory is fragmented, the KV cache is stored in a large unused block of memory in standard self-attention (see sub-figure (b)), but the fragmented memory is not used. By contrast, in PagedAttention (see subfigure (c)), the KV cache is divided into smaller blocks and thus fits into fragmented memory.</p></div>
<h2>standard implementation of self-attention, the KV cache is stored in a contiguous block of mem- ory, allowing us to efficiently access this continuous memory. However, in a paged KV caching system, the KV cache is divided into smaller, fixed-size memory blocks which are not necessar- ily contiguous. These smaller KV cache blocks can be more effectively allocated to fragmented memory regions, thereby improving memory utilization. Another benefit of distributing chunks of</h2>
<div class="para"><p><em>5.2 Efficient Inference Techniques</em> 231</p></div>
<div class="para"><p>the KV cache across different memory blocks is that it enables parallelization of the caching process. For example, if the input sequence is long and the memory bandwidth is sufficient, it would be beneficial to write and read the key and value vectors of different segments of the sequence in parallel across multiple memory blocks.</p></div>
<div class="para"><p>In general, storing contiguous data in non-contiguous regions can cause issues, for example,accessing fragmented data requires additional seek time, which reduces I/O efficiency. However,when handling large-scale data (e.g., performing multiplication on extremely large matrices), we typically do not process all the data at once but instead divide it into smaller blocks for block-level computation. From this perspective, it is also reasonable to partition the attention computation. If the paging strategy is well designed, the additional overhead in memory access can be minimal,while the improvement in memory utilization can be significant.</p></div>
<div class="para"><p><strong>5.2.2.4</strong> <strong>Chunked Prefilling</strong></p></div>
<div class="para"><p>We have seen that, in iteration-level scheduling, prefilling and decoding for different sequences can occur simultaneously. This can be seen as a prefilling-prioritized strategy which can maximize the throughput. However, one such iteration can take a long time if the input sequence is very long and the prefilling process dominates the computation. In this case, decoding for other sequences has to wait until the prefilling completes, leading to increased latency for generating output tokens.Therefore, while prefilling-prioritized strategies are effective for maximizing hardware utilization,they may introduce significant variability in token generation latency, particularly when the system is handling a mix of long and short input sequences.</p></div>
<div class="para"><p>A simple way to reduce decoding latency is to make computations for different sequences in the batch comparable. One such method is to divide sequences into chunks and perform prefilling chunk by chunk. This approach, often referred to as chunked prefilling, processes smaller portions of each sequence at a time, allowing the system to better balance the computational load across sequences [Agrawal et al., 2023]. By choosing an appropriate chunk size, we can ensure that when prefilling and decoding overlap for two sequences, their processing within the same iteration tends to take a similar amount of time. As a result, decoding idle time is reduced and overall throughput is improved.</p></div>
<div class="para"><p>Figure 5.13 shows an illustration of chunked prefilling in a few iterations. In this example, the batch contains two sequences. The whole prefilling process of the first sequence is divided into three prefilling steps, giving rise to the chunks denoted<em> P</em> 11,<em> P</em> 12 and<em> P</em> 13. Each chunk corresponds to one iteration and can thus overlap with one decoding step. In this way, during the prefilling of the first sequence, we can perform three decoding steps, rather than only a single decoding step, as is the case in standard iteration-level scheduling. As a result, the idle time of the decoding process is reduced, and the output tokens can be generated earlier.</p></div>
<div class="para"><p>Chunked Prefilling improves decoding efficiency by overlapping prefilling and decoding, but at the cost of additional memory overhead and scheduling complexity. In standard prefilling, we process the whole input sequence once, building the KV cache in one go. By contrast, in chunked prefilling, each chunk needs a separate forward pass to compute its attention outputs and update the KV cache. As a result, we need to maintain the KV cache of early chunks while processing later chunks. This also compromises the parallelism of completing the prefilling for the entire sequence in a single pass. In practice, it is usually possible to balance throughput and latency by choosing an appropriate chunk size.</p></div>
<h3>232 Inference</h3>
<div class="para"><p>Prefilling in One Go</p></div>
<div class="para"><p>Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5</p></div>
<div class="para"><p>Sequence 1</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><em>P</em> 11</p></div>
<div class="para"><p><em>D</em> 11</p></div>
<div class="para"><p><em>D</em> 12</p></div>
<div class="para"><p><em>D</em> 13</p></div>
<div class="para"><p><em>· · ·</em>Sequence 2</p></div>
<div class="para"><p><em>P</em> 21</p></div>
<div class="para"><p><em>D</em> 21</p></div>
<div class="para"><p><em>D</em> 22</p></div>
<div class="para"><p><em>D</em> 23</p></div>
<div class="para"><p><em>D</em> 24</p></div>
<div class="para"><p>Idle Time</p></div>
<div class="para"><p>The prediction of the second output token is delayed.</p></div>
<div class="para"><p>(a) Simple Iteration-level Scheduling</p></div>
<div class="para"><p>Iter. 1 Iter. 2 Iter. 3 Iter. 4 Iter. 5 Iter. 6 Chunk 1 Chunk 2 Chunk 3</p></div>
<div class="para"><p>Sequence 1</p></div>
<div class="para"><p><em>· · ·</em></p></div>
<div class="para"><p><em>P</em> 11</p></div>
<div class="para"><p><em>P</em> 12</p></div>
<div class="para"><p><em>P</em> 13</p></div>
<div class="para"><p><em>D</em> 11</p></div>
<div class="para"><p><em>D</em> 12</p></div>
<div class="para"><p><em>· · ·</em>Sequence 2</p></div>
<div class="para"><p><em>P</em> 21</p></div>
<div class="para"><p><em>D</em> 21</p></div>
<div class="para"><p><em>D</em> 22</p></div>
<div class="para"><p><em>D</em> 23</p></div>
<div class="para"><p><em>D</em> 24</p></div>
<div class="para"><p><em>D</em> 25</p></div>
<div class="para"><p>The second output token can be predicted during prefilling for sequence 1.</p></div>
<div class="para"><p>(b) Chunked Prefilling</p></div>
<div class="para"><p><strong>Fig. 5.13:</strong> Comparison of simple iteration-based scheduling and chunked prefilling.<em> P</em> <em>xy</em> denotes the<em> y</em>-th prefilling step for sequence<em> x</em>, and<em> D</em> <em>xy</em> denotes the<em> y</em>-th decoding step for sequence<em> x</em>. In simple iteration-based scheduling (or prefilling-prioritized scheduling), since prefilling is treated as a single iteration,<em> D</em> 22 has to wait for the completion of the prefilling of sequence 1. In chunked prefilling, the prefilling process can be divided into multiple steps. Thus,<em> D</em> 22 can execute during prefilling for sequence 1 (i.e., during<em> P</em> 12).</p></div>
<h3>It is worth noting that the methods discussed in this subsection can broadly be categorized as priority-based scheduling methods. In these methods, we can give priority to certain requests, or to certain prefilling or decoding steps, so that system resources are allocated in a way that better aligns with specific performance goals. As presented above, for example, we may prioritize de- coding over prefilling to minimize token generation latency, or prioritize prefilling over decoding to maximize overall throughput in batch-processing scenarios. Practitioners can design custom priority policies for specific needs and operational constraints in real-world applications, such as request deadlines and importance levels defined by users.</h3>
<h3>5.2.3 Parallelization</h3>
<h3>Parallelization is a widely used approach to scale up LLM inference, especially for large-scale deployments. In Chapter 1, we have discussed several common parallelization strategies to paral- lelize LLM pre-training, such as model parallelism, tensor parallelism, and pipeline parallelism. We have also discussed efficient architectures that are easy to deploy in distributed computing systems. For example, in MoE models, we assigns different experts to different devices 3. Only the active experts for a given input are executed, which significantly improves computational effi- ciency while maintaining model quality. Many of these methods can be directly applied to LLM inference with minimal modifications.</h3>
<div class="para"><p>3 In LLMs, the experts are typically modular FFNs. So each expert is a part of the FFN component in the Transformer architecture.</p></div>
<div class="para"><p><em>5.2 Efficient Inference Techniques</em> 233</p></div>
<div class="para"><p>However, applying these parallelization techniques to inference poses new challenges compared to pre-training. These issues become especially pronounced in real-time or low-latency inference scenarios, where load imbalance across devices and communication overhead can significantly impact performance. For example, unlike pre-training, where batches can be prepared in advance, inference must handle variable-length sequences in real time. This makes it harder to maintain optimal device utilization and complicates scheduling across heterogeneous computational resources. A related issue is load balancing. When a large number of requests arrive in a short period of time, the system must efficiently distribute workloads across available devices. For example, real-world requests typically exhibit highly variable computational demands due to differences in task types and prompt lengths. Such variability renders simple static load balancing approaches ineffective, and so we need to use finer-grained strategies that can adapt to runtime conditions. The problem becomes even more complicated when we deploy the system on heterogeneous hardware and there are strict latency constraints.</p></div>
<div class="para"><p>In the development of LLMs, parallelization is closely related to LLM serving. Generally,building a high-quality LLM serving system is not a simple task — it typically requires the combination of multiple techniques, such as architectural design, workload distribution, and LLMspecific hardware/software optimizations. As such, LLM serving constitutes an exceptionally broad subject that often demands substantial engineering expertise. Here, we will not go into the details of LLM serving. For related concepts and techniques, readers may refer to relevant opensource systems (such as vLLM 4, TensorRT-LLM 5 and TGI 6) and papers [Pope et al., 2023; Li et al., 2024a].</p></div>
<div class="para"><p><strong>5.2.4</strong> <strong>Remarks</strong></p></div>
<div class="para"><p>We have considered many methods for improving the efficiency of LLMs in this and previous chapters. Although these approaches address different issues, most of them essentially explore trade-offs between various performance factors. One important trade-off is between inference speed and accuracy. For example, techniques like quantization, pruning, and knowledge distillation can significantly reduce computational overhead and latency but may introduce minor degradations in model performance. Conversely, preserving full precision or using larger models enhances accuracy but at the cost of slower inference and higher resource demands.</p></div>
<div class="para"><p>Another important consideration in LLM inference is the memory-compute trade-off. As in computer system design, we need to consider the balance between memory usage and computation required to generate the output. In particular, storing intermediate results such as KV caches during inference can significantly reduce redundant computation, but at the cost of increased memory usage. In KV caching, storing past attention states avoids recomputation of self-attention over previous tokens, thereby reducing compute time per token. However, as the number of tokens grows, so does the memory footprint of the KV cache, especially when processing very long sequences or multiple sequences in parallel. In response, various techniques have been developed to reduce memory consumption by partially recomputing intermediate states. For instance, chunked or windowed attention limits the attention span to a recent subset of tokens, reducing KV cache size at the cost of reduced context or additional compute if past information must be reprocessed.</p></div>
<div class="para"><p>4 https://github.com/vllm-project/vllm 5 https://github.com/NVIDIA/TensorRT-LLM 6 https://github.com/huggingface/text-generation-inference</p></div>
<div class="para"><p>234 <em>Inference</em></p></div>
<div class="para"><p>Note that considering the memory-compute trade-off is a very general principle. It can be extended beyond attention mechanisms and Transformers to other components in system design. An example is the choice of data precision. Using lower-precision formats such as FP16 or INT8 can reduce both memory usage and memory bandwidth requirements, effectively alleviating pressure on the memory subsystem. However, lower precision may lead to numerical instability or slight accuracy degradation, requiring careful calibration or retraining. Thus, this trade-off can also be seen as a memory-compute-accuracy triangle, where improvements in one dimension may come at the expense of another.</p></div>
<div class="para"><p>Beyond speed, accuracy, and memory, several other dimensions also influence LLM inference efficiency. Some of these dimensions have been discussed in this chapter, while others have not.Here we outline them as follows.</p></div>
<div class="para"><p>•<strong> Throughput vs. Latency</strong>: In large-scale multi-user LLM serving scenarios, we often aim to maximize system throughput. For example, as discussed in this section, we can batch multiple requests together to increase the number of tokens processed at the same time.However, batching increases waiting time and may lead to higher per-request latency, especially for short or interactive requests. By contrast, optimizing for low latency often requires serving requests individually or in smaller batches, which underutilizes hardware resources and reduces throughput. Achieving a good balance depends on the quality-of-service requirements and user interaction patterns.</p></div>
<div class="para"><p>•<strong> Generalization vs. Specialization</strong>: General-purpose LLMs are trained to perform a wide range of tasks with a single set of parameters. While flexible, they may be less efficient or accurate for specific tasks. Specialized models can yield better performance and lower inference costs for targeted applications. However, maintaining multiple specialized models increases system complexity and storage requirements. The trade-off between maintaining a single general model versus multiple specialized models is an important system-level design choice.</p></div>
<div class="para"><p>•<strong> Energy Efficiency vs. Performance</strong>: High-performance inference often requires running large models at high throughput on powerful accelerators, which consumes considerable energy. This may be problematic for edge deployments or energy-sensitive environments.Techniques like model compression can improve energy efficiency, but usually with some degradation in output quality or increase in latency. Energy constraints thus introduce another important dimension in optimizing LLM inference.</p></div>
<h3>5.3 Inference-time Scaling</h3>
<div class="para"><p>Scaling laws can be considered one of the fundamental principles guiding the development of LLMs. In previous chapters, we discussed several times that scaling up training data, model size,and compute can effectively improve the performance of pretraining. In fact, scaling laws also apply to downstream stages such as fine-tuning and inference (see Figure 5.14). Here we consider<strong> inference-time scaling</strong>, which has been widely employed by recent LLMs to solve complex</p></div>
<div class="para"><p><em>5.3 Inference-time Scaling</em> 235</p></div>
<div class="para"><p>Inference-time Scaling</p></div>
<div class="para"><p>Performance</p></div>
<div class="para"><p>Fine-tuning Scaling</p></div>
<div class="para"><p>Pre-training Scaling</p></div>
<div class="para"><p>Compute</p></div>
<div class="para"><p><strong>Fig. 5.14:</strong> Scaling for pre-training, fine-tuning and inference stages [Briski, 2025].</p></div>
<div class="para"><p>problems, such as complex math problems [Snell et al., 2025]. Unlike pre-training and finetuning scaling, which focuses on improving LLMs via parameter updates, inference-time scaling improves these models during inference without further training. This includes a large variety of methods which scale LLMs in different dimensions, such as ensembling multiple model outputs,increasing context length, adopting more aggressive decoding algorithms, and using external tools to extend model capabilities.</p></div>
<div class="para"><p>While inference-time scaling is wide-ranging, in this section we consider those methods that incorporate more compute into inference (called inference-time compute scaling). Here is a list of inference-time (test-time) compute scaling methods, organized by category:</p></div>
<div class="para"><p>•<strong> Context Scaling</strong>. It involves scaling the input or context to improve generation (or potentially scale the output).</p></div>
<div class="para"><p>•<strong> Search Scaling</strong>. It involves increasing computational effort during decoding.</p></div>
<div class="para"><p>•<strong> Output Ensembling</strong>. It involves combining multiple model outputs.</p></div>
<div class="para"><p>•<strong> Generating and Verifying Thinking Paths</strong>. It involves guiding LLMs to generate and verify thinking paths for solving complex reasoning problems.</p></div>
<div class="para"><p>We will describe these methods in the following subsections.</p></div>
<div class="para"><p><strong>5.3.1</strong> <strong>Context Scaling</strong></p></div>
<div class="para"><p>Context scaling improves LLM performance by extending the input to the model. A straightforward approach is to incorporate more helpful context during inference, allowing the model to</p></div>
<div class="para"><p>236 <em>Inference</em></p></div>
<div class="para"><p>condition its predictions on more prior information. One example is few-shot prompting. It augments the context with multiple input-output examples, and so the model can learn task behavior implicitly from these examples without parameter updates. On top of few-shot prompting, we can use chain-of-thought prompting to encourage the model to produce intermediate reasoning steps before final answers. Note that chain-of-thought prompting is one of the most important methods in addressing reasoning problems. By explicitly providing intermediate steps in problem-solving,we can prompt the model to break down complex tasks into simpler sub-tasks, which is found to be very beneficial for generating accurate and interpretable outputs.</p></div>
<div class="para"><p>Beyond extending the prompt with examples or reasoning steps, another approach to context scaling involves dynamically incorporating external knowledge. This is often achieved through RAG. RAG systems first retrieve relevant document snippets from a large collection of documents or a database based on the current input. These retrieved pieces of information are then added to the context provided to the LLM. This essentially expands the context to include timely or specialized external knowledge. By doing so, the model grounds its responses in specific knowledge found in the external source. The LLM thus can generate responses that are not only relevant to the input but also factually accurate and up-to-date.</p></div>
<div class="para"><p>However, as the context grows, these methods often suffer from the constraints of finite context window length. While model architectures and techniques (like efficient attention models)are continually evolving to support longer contexts, processing extremely long inputs still poses challenges. Increased computational cost is one factor. More critically, when the context window becomes very large, the model might struggle to attend effectively to the most relevant information (e.g., the “lost in the middle” phenomenon). Therefore, effective context scaling is not just about adding more information, but also about strategically selecting, structuring, and presenting the most pertinent information within the model’s processing capabilities.</p></div>
<div class="para"><p>Here we omit the detailed discussion of these methods, as they have already been covered in previous chapters. See Chapters 2 and 3 for more details, including prompting, RAG, and long-sequence modeling methods.</p></div>
<div class="para"><p><strong>5.3.2</strong> <strong>Search Scaling</strong></p></div>
<div class="para"><p>In LLMs, decoding is a search process that aims to efficiently find the best output sequence given the input sequence. Search scaling (or decoding scaling) typically involves two aspects: scaling the output length and scaling the search space.</p></div>
<div class="para"><p>Scaling the output length refers to increasing the number of tokens generated during inference.This is especially important in tasks that require long-form generation, such as story writing. More recently, generating outputs with long thinking paths has shown strong performance in math problem solving and code generation. For example, encouraging the model to generate long thinking paths before producing the final answers has been found to be very beneficial in performing complex reasoning. This idea has been widely used in developing recent LLMs for reasoning, such as OpenAI [2024]’s o1 and Deepseek [2025]’s R1. We will discuss more about output length scaling in Section 5.3.4.</p></div>
<div class="para"><p>Scaling the search space, on the other hand, refers to expanding the set of candidate output sequences considered during search, so that higher-quality outputs can be found. As discussed in Section 5.1.3, a simple example is that in beam search we increase the beam width to allow more candidate sequences to be explored in parallel at each decoding step. This increases the chance</p></div>
<div class="para"><p><em>5.3 Inference-time Scaling</em> 237</p></div>
<div class="para"><p>of discovering better outputs, especially in tasks where the optimal solution is not immediately apparent from local decisions.</p></div>
<div class="para"><p>In addition to decoding algorithm adjustments, it is also possible to explore compact structures to encode a large number of outputs. For example, we can construct and navigate a tree or graph of reasoning steps [Yao et al., 2024]. In this paradigm, each node represents a partial solution or intermediate step, and edges represent transitions between reasoning states. Such structured search enables the model to consider multiple paths simultaneously. Another related direction is Monte Carlo tree search-inspired decoding, where the model stochastically explores and scores different paths based on learned heuristics or external reward models.</p></div>
<div class="para"><p>Search scaling is a very general idea, and it is often implicitly involved in the design of search procedures that exploit search structure, heuristics, and model uncertainty. Many of the above methods have been discussed previously, though they were not originally developed with scaling as their primary goal. However, search scaling inherently comes with computational costs.Increasing beam width, for instance, directly translates to higher memory usage and longer inference times. In practice, there is often a point of diminishing returns, where further expansion of the search space yields marginal improvements in output quality at a significant computational expense. Therefore, an effective strategy often involves finding an optimal balance between scaling and computational feasibility.</p></div>
<div class="para"><p><strong>5.3.3</strong> <strong>Output Ensembling</strong></p></div>
<div class="para"><p>If we have multiple model outputs, it is often beneficial to combine them to mitigate the impact of individual model errors and synthesize a superior final output. Each model might capture different aspects of the underlying data distribution or possess unique strengths and weaknesses.By ensembling, we can average out the noise or random errors present in individual predictions,leading to a more stable and reliable outcome. In LLM ensembling, one of the simplest approaches is to average the probability distributions over the next token from each model, and select the best token using this averaged distribution. Or, if we regard the problem as a discrete decision-making task, majority voting can be employed. More sophisticated methods might involve re-ranking candidate outputs generated by different models based on a separate scoring function or even using a meta-learner to intelligently combine the predictions.</p></div>
<div class="para"><p>The “scaling” from output ensembling comes at the cost of running multiple models or sampling multiple outputs. This not only increases the latency of inference but also leads to the additional complexity of managing multiple models. But the quality of outputs does not continue to improve indefinitely as more models are added. In some cases, the benefits of output ensembling may diminish as the number of component models in the ensemble exceeds a certain threshold.Instead, the benefits of ensembling are generally greater when the individual models are diverse(i.e., they make different errors), even if there are a relatively small number of component models.Therefore, it is common practice to use a set of diverse LLMs which differ in their training data,model architectures, or fine-tuning objectives.</p></div>
<div class="para"><p>In LLMs, “scaling” often implies making things “bigger” for quality with more resources.However, in addition to scaling up the quality, scaling can mean more. It can also signify scaling up the robustness (making the system less prone to errors and more reliable) and exploration (covering a wider range of potential solutions). In output ensembling, these dimensions are naturally integrated. For instance, the very act of averaging or voting across different model outputs is a</p></div>
<div class="para"><p>238 <em>Inference</em></p></div>
<div class="para"><p>direct strategy to scale up robustness against individual model failures. Furthermore, by intentionally including varied models, ensembling increases the chances of discovering novel or superior solutions. In this sense, scaling is not limited to making models larger or running them longer —it also means strategies for making inference more robust, exploratory, and adaptive.</p></div>
<div class="para"><p><strong>5.3.4</strong> <strong>Generating and Verifying Thinking Paths</strong></p></div>
<div class="para"><p>So far, we have viewed inference-time scaling as a general class of methods for scaling various aspects of inference, such as sequence length, model size, and/or search strategies. In fact, one successful application is the use of inference-time scaling to enhance the reasoning capabilities of LLMs. As we have seen, the reasoning performance of LLMs can be improved by using chainof-thought methods. We can therefore make use of the chain-of-thought prompts to generate intermediate reasoning steps and reach a correct answer. However, reasoning problems are often so complicated that we cannot obtain high-quality solutions by providing simple chain-of-thought prompts. For example, when solving a math problem, we typically need to reason over a sequence of steps. At each step, we need to work out some intermediate result, verify it, and then determine what to do next. The reasoning path is not a fixed pattern but a dynamically generated thinking process that often involves trial-and-error, backtracking, and self-correction. This requires more sophisticated prompting strategies or search algorithms to navigate such complex reasoning. In this subsection, we focus on inference-scaling methods that go beyond simple chain-of-thought to address complex reasoning problems more effectively.</p></div>
<div class="para"><p>At a high level, methods for scaling the reasoning of LLMs can be categorized into two classes:</p></div>
<div class="para"><p>•<strong> Training-free Methods</strong>. These methods aim to improve reasoning capabilities without requiring any modification or retraining of the pre-trained parameters. Instead, they focus on techniques applied during inference, such as sophisticated prompting strategies (e.g.,chain-of-thought) and algorithmic control over the reasoning process (e.g., search).</p></div>
<div class="para"><p>•<strong> Training-based Methods</strong>. These methods involve further training or fine-tuning the model parameters to explicitly improve reasoning abilities, such as supervised fine-tuning on datasets with reasoning examples (e.g., math problems with step-by-step solutions).</p></div>
<div class="para"><p>In the following, we first discuss training-free methods, and then training-based methods.</p></div>
<div class="para"><p><strong>5.3.4.1</strong> <strong>Solution-level Search with Verifiers</strong></p></div>
<div class="para"><p>Given an input sequence (e.g., a math problem), there are many possible output sequences (e.g.,solutions to the problem). If we have a model to evaluate or verify each solution, we can select the best one. This is the fundamental principle behind methods like best-of-<em>N</em> sampling, where multiple outputs are generated, and the optimal result is picked based on some selection mechanism.Such a selection process can be viewed as a search problem, which involves two components:</p></div>
<div class="para"><p>•<strong> Search Algorithm</strong>. This defines the strategy used to explore the space of possible output sequences (solutions) and generate a set of candidates. It can range from simple independent sampling to more sophisticated search techniques as discussed in Section 5.1.3.</p></div>
<h3>5.3 Inference-time Scaling 239</h3>
<div class="para"><p>Express (5<em> −</em>4 <em>i</em>)<em> −</em>2(3 + 6 <em>i</em>) as a x complex number. Think step by step.</p></div>
<div class="para"><p>Problem (<strong>x</strong>)</p></div>
<div class="para"><p>Now we distribute the 2 to the terms in the parenthesis. 2(3 + 6 <em>i</em>) = 6 + 12 <em>i</em>.</p></div>
<div class="para"><p>Step 1 (<em>a</em> 1)</p></div>
<div class="para"><p>So (5<em> −</em>4 <em>i</em>)<em> −</em>2(3 + 6 <em>i</em>) is equivalent to(5<em> −</em>4 <em>i</em>)<em> −</em>(6 + 12 <em>i</em>).</p></div>
<div class="para"><p>Step 2 (<em>a</em> 2)</p></div>
<div class="para"><p>Solution with A Reasoning Path</p></div>
<div class="para"><p>Now we subtract the terms.x 5<em> −</em>4 <em>i</em><em> −</em>6<em> −</em>12 <em>i</em> =<em> −</em>1<em> −</em>16 <em>i</em>.</p></div>
<div class="para"><p>Step 3 (<em>a</em> 3)</p></div>
<div class="para"><p>That’s the answer.x</p></div>
<div class="para"><p>Step 4 (<em>a</em> 4)</p></div>
<div class="para"><p>You can also write it as<em> −</em>1<em> −</em>16 <em>i</em>.x Answer<em> −</em>1<em> −</em>16 <em>i</em>.</p></div>
<div class="para"><p>Step 5 (<em>a</em> 5)</p></div>
<div class="para"><p><strong>Fig. 5.15:</strong> Illustration of multi-step reasoning. This example is from the PRM800K dataset [Lightman et al., 2024].Given a math problem, the LLM is prompted to generate a thinking path (or reasoning path) consisting of several reasoning steps. Each step addresses a sub-problem based on the results of the previous steps. The answer to the original problem is contained in the last step.</p></div>
<h3>Verifier. This is a model or function responsible for evaluating the quality, correctness, or utility of each candidate solution generated by the search algorithm. It provides a score, a probability, or a judgment that allows the system to select the best among the candidates. The verifier can be another LLM, or even a set of predefined rules or heuristics.</h3>
<h3>Given an input problem x, we define that an output solution y can be represented as a sequence of reasoning steps:</h3>
<h3>y = (a 1, a 2, ..., a n r) (5.37)</h3>
<h3>where a i is the i-th reasoning step, and a n r is the last step which should contain the answer to the problem. See Figure 5.15 for an example of a multi-step reasoning path.</h3>
<h3>The search algorithm can efficiently generate a set of candidate solutions</h3>
<h3>D c = {y 1, ..., y K} (5.38)</h3>
<h3>Then, we can use a verifier, which evaluates each solution by the function V (y), to score the candidates in D c. The final output is the best candidate selected by the verifier</h3>
<h3>ˆ y = arg max y∈D c V (y) (5.39)</h3>
<div class="para"><p>240 <em>Inference</em></p></div>
<div class="para"><p>Although verifying the entire reasoning path is possible, a simpler alternative is to verify only the final reasoning step. In this way the verifier function<em> V</em> (<strong>y</strong>) is simplified to depend solely on the final answer contained within<em> a</em> <em>n</em> <em>r</em>. This can be achieved in various ways, depending on the nature of the problem and the expected answer format.</p></div>
<div class="para"><p>• For some math and coding problems, we can use off-the-shelf tools as verifiers. Examples include proof checkers for mathematical theorems, interpreters or compilers for code execution, and unit test systems for verifying program correctness against predefined test cases.</p></div>
<div class="para"><p>• If there is labeled data for evaluating the answer, such as human preference data, we can train a reward model on such data. The learned reward model is then used as the verifier which assigns a scalar score to each candidate answer.</p></div>
<div class="para"><p>• If there are no existing systems or suitable reward models, we can use another LLM to act as the verifier. This LLM is prompted to assess the quality of the candidate answer. It could potentially be a more capable model, or the same LLM used with a specific “evaluator”prompt.</p></div>
<div class="para"><p>• Alternatively, simpler heuristic-based verifiers can be designed. A commonly used approach is to employ majority voting, where the most frequently occurring answer among a set of candidates is selected.</p></div>
<div class="para"><p>Based on these verifiers, we can search to obtain a set of candidate solutions for selection.One simple strategy, which is often referred to as<strong> parallel scaling</strong> [Brown et al., 2024; Snell et al.,2024], involves generating<em> K</em> candidate solutions by running the base LLM<em> K</em> times independently. In this process, we can adjust the temperature in sampling to control the diversity in the outputs. The verifier then assesses each of these<em> K</em> complete solutions, and the one with the highest score is selected as the final output. This is conceptually very similar to best-of-<em>N</em> sampling,which in previous chapters we primarily described as a method of selecting the best one from a set of sampled outputs using a reward model.</p></div>
<div class="para"><p>Another approach is<strong> sequential scaling</strong>, which builds a sequence of solutions incrementally[Gou et al., 2024; Zhang et al., 2024]. It starts with an initial solution generated by the LLM with prompting. Then, we use a verifier (often the same LLM) to evaluate the solution. This can be seen as a critique stage. The output of this stage is some form of feedback, such as textual critiques pinpointing errors or suggesting improvements, numerical scores reflecting solution quality, or even a revised plan or intermediate step to guide the next generation. This feedback, along with the original problem and the current solution, is then used to prompt the LLM to generate a potentially improved solution. This can be seen as a refine stage. This critique-refine cycle can be repeated, forming an iterative loop:</p></div>
<div class="para"><p><strong>y</strong> <em>k</em>+1=Refine(<strong>x</strong><em>,</em><strong> y</strong> <em>k</em><em>,</em> Feedback(<strong>y</strong> <em>k</em>))(5.40)</p></div>
<div class="para"><p>where Feedback(<strong>y</strong> <em>k</em>) represents the feedback from the verifier. The Refine(<em>·</em>) function generates the improved solution<strong> y</strong> <em>k</em>+1 by prompting the LLM with the original problem<strong> x</strong>, the previous solution<strong> y</strong> <em>k</em>, and this feedback. The process can be iterated for<em> K</em> times, or until the solution</p></div>
<div class="para"><p><em>5.3 Inference-time Scaling</em> 241</p></div>
<div class="para"><p>Solution 1</p></div>
<div class="para"><p>Sampling</p></div>
<div class="para"><p><strong>y</strong> 1</p></div>
<div class="para"><p>Problem Solution 2</p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p><strong>y</strong> 2</p></div>
<div class="para"><p>Solution 3</p></div>
<div class="para"><p><strong>y</strong> 3</p></div>
<div class="para"><p>(a) Parallel Scaling</p></div>
<div class="para"><p>Self-refinement</p></div>
<div class="para"><p>Problem Solution 1 Solution 2 Solution 3 <strong>x</strong> <strong>y</strong> 1 <strong>y</strong> 2 <strong>y</strong> 3</p></div>
<div class="para"><p>(b) Sequential Scaling</p></div>
<div class="para"><p><strong>Fig. 5.16:</strong> Illustrations of parallel scaling and sequential scaling. In parallel scaling, we obtain multiple solutions by running the LLM several times independently. In sequential scaling, the LLM generates an initial solution. Then, we use the LLM to refine it iteratively, with each refinement yielding a new, possibly better solution.</p></div>
<div class="para"><p>quality, as assessed by the verifier, converges to a satisfactory level. This iterative framework,where a solution is progressively improved through cycles of generation, evaluation (critique), and revision, is precisely what constitutes self-refinement [Shinn et al., 2023; Madaan et al., 2024]. In such scenarios, the role of the verifieris not just to pick the best complete solution from a static set, but to actively guide the generation process itself.</p></div>
<div class="para"><p>See Figure 5.16 for illustrations of parallel scaling and sequential scaling. Note that there are other ways to perform search and obtain different sets of candidate solutions. One alternative method is to organize search as a tree structure. This approach, often referred to as tree search,provides a more structured way to explore the space of possible reasoning paths. In solution-level search, each node of the tree represents a complete solution. During search, we need to expand a node to a set of child nodes, representing new solutions that can be considered in verification.The expansion process typically involves taking an existing solution (the parent node) and using the LLM to generate variations or alternative solutions.</p></div>
<div class="para"><p><strong>5.3.4.2</strong> <strong>Step-level Search with Verifiers</strong></p></div>
<div class="para"><p>While the methods discussed above primarily focus on generating complete solutions before final selection, the search process can also be integrated more deeply into the step-by-step generation of the reasoning path itself. This leads to approaches that perform step-level search with verifiers,where guidance or pruning occurs at intermediate reasoning steps<em> {</em><em>a</em> 1<em>, ..., a</em> <em>n</em> <em>k</em><em>}</em> rather than only after a full solution<strong> y</strong> is formed.</p></div>
<div class="para"><p>Such fine-grained control is particularly beneficial for complex reasoning problems where a single incorrect intermediate step can render the entire subsequent reasoning chain invalid. By evaluating or guiding the generation at each intermediate step, the LLM can explore the reasoning space more effectively, potentially pruning unpromising paths early or allocating more resources to explore more plausible ones.</p></div>
<div class="para"><p>242 <em>Inference</em></p></div>
<div class="para"><p>Step-level search with verifiers can also be modeled as a tree search problem. In this paradigm,each node (or state) corresponds to a partial reasoning path,<strong> a</strong><em>≤</em><em>i</em> = (<em>a</em> 1<em>, ..., a</em> <em>i</em>), representing the sequence of<em> i</em> reasoning steps taken so far (i.e., a path from the root node to the current node).The objective of the search process is to explore the underlying state space, starting from an initial empty path, to find a complete path that constitutes a correct solution. Note that we use<strong> a</strong><em>≤</em><em>i</em> here to represent a partial reasoning path instead of<strong> y</strong><em>≤</em><em>i</em>. While this makes notation a bit inconsistent with that used for representing complete solutions (<strong>y</strong>) or full paths in solution-level search, it serves to highlight the focus on individual actions or steps.</p></div>
<div class="para"><p>The core components of step-level search with verifiers are:</p></div>
<div class="para"><p>•<strong> Node Representation</strong>. A node is a partial reasoning path<strong> a</strong><em>≤</em><em>i</em> = (<em>a</em> 1<em>, ..., a</em> <em>i</em>). The root node is an empty path, and terminal nodes are complete reasoning paths.</p></div>
<div class="para"><p>•<strong> Node Expansion</strong>. Given a current partial path<strong> a</strong><em>≤</em><em>i</em>, the LLM is used to generate one or more candidate next reasoning steps<em> {</em><em>a</em>(1)<em>i</em>+1<em>, ..., a</em>(<em>M</em>)<em>i</em>+1<em> </em><em>}</em>. Each candidate step, when appended</p></div>
<div class="para"><p>to<strong> a</strong><em>≤</em><em>i</em>, forms a new potential partial path<strong> a</strong><em>≤</em><em>i</em>+1 = (<em>a</em> 1<em>, ..., a</em> <em>i</em><em>, a</em>(<em>j</em>)<em>i</em>+1).</p></div>
<div class="para"><p>•<strong> Verification</strong>. The verifier<em> V</em> (<em>·</em>) evaluates the quality of a newly generated step in the context of the current partial path<strong> a</strong><em>≤</em><em>i</em> = (<em>a</em> 1<em>, ..., a</em> <em>i</em>) and the original problem<strong> x</strong>. As with solutionlevel verification, step-level verifiers might output a numerical score, a categorical label,and textual feedback.</p></div>
<div class="para"><p>•<strong> Search</strong>. This governs how the search space is explored. Based on the evaluations from the verifier, the search strategy decides which partial paths to extend further, which to prune,and the order of exploration.</p></div>
<div class="para"><p>This step-by-step verification allows for dynamic adjustments to the reasoning process. If a step<em> a</em> <em>i</em>+1 is deemed incorrect or unpromising by<em> V</em> (<em>·</em>), the search algorithm can backtrack and explore alternative steps from<strong> a</strong><em>≤</em><em>i</em>, or even from an earlier node<strong> a</strong><em>≤</em><em>i</em><em>′</em> (where<em> i</em><em>′</em><em> </em><em>&lt; i</em>). Conversely,if a step is highly rated, resources can be focused on extending that path. See Figure 5.17 for an illustration of step-level search with verifiers.</p></div>
<div class="para"><p>Clearly, this search framework is very similar to that used in decoding methods for LLMs,as discussed in Section 5.1.3. For example, beam search maintains a set of<em> K</em> most promising partial sequences at each generation step. This is a form of step-level search where the “verifier”is implicitly the LLM’s own probability model, and the “search” is the pruning mechanism to maintain the beam size.</p></div>
<div class="para"><p>However, step-level search with explicit verifiers, as described here, presents differences from standard decoding. One of them is that the verifier can be a much more sophisticated component than just the raw output probabilities of the generative LLM. The design of step-level verifiers basically follows that of solution-level verification. A step-level verifier might be a language model that assesses the quality of an individual reasoning step within the context of the preceding path.This LLM can even be fine-tuned to enhance its verification capability. Alternatively, for domains with well-defined rules, it could be a symbolic engine or a set of programmatic checks. Furthermore, verifiers can be designed to predict the future utility or likelihood of success given the current partial path, drawing inspiration from value functions in reinforcement learning. Human</p></div>
<h3>5.3 Inference-time Scaling 243</h3>
<div class="para"><p>Input Problem (Root)</p></div>
<div class="para"><p><strong>x</strong></p></div>
<div class="para"><p>Pruned by Likelihood</p></div>
<div class="para"><p><em>a</em>(3)1 <em>a</em>(2)1 <em>a</em>(1)1 <em>a</em>(4)1 <em>a</em>(5)1⊠⊠⊠⊠</p></div>
<div class="para"><p>Pruned by Verification</p></div>
<div class="para"><p><em>a</em>(3)2 <em>a</em>(2)2 <em>a</em>(1)2 <em>a</em>(4)2 <em>a</em>(5)2⊠⊠⊠⊠</p></div>
<div class="para"><p>Selected Reasoning Step</p></div>
<div class="para"><p><em>a</em>(3)3 <em>a</em>(2)3 <em>a</em>(1)3 <em>a</em>(4)3 <em>a</em>(5)3⊠⊠⊠⊠</p></div>
<div class="para"><p><em>a</em>(3)4 <em>a</em>(2)4 <em>a</em>(1)4 <em>a</em>(4)4 <em>a</em>(5)4⊠⊠⊠⊠</p></div>
<div class="para"><p><strong>Fig. 5.17:</strong> Illustration of step-level search with verifiers.<em> a</em>(<em>j</em>)<em>i</em>= the<em> j</em>-th candidate for the<em> i</em>-th reasoning step, ⊠=candidate pruned by the LLM’s output probability, and ⊠= candidate pruned by the verifier. Given the input problem as the root node, we expand the tree by generating multiple reasoning steps at each expansion. Each candidate can be pruned by either likelihood (as in standard decoding) or step-level verification. The unpruned candidates are then expanded to generate further reasoning steps. The process is iterated until a complete reasoning chain leading to a final answer is generated, or until a predefined search limit is reached.</p></div>
<h3>expertise can also be incorporated to provide judgments on critical steps, especially in high-stakes scenarios.</h3>
<h3>One example of such a step-level verifier, particularly when using human feedback to assess intermediate progress, is the process reward model (PRM). A PRM is typically a separate lan- guage model trained to output a scalar reward for each reasoning step a i′ within a partial path a≤i. It provides a more direct and fine-grained supervisory signal compared to outcome reward mod- els (ORMs) which only evaluate the final solution. However, the development of PRMs relies on step-level human annotations, such as preferences on different next steps. Collecting supervi- sion for each intermediate step is considerably more labor-intensive and requires greater cognitive effort from human annotators than simply labeling final outcomes.</h3>
<h3>One alternative approach to developing training data for step-level verification is to use LLMs to generate such annotations automatically. For example, we can take a strong LLM, referred to as a teacher model, and prompt it to first generate a complete reasoning path for a given problem. Then, at each intermediate step within this path, we can prompt the same teacher LLM (or another capable LLM) to generate several alternative candidate next steps in addition to the one it origi- nally chose. The teacher LLM can then be prompted again to evaluate these alternatives. These evaluation results (e.g., correct vs. incorrect) can then serve as data annotations. Alternatively, the generalization capabilities of PRMs can be leveraged. We can train a PRM on tasks where step- level verification is easier and then generalize this PRM to other tasks with little or no additional</h3>
<div class="para"><p>244 <em>Inference</em></p></div>
<div class="para"><p>training.</p></div>
<div class="para"><p>Note that step-level verification also comes with its own problems. Frequent verification,especially if using an LLM as the verifier, can substantially increase computational costs and latency. The design of effective step-level verifiers is non-trivial itself. An inaccurate verifier might prematurely discard good reasoning paths or fail to identify flawed ones, thereby misleading the search. This makes the development of such systems more complex and difficult.</p></div>
<div class="para"><p><strong>5.3.4.3</strong> <strong>Encouraging Long Thinking</strong></p></div>
<div class="para"><p>So far in this subsection, most of the methods are implicitly based on a simple idea: generating longer reasoning paths can help. In addition to CoT and search with verifications, we can consider alternative methods to achieve this. For example, we can prompt the LLM by explicitly asking for extended deliberation. Beyond direct prompting, we can also make modifications to the decoding process itself, such as adjusting token limits or applying penalties for short outputs. Another approach is to employ multi-stage generation schemes where the model incrementally builds upon its reasoning.</p></div>
<div class="para"><p><strong>5.3.4.4</strong> <strong>Training-based Scaling</strong></p></div>
<div class="para"><p>As well as considering inference-time scaling methods without training, we also wish to consider methods that can improve intrinsic reasoning capabilities of LLMs by modifying their parameters through further training. While such training-based scaling methods typically require additional training cost and computational resources, they instill stronger reasoning skills directly into the model parameters, which in turn can lead to more effective and efficient reasoning performance.We can even combine them with training-free methods for better inference-time scaling results.</p></div>
<div class="para"><p>Although our discussion here is restricted to reasoning problems, methods for training-based scaling are common. Most of them have been discussed in Chapter 4. Here, we will briefly describe how these methods can be applied to improving inference-time scaling for reasoning problems.</p></div>
<div class="para"><p>•<strong> Fine-tuning on Reasoning Data</strong>. One of the most direct ways to enhance reasoning is by fine-tuning pre-trained LLMs on datasets specifically curated for reasoning tasks. These datasets can range from simple input-output pairs to more structured formats that include step-by-step reasoning processes. Typical examples include datasets of math word problems, logical deduction exercises, or code generation with explanations. By training on such data, the model learns from common reasoning patterns, and thus can generate detailed and coherent reasoning paths at test time.</p></div>
<div class="para"><p>•<strong> Reinforcement Learning for Reasoning</strong>. If we regard a verifier as a reward model, we can see that the methods discussed in the previous subsection are a direct application of the reward model to reasoning problems, though they are training-free. Of course, we can apply this reward model to LLM fine-tuning. This follows a standard paradigm of reinforcement learning. Given a reward model, the LLM, acting as a policy, is fine-tuned using reinforcement learning algorithms. The LLM generates reasoning steps or full solutions, receives feedback (rewards) from the reward model, and updates its parameters to produce outputs</p></div>
<div class="para"><p><em>5.4 Summary</em> 245</p></div>
<div class="para"><p>that maximize these rewards. This process aligns the LLM output with notions of highquality reasoning, thereby encouraging the LLM to generate more reliable reasoning paths.Another key issue is the training of the reward model. Generally, this reward model could be an outcome reward model that evaluates the correctness or quality of the final answer,or a process reward model that assesses the quality of each intermediate reasoning step, as discussed in the context of step-level verifiers. In some cases, we can even develop a reward model based on simple rules, such as giving bonuses to longer outputs.</p></div>
<div class="para"><p>•<strong> Knowledge Distillation for Reasoning</strong>. In this approach, a smaller, more efficient student LLM is trained to mimic the reasoning outputs or internal representations of a larger, more capable teacher LLM. The teacher model might generate detailed reasoning steps for a variety of problems. The student model then learns to reproduce these high-quality reasoning demonstrations. This strategy makes stronger reasoning capabilities more accessible by deploying them in smaller models that are less computationally expensive at inference time.</p></div>
<div class="para"><p>•<strong> Iterative Refinement</strong>. Training-based scaling can also involve iterative refinement. For example, an LLM can generate solutions to a set of problems. These solutions and their reasoning paths are then verified, either by humans or automatic verifiers. The correct reasoning paths are subsequently added to the training data, and the LLM is further fine-tuned on this augmented dataset. This creates a cycle where the LLM progressively improves its reasoning capabilities through repeated generation, critique, and learning.</p></div>
<div class="para"><p>The primary advantage of these training-based scaling methods is that they endow the LLM with stronger inherent reasoning skills. This directly contributes to improved inference-time scaling in several ways: it can lead to more efficient inference, as the LLM might require less extensive search or fewer generation samples to arrive at a correct solution. Moreover, the base quality of generated steps or solutions is higher. Therefore, a well-trained LLM might generalize its learned reasoning abilities to novel problems more effectively than an LLM relying solely on in-context learning or training-free inference schemes.</p></div>
<div class="para"><p>On the other hand, training-based approaches also present challenges, compared to the trainingfree counterparts. The creation of high-quality, large-scale training datasets for reasoning can be expensive and labor-intensive. The fine-tuning process itself, particularly for the largest LLMs or when using RL, can be computationally intensive and require substantial engineering effort.There is also the risk of the model overfitting to the specific types of problems or reasoning styles present in the training data, potentially limiting its performance on out-of-distribution tasks.</p></div>
<h3>5.4 Summary</h3>
<div class="para"><p>In this chapter, we have discussed the inference issue for LLMs. We have presented the prefillingdecoding framework and related decoding algorithms for LLM inference. Then, we have described several techniques for efficient inference. We have also discussed inference-time scaling,which has been considered one of the most important methods for improving LLM reasoning.</p></div>
<div class="para"><p>Inference over sequential data has long been a concern in AI [Wozengraft and Reiffen, 1961;Viterbi, 1967; Forney, 1972]. In the context of NLP, this line of work dates back to the very early days of speech recognition and statistical machine translation [Koehn, 2010], where researchers</p></div>
<div class="para"><p>246 <em>Inference</em></p></div>
<div class="para"><p>faced the challenge of efficiently searching vast hypothesis spaces to find the most probable output sequence. Techniques like beam search and various pruning strategies were developed then to make this computationally tractable. At that time, models were relatively weak, and much of the research focused on developing powerful search algorithms to reduce search errors. These foundational ideas continue to influence modern approaches.</p></div>
<div class="para"><p>As we enter the era dominated by deep learning methods, models based on deep neural networks have become extremely powerful. Even with very simple search algorithms, these models can achieve excellent results. In this context, inference no longer seems as “important” as it once was, and research attention has gradually shifted toward model architectures, training methods,and scaling up models.</p></div>
<div class="para"><p>However, history tends to repeat itself. With the rise of LLMs, inference has once again attracted significant attention. This renewed focus is primarily manifested in two aspects:</p></div>
<div class="para"><p>• The inference cost for LLMs is very high. For example, efficiently deploying LLMs in highconcurrency, low-latency scenarios remains a challenging problem, making inference efficiency critically important. In this context, efficient architecture designs, optimized search algorithms, and various inference optimization strategies hold substantial practical significance.</p></div>
<div class="para"><p>• Input and output sequence lengths have significantly increased in complex tasks. Especially in tasks like mathematical reasoning, the growth of sequence lengths further highlights the importance of inference efficiency. Moreover, scaling the inference process has recently proven to be an effective way to improve the reasoning capabilities of models. Therefore,achieving efficient inference scaling is emerging as a particularly promising research direction.</p></div>
<div class="para"><p>Inference is now a wide-ranging topic that encompasses many techniques. It involves not only the development of model architectures and decoding algorithms, but is increasingly shaped by the intricate engineering and sophisticated systems-level optimizations required to deploy LLMs effectively and efficiently. Many of these techniques are beyond the scope of NLP or a specific AI area. Instead, the frontier of LLM inference optimization now extends deeply into domains traditionally considered core computer science and engineering. This systemic perspective has brought many new ideas to the study of inference problems. Unfortunately, this chapter cannot cover all relevant techniques — indeed, that would be an almost impossible task in itself. Ultimately, the best way to better understand and master these techniques may still lie in hands-on practice.</p></div>
<h1>Bibliography</h1>
<div class="para"><p>[Agrawal et al., 2023] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani, and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills.<em> arXiv preprint arXiv:2308.16369</em>, 2023.</p></div>
<div class="para"><p>[Agrawal et al., 2024] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra,Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming<em> {</em>Throughput-Latency<em>}</em> tradeoff in<em> {</em>LLM<em>}</em> inference with<em> {</em>Sarathi-Serve<em>}</em>. In<em> 18th USENIX Symposium on Operating Systems Design</em> <em>and Implementation (OSDI 24)</em>, pages 117–134, 2024.</p></div>
<div class="para"><p>[Ainslie et al., 2020] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang.Etc: Encoding long and structured inputs in transformers. In<em> Proceedings of the 2020 Conference on Empirical Methods in</em> <em>Natural Language Processing (EMNLP)</em>, pages 268–284, 2020.</p></div>
<div class="para"><p>[Ainslie et al., 2023] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multihead checkpoints. In<em> Proceedings of the 2023 Conference on Empirical Methods in Natural Language</em> <em>Processing</em>, pages 4895–4901, 2023.</p></div>
<div class="para"><p>[Akyürek et al., 2023] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.What learning algorithm is in-context learning? investigations with linear models. In<em> Proceedings of</em> <em>The Eleventh International Conference on Learning Representations</em>, 2023.</p></div>
<div class="para"><p>[Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in language and vision.<em> Advances in Neural Information Processing Systems</em>, 35:22300–22312, 2022.</p></div>
<div class="para"><p>[Allal et al., 2024] Loubna Ben Allal, Anton Lozhkov, and Daniel van Strien. cosmopedia: how to create large-scale synthetic data for pre-training. https://huggingface.co/blog/cosmopedia, 2024.</p></div>
<div class="para"><p>[Almazrouei et al., 2023] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language models.<em> arXiv preprint arXiv:2311.16867</em>, 2023.</p></div>
<div class="para"><p>[Andreas et al., 2016] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In<em> Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 39–48, 2016.</p></div>
<div class="para"><p>[Arjovsky et al., 2016] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In<em> International conference on machine learning</em>, pages 1120–1128, 2016.</p></div>
<div class="para"><p>[Aschenbrenner, 2024] Leopold Aschenbrenner. Situational awareness: The decade ahead, 2024. URL</p></div>
<div class="para"><p>https://situational-awareness.ai/.</p></div>
<div class="para"><p>[Askell et al., 2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds,Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown,Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment.<em> arXiv preprint arXiv:2112.00861</em>, 2021.</p></div>
<div class="para"><p>[Bach et al., 2022] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V.Nayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Févry, Zaid Alyafeai, Manan Dey,Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang,Dragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and repository for natural language prompts. In<em> Proceedings of the 60th Annual</em> <em>Meeting of the Association for Computational Linguistics: System Demonstrations</em>, pages 93–104, 2022.</p></div>
<div class="para"><p>247</p></div>
<div class="para"><p>248 <em>Inference</em></p></div>
<div class="para"><p>[Bengio et al., 2003] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model.<em> Journal of Machine Learning Research</em>, 3:1137–1155, 2003.</p></div>
<div class="para"><p>[Bengio et al., 2006] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layerwise training of deep networks.<em> Advances in neural information processing systems</em>, 19, 2006.</p></div>
<div class="para"><p>[Bengio et al., 2024] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian K. Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atilim Gunes Baydin, Sheila A. McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Markus Brauner, and Sören Mindermann. Managing extreme ai risks amid rapid progress.<em> Science</em>, 384(6698):842–845, 2024.</p></div>
<div class="para"><p>[Bentivogli and Giampiccolo, 2011] Luisa Bentivogli and Danilo Giampiccolo. Pascal recognizing textual entailment challenge (rte-7) at tac 2011. https://tac.nist.gov/2011/RTE/, 2011.</p></div>
<div class="para"><p>[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. In<em> Proceedings of</em> <em>the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 17682–17690, 2024.</p></div>
<div class="para"><p>[Biderman et al., 2021] Stella Biderman, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He,Ben Wang, and Phil Wang. Rotary embeddings: A relative revolution. https://blog.eleuther.ai/rotary-embeddings/, 2021.</p></div>
<div class="para"><p>[Bishop, 2006] Christopher M. Bishop.<em> Pattern Recognition and Machine Learning</em>. Springer, 2006.</p></div>
<div class="para"><p>[Blum and Mitchell, 1998] Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In<em> Proceedings of the eleventh annual conference on Computational learning theory</em>, pages 92–100, 1998.</p></div>
<div class="para"><p>[Bradley and Terry, 1952] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons.<em> Biometrika</em>, 39(3/4):324–345, 1952.</p></div>
<div class="para"><p>[Brandon et al., 2024] William Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan Ragan Kelly. Reducing transformer key-value cache size with cross-layer attention.<em> arXiv</em> <em>preprint arXiv:2405.12981</em>, 2024.</p></div>
<div class="para"><p>[Brill, 1992] Eric Brill. A simple rule-based part of speech tagger. In<em> Speech and Natural Language:</em><em>Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992</em>, 1992.</p></div>
<div class="para"><p>[Briski, 2025] Kari Briski. How scaling laws drive smarter, more powerful ai, 2025.</p></div>
<div class="para"><p>[Brown et al., 2024] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling.<em> arXiv preprint arXiv:2407.21787</em>, 2024.</p></div>
<div class="para"><p>[Brown et al., 1993] Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer.The mathematics of statistical machine translation: Parameter estimation.<em> Computational Linguistics</em>,19(2):263–311, 1993.</p></div>
<div class="para"><p>[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.Language models are few-shot learners.<em> Advances in neural information processing systems</em>, 33:1877–1901, 2020.</p></div>
<div class="para"><p>[Bubeck et al., 2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments</p></div>
<div class="para"><p><em>5.4 Summary</em> 249</p></div>
<div class="para"><p>with gpt-4.<em> arXiv preprint arXiv:2303.12712</em>, 2023.</p></div>
<div class="para"><p>[Bulatov et al., 2022] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.<em>Advances in Neural Information Processing Systems</em>, 35:11079–11091, 2022.</p></div>
<div class="para"><p>[Burges et al., 2005] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton,and Greg Hullender. Learning to rank using gradient descent. In<em> Proceedings of the 22nd international</em> <em>conference on Machine learning</em>, pages 89–96, 2005.</p></div>
<div class="para"><p>[Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu.Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.<em>arXiv preprint</em> <em>arXiv:2312.09390</em>, 2023a.</p></div>
<div class="para"><p>[Burns et al., 2023] Collin Burns, Jan Leike, Leopold Aschenbrenner, Jeffrey Wu, Pavel Izmailov, Leo Gao, Bowen Baker, and Jan Hendrik Kirchner. Weak-to-strong generalization, 2023b. URL https://https://openai.com/index/weak-to-strong-generalization.</p></div>
<div class="para"><p>[Caballero et al., 2023] Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In<em> ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation</em> <em>Models</em>, 2023.</p></div>
<div class="para"><p>[Cao et al., 2007] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach to listwise approach. In<em> Proceedings of the 24th international conference on Machine</em> <em>learning</em>, pages 129–136, 2007.</p></div>
<div class="para"><p>[Chang et al., 2024] Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao, and Jingbo Zhu.Efficient prompting methods for large language models: A survey.<em>arXiv preprint</em> <em>arXiv:2404.01077</em>, 2024.</p></div>
<div class="para"><p>[Charniak, 1997] Eugene Charniak. Statistical parsing with a context-free grammar and word statistics.<em>AAAI/IAAI</em>, 2005(598-603):18, 1997.</p></div>
<div class="para"><p>[Chen et al., 2023] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin Zhu. Unleashing the potential of prompt engineering in large language models: a comprehensive review.<em> arXiv preprint</em> <em>arXiv:2310.14735</em>, 2023a.</p></div>
<div class="para"><p>[Chen et al., 2023] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca with fewer data.<em> arXiv preprint arXiv:2307.08701</em>, 2023b.</p></div>
<div class="para"><p>[Chen et al., 2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca with fewer data. In<em> The Twelfth International Conference on Learning Representations</em>, 2024a.</p></div>
<div class="para"><p>[Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation.<em> arXiv preprint arXiv:2306.15595</em>,2023c.</p></div>
<div class="para"><p>[Chen et al., 2020] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks.<em> Advances in</em> <em>neural information processing systems</em>, 33:15834–15846, 2020.</p></div>
<div class="para"><p>[Chen et al., 2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play finetuning converts weak language models to strong language models.<em> arXiv preprint arXiv:2401.01335</em>,2024b.</p></div>
<div class="para"><p>[Chevalier et al., 2023] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In<em> Proceedings of the 2023 Conference on Empirical Methods</em> <em>in Natural Language Processing</em>, pages 3829–3846, 2023.</p></div>
<div class="para"><p>[Chi et al., 2022] Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky.Kerple:</p></div>
<div class="para"><p>250 <em>Inference</em></p></div>
<div class="para"><p>Kernelized relative positional embedding for length extrapolation.<em> Advances in Neural Information Pro-</em><em>cessing Systems</em>, 35:8386–8399, 2022.</p></div>
<div class="para"><p>[Chi et al., 2023] Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. In<em> Proceedings of the 61st Annual</em> <em>Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 13522–13537,2023.</p></div>
<div class="para"><p>[Chiang et al., 2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.URL https://lmsys.org/blog/2023-03-30-vicuna/.</p></div>
<div class="para"><p>[Chowdhery et al., 2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury,Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov,Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways.<em> arXiv preprint arXiv:2204.02311</em>, 2022.</p></div>
<div class="para"><p>[Christiano et al., 2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences.<em> Advances in neural information pro-</em><em>cessing systems</em>, 30, 2017.</p></div>
<div class="para"><p>[Chu et al., 2023] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang,Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chain of thought reasoning: Advances,frontiers and future.<em> arXiv preprint arXiv:2309.15402</em>, 2023.</p></div>
<div class="para"><p>[Chung et al., 2022] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus,Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov,Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.Scaling instruction-finetuned language models.<em> arXiv preprint arXiv:2210.11416</em>, 2022.</p></div>
<div class="para"><p>[Clark et al., 2019] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra:Pre-training text encoders as discriminators rather than generators.In<em> Proceedings of International</em> <em>Conference on Learning Representations</em>, 2019.</p></div>
<div class="para"><p>[Cobbe et al., 2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.<em> arXiv preprint arXiv:2110.14168</em>, 2021.</p></div>
<div class="para"><p>[Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.Unsupervised cross-lingual representation learning at scale. In<em> Proceedings of the 58th Annual Meeting</em> <em>of the Association for Computational Linguistics</em>, pages 8440–8451, 2020.</p></div>
<div class="para"><p>[Coste et al., 2024] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help mitigate overoptimization. In<em> The Twelfth International Conference on Learning Representa-</em><em>tions</em>, 2024.</p></div>
<div class="para"><p>[Cui et al., 2024] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, Zhiyuan Liu, and Maosong Sun. ULTRAFEEDBACK:</p></div>
<div class="para"><p><em>5.4 Summary</em> 251</p></div>
<div class="para"><p>Boosting language models with scaled AI feedback. In<em> Proceedings of the 41st International Conference</em> <em>on Machine Learning</em>, volume 235, pages 9722–9744, 2024.</p></div>
<div class="para"><p>[Dai et al., 2023] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers.In<em> Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 4005–4019, 2023.</p></div>
<div class="para"><p>[Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In<em> Proceed-</em><em>ings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 2978–2988,2019.</p></div>
<div class="para"><p>[Dao et al., 2022] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.<em> Advances in Neural Information Processing</em> <em>Systems</em>, 35:16344–16359, 2022.</p></div>
<div class="para"><p>[Deepseek, 2025] Deepseek. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.<em> arXiv preprint arXiv:2501.12948</em>, 2025.</p></div>
<div class="para"><p>[Dehghani et al., 2018] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers.<em> arXiv preprint arXiv:1807.03819</em>, 2018.</p></div>
<div class="para"><p>[Deletang et al., 2024] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness. Language modeling is compression. In<em> The Twelfth Interna-</em><em>tional Conference on Learning Representations</em>, 2024.</p></div>
<div class="para"><p>[Deng et al., 2022] Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu,Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. In<em> Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>,pages 3369–3391, 2022.</p></div>
<div class="para"><p>[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pretraining of deep bidirectional transformers for language understanding.In<em> Proceedings of the 2019</em> <em>Conference of the North American Chapter of the Association for Computational Linguistics: Human</em> <em>Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, 2019.</p></div>
<div class="para"><p>[Ding et al., 2024] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens.<em> arXiv</em> <em>preprint arXiv:2402.13753</em>, 2024.</p></div>
<div class="para"><p>[Dolan and Brockett, 2005] Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In<em> Proceedings of Third International Workshop on Paraphrasing (IWP2005)</em>, 2005.</p></div>
<div class="para"><p>[Dong et al., 2019] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon.Unified language model pre-training for natural language understanding and generation.<em> Advances in neural information processing systems</em>, 32, 2019.</p></div>
<div class="para"><p>[Dong et al., 2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,Jingjing Xu, and Zhifang Sui. A survey on in-context learning.<em> arXiv preprint arXiv:2301.00234</em>, 2022.</p></div>
<div class="para"><p>[Dong et al., 2021] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In<em> International Conference on Machine</em> <em>Learning</em>, pages 2793–2803. PMLR, 2021.</p></div>
<div class="para"><p>[Drozdov et al., 2022] Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song,Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. In<em> Proceedings of The Eleventh International Conference on Learning Representations</em>, 2022.</p></div>
<div class="para"><p>[Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for decomposing complex questions. In<em> Proceedings of the 2022 Conference on Empirical Methods in</em> <em>Natural Language Processing</em>, pages 1251–1265, 2022.</p></div>
<div class="para"><p>252 <em>Inference</em></p></div>
<div class="para"><p>[Dubey et al., 2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.<em> arXiv preprint arXiv:2407.21783</em>, 2024.</p></div>
<div class="para"><p>[Dubois et al., 2024] Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback.<em> Advances in Neural Information Processing Systems</em>, 36,2024.</p></div>
<div class="para"><p>[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour,DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, and Peter Shaw.Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking.<em> arXiv</em> <em>preprint arXiv:2312.09244</em>, 2023.</p></div>
<div class="para"><p>[Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search:A survey.<em> Journal of Machine Learning Research</em>, 20(55):1–21, 2019.</p></div>
<div class="para"><p>[Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised pre-training help deep learning? In<em> Proceedings of the thirteenth international conference</em> <em>on artificial intelligence and statistics</em>, pages 201–208, 2010.</p></div>
<div class="para"><p>[Fan et al., 2018] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:</em><em>Long Papers)</em>, pages 889–898, 2018.</p></div>
<div class="para"><p>[Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In<em> Proceedings of International Conference on Learning Representations</em>, 2019.</p></div>
<div class="para"><p>[Fedus et al., 2022] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.<em> The Journal of Machine Learning Research</em>,23(1):5232–5270, 2022.</p></div>
<div class="para"><p>[Fernandes et al., 2023] Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, and André F. T. Martins. Bridging the gap: A survey on integrating (human) feedback for natural language generation.<em> Transactions of the Association for Computational Linguistics</em>, 11:1643–1668, 2023.</p></div>
<div class="para"><p>[Forney, 1972] GDJR Forney. Maximum-likelihood sequence estimation of digital sequences in the presence of intersymbol interference.<em> IEEE Transactions on Information theory</em>, 18(3):363–378, 1972.</p></div>
<div class="para"><p>[Franklin and Graesser, 1996] Stan Franklin and Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. In<em> International workshop on agent theories, architectures, and languages</em>,pages 21–35. Springer, 1996.</p></div>
<div class="para"><p>[Frensch and Funke, 2014] Peter A Frensch and Joachim Funke.<em> Complex problem solving: The European</em> <em>perspective</em>. Psychology Press, 2014.</p></div>
<div class="para"><p>[Gale et al., 2019] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks.<em>arXiv preprint arXiv:1902.09574</em>, 2019.</p></div>
<div class="para"><p>[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain,Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau,Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, and Jared Kaplan.The capacity for moral self-correction in large language models.<em> arXiv preprint arXiv:2302.07459</em>, 2023.</p></div>
<div class="para"><p>[Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In<em> International Conference on Machine Learning</em>, pages 10835–10866. PMLR, 2023a.</p></div>
<div class="para"><p><em>5.4 Summary</em> 253</p></div>
<div class="para"><p>[Gao et al., 2023] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In<em> International Conference on</em> <em>Machine Learning</em>, pages 10764–10799. PMLR, 2023b.</p></div>
<div class="para"><p>[Gao et al., 2023] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey.<em> arXiv</em> <em>preprint arXiv:2312.10997</em>, 2023c.</p></div>
<div class="para"><p>[Garg et al., 2022] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes.<em> Advances in Neural Information</em> <em>Processing Systems</em>, 35:30583–30598, 2022.</p></div>
<div class="para"><p>[Ge et al., 2024] Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Hongxia Ma, Li Zhang, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and Jingbo Zhu. Clustering and ranking: Diversity-preserved instruction selection through expert-aligned quality estimation.<em> arXiv preprint</em> <em>arXiv:2402.18191</em>, 2024.</p></div>
<div class="para"><p>[Gemma Team, 2024] Google DeepMind Gemma Team. Gemma: Open Models Based on Gemini Research and Technology, 2024.</p></div>
<div class="para"><p>[Goodhart, 1984] Charles AE Goodhart.<em> Problems of monetary management: the UK experience</em>. Springer,1984.</p></div>
<div class="para"><p>[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws for neural machine translation. In<em> Proceedings of the 2021 Conference on Empirical Methods in Natural</em> <em>Language Processing</em>, pages 5915–5922, 2021.</p></div>
<div class="para"><p>[Gou et al., 2024] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al.Critic: Large language models can self-correct with tool-interactive critiquing. In<em> The Twelfth Interna-</em><em>tional Conference on Learning Representations</em>, 2024.</p></div>
<div class="para"><p>[Gu and Dao, 2023] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.<em> arXiv preprint arXiv:2312.00752</em>, 2023.</p></div>
<div class="para"><p>[Gunasekar et al., 2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need.<em> arXiv preprint arXiv:2306.11644</em>, 2023.</p></div>
<div class="para"><p>[Guo et al., 2024] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. In<em> The Twelfth International Conference on Learning Representations</em>, 2024.</p></div>
<div class="para"><p>[Gupta and Berant, 2020] Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers.<em> arXiv preprint arXiv:2006.03274</em>, 2020.</p></div>
<div class="para"><p>[Gupta et al., 2021] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memoryefficient transformers via top-k attention. In<em> Proceedings of the Second Workshop on Simple and Efficient</em> <em>Natural Language Processing</em>, pages 39–52, 2021.</p></div>
<div class="para"><p>[Han et al., 2021] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu,Liang Zhang, Wentao Han, Minlie Huang, Qin Jin, Yanyan Lan, Yang Liu, Zhiyuan Liu, Zhiwu Lu,Xipeng Qiu, Ruihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan, Wayne Xin Zhao, and Jun Zhu. Pretrained models: Past, present and future.<em> AI Open</em>, 2:225–250, 2021.</p></div>
<div class="para"><p>[Han et al., 2024] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large models: A comprehensive survey.<em> arXiv preprint arXiv:2403.14608</em>, 2024.</p></div>
<div class="para"><p>[Harlap et al., 2018] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training.<em> arXiv</em> <em>preprint arXiv:1806.03377</em>, 2018.</p></div>
<div class="para"><p>[He et al., 2019] Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. In</p></div>
<div class="para"><p>254 <em>Inference</em></p></div>
<div class="para"><p><em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 4918–4927, 2019.</p></div>
<div class="para"><p>[He et al., 2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decodingenhanced bert with disentangled attention. In<em> Proceedings of International Conference on Learning</em> <em>Representations</em>, 2021.</p></div>
<div class="para"><p>[Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus).<em>arXiv preprint arXiv:1606.08415</em>, 2016.</p></div>
<div class="para"><p>[Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan,and Dawn Song. Pretrained transformers improve out-of-distribution robustness. In<em> Proceedings of the</em> <em>58th Annual Meeting of the Association for Computational Linguistics</em>, pages 2744–2751, 2020.</p></div>
<div class="para"><p>[Hendrycks et al., 2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In<em> Proceedings of</em> <em>International Conference on Learning Representations</em>, 2021.</p></div>
<div class="para"><p>[Hestness et al., 2017] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically.<em> arXiv preprint arXiv:1712.00409</em>, 2017.</p></div>
<div class="para"><p>[Hewitt, 2024] John Hewitt. Instruction following without instruction tuning, 2024. URL https://nlp.stanford.edu/~johnhew/instruction-following.html.</p></div>
<div class="para"><p>[Hewitt et al., 2024] John Hewitt, Nelson F Liu, Percy Liang, and Christopher D Manning. Instruction following without instruction tuning.<em> arXiv preprint arXiv:2409.14254</em>, 2024.</p></div>
<div class="para"><p>[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory.<em>Neural computation</em>, 9(8):1735–1780, 1997.</p></div>
<div class="para"><p>[Hoffmann et al., 2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models.<em> arXiv preprint arXiv:2203.15556</em>, 2022.</p></div>
<div class="para"><p>[Holtzman et al., 2020] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In<em> International Conference on Learning Representations</em>, 2020.</p></div>
<div class="para"><p>[Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language models with (almost) no human labor. In<em> Proceedings of the 61st Annual Meeting</em> <em>of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14409–14428, 2023.</p></div>
<div class="para"><p>[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In<em> Proceedings of the 36th International Conference on Machine Learning</em>, pages 2790–2799. PMLR, 2019.</p></div>
<div class="para"><p>[Hu et al., 2022] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In<em> International</em> <em>Conference on Learning Representations</em>, 2022.</p></div>
<div class="para"><p>[Huang, 2009] Liang Huang. Dynamic programming-based search algorithms in NLP. In<em> Proceedings</em> <em>of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the</em> <em>Association for Computational Linguistics, Companion Volume: Tutorial Abstracts</em>, 2009.</p></div>
<div class="para"><p>[Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism.<em> Advances in neural information processing</em> <em>systems</em>, 32, 2019.</p></div>
<div class="para"><p>[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.Block-recurrent transformers.<em>Advances in neural information processing systems</em>, 35:</p></div>
<div class="para"><p><em>5.4 Summary</em> 255</p></div>
<div class="para"><p>33248–33261, 2022.</p></div>
<div class="para"><p>[Jelinek, 1998] Frederick Jelinek.<em> Statistical methods for speech recognition</em>. MIT Press, 1998.</p></div>
<div class="para"><p>[Jiang et al., 2023] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b.<em> arXiv preprint arXiv:2310.06825</em>,2023a.</p></div>
<div class="para"><p>[Jiang et al., 2023] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua:Compressing prompts for accelerated inference of large language models. In<em> Proceedings of the 2023</em> <em>Conference on Empirical Methods in Natural Language Processing</em>, pages 13358–13376, 2023b.</p></div>
<div class="para"><p>[Jiang et al., 2020] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know?<em>Transactions of the Association for Computational Linguistics</em>, 8:423–438,2020.</p></div>
<div class="para"><p>[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,and Qun Liu. Tinybert: Distilling bert for natural language understanding. In<em> Findings of the Association</em> <em>for Computational Linguistics: EMNLP 2020</em>, pages 4163–4174, 2020.</p></div>
<div class="para"><p>[Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.In<em> Proceedings of the 55th</em> <em>Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1601–1611, 2017.</p></div>
<div class="para"><p>[Joshi et al., 2020] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.Spanbert: Improving pre-training by representing and predicting spans.<em>Transactions of the</em> <em>association for computational linguistics</em>, 8:64–77, 2020.</p></div>
<div class="para"><p>[Jurafsky and Martin, 2008] Dan Jurafsky and James H. Martin.<em> Speech and Language Processing (2nd</em> <em>ed.)</em>. Prentice Hall, 2008.</p></div>
<div class="para"><p>[Kahneman, 2011] Daniel Kahneman.<em> Thinking, fast and slow</em>. macmillan, 2011.</p></div>
<div class="para"><p>[Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.<em> arXiv preprint arXiv:2001.08361</em>, 2020.</p></div>
<div class="para"><p>[Katharopoulos et al., 2020] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.Transformers are rnns: Fast autoregressive transformers with linear attention. In<em> International conference</em> <em>on machine learning</em>, pages 5156–5165. PMLR, 2020.</p></div>
<div class="para"><p>[Khandelwal et al., 2020] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.Generalization through memorization: Nearest neighbor language models.In<em> International</em> <em>Conference on Learning Representations</em>, 2020.</p></div>
<div class="para"><p>[Khot et al., 2023] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark,and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In <em>Proceedings of The Eleventh International Conference on Learning Representations</em>, 2023.</p></div>
<div class="para"><p>[Kim et al., 2023] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, and Amir Gholami. Full stack optimization of transformer inference: a survey.<em> arXiv preprint</em> <em>arXiv:2302.14017</em>, 2023.</p></div>
<div class="para"><p>[Kirkpatrick et al., 2017] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska,Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.<em> Proceedings of the national academy of sciences</em>, 114(13):3521–3526,2017.</p></div>
<div class="para"><p>256 <em>Inference</em></p></div>
<div class="para"><p>[Koehn, 2010] Philipp Koehn.<em> Statistical Machine Translation</em>. Cambridge University Press, 2010.</p></div>
<div class="para"><p>[Kojima et al., 2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners.<em> Advances in neural information processing</em> <em>systems</em>, 35:22199–22213, 2022.</p></div>
<div class="para"><p>[Korthikanti et al., 2023] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee,Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models.<em> Proceedings of Machine Learning and Systems</em>, 5, 2023.</p></div>
<div class="para"><p>[Krakovna et al.,2020]Victoria Krakovna,Jonathan Uesato,Vladimir Mikulik,Matthew Rahtz,Tom Everitt,Ramana Kumar,Zac Kenton,Jan Leike,and Shane Legg.Specification gaming:the flip side of ai ingenuity.https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity, 2020.</p></div>
<div class="para"><p>[Kumar and Byrne, 2004] Shankar Kumar and William Byrne. Minimum bayes-risk decoding for statistical machine translation. In<em> Proceedings of the Human Language Technology Conference of the North</em> <em>American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</em>, pages 169–176,2004.</p></div>
<div class="para"><p>[Kung and Peng, 2023] Po-Nien Kung and Nanyun Peng. Do models really learn to follow instructions?an empirical study of instruction tuning.<em> arXiv preprint arXiv:2305.11383</em>, 2023.</p></div>
<div class="para"><p>[Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention.<em> arXiv preprint arXiv:2309.06180</em>, 2023.</p></div>
<div class="para"><p>[Lake and Baroni, 2018] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In<em> International conference on</em> <em>machine learning</em>, pages 2873–2882. PMLR, 2018.</p></div>
<div class="para"><p>[Lambert et al., 2024] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi.Rewardbench: Evaluating reward models for language modeling.<em>arXiv preprint</em> <em>arXiv:2403.13787</em>, 2024.</p></div>
<div class="para"><p>[Lample and Conneau, 2019] Guillaume Lample and Alexis Conneau.Cross-lingual language model pretraining.<em> arXiv preprint arXiv:1901.07291</em>, 2019.</p></div>
<div class="para"><p>[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. In<em> Proceedings</em> <em>of International Conference on Learning Representations</em>, 2020.</p></div>
<div class="para"><p>[Lee et al., 2023] Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback.<em> arXiv preprint arXiv:2309.00267</em>, 2023.</p></div>
<div class="para"><p>[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameterefficient prompt tuning. In<em> Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-</em><em>guage Processing</em>, pages 3045–3059, 2021.</p></div>
<div class="para"><p>[Leviathan et al., 2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In<em> Proceedings of International Conference on Machine Learning</em>, pages 19274–19286. PMLR, 2023.</p></div>
<div class="para"><p>[Lewis et al., 2020] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In<em> Proceedings of the 58th</em> <em>Annual Meeting of the Association for Computational Linguistics</em>, pages 7871–7880, 2020.</p></div>
<div class="para"><p>[Li et al., 2024] Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. Llm inference serving:Survey of recent advances and opportunities.<em> arXiv preprint arXiv:2407.12391</em>, 2024a.</p></div>
<div class="para"><p><em>5.4 Summary</em> 257</p></div>
<div class="para"><p>[Li et al., 2023] Bei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong Xiao, Jiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text generation.<em> arXiv preprint arXiv:2305.19835</em>, 2023a.</p></div>
<div class="para"><p>[Li, 2011] Hang Li.<em> Learning to Rank for Information Retrieval and Natural Language Processing</em>. Online access: Morgan &amp; Claypool Synthesis Collection Five. Morgan &amp; Claypool Publishers, 2011. ISBN 9781608457076.</p></div>
<div class="para"><p>[Li et al., 2022] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. A survey on retrievalaugmented text generation.<em> arXiv preprint arXiv:2202.01110</em>, 2022.</p></div>
<div class="para"><p>[Li et al., 2024] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. In<em> The Twelfth International Conference on</em> <em>Learning Representations</em>, 2024b.</p></div>
<div class="para"><p>[Li et al., 2023] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. In<em> Proceedings of the 61st Annual Meeting</em> <em>of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2391–2404, 2023b.</p></div>
<div class="para"><p>[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In<em> Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</em> <em>and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>,pages 4582–4597, 2021.</p></div>
<div class="para"><p>[Li, 2023] Yinheng Li. A practical survey on zero-shot prompt design for in-context learning. In<em> Proceed-</em><em>ings of the 14th International Conference on Recent Advances in Natural Language Processing</em>, pages 641–647, 2023.</p></div>
<div class="para"><p>[Li et al., 2023] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In<em> Proceedings of the 2023 Conference on Empirical</em> <em>Methods in Natural Language Processing</em>, pages 6342–6353, 2023c.</p></div>
<div class="para"><p>[Lialin et al., 2023] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up:A guide to parameter-efficient fine-tuning.<em> arXiv preprint arXiv:2303.15647</em>, 2023.</p></div>
<div class="para"><p>[Lightman et al., 2024] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker,Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In<em> The</em> <em>Twelfth International Conference on Learning Representations</em>, 2024.</p></div>
<div class="para"><p>[Liu et al., 2024] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report.<em> arXiv preprint</em> <em>arXiv:2412.19437</em>, 2024a.</p></div>
<div class="para"><p>[Liu et al., 2022] Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In<em> Proceedings of Deep Learning Inside</em> <em>Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning</em> <em>Architectures</em>, pages 100–114, 2022.</p></div>
<div class="para"><p>[Liu et al., 2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.<em> ACM Computing Surveys</em>, 55(9):1–35, 2023a.</p></div>
<div class="para"><p>[Liu et al., 2024] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. In<em> The Twelfth International</em> <em>Conference on Learning Representations</em>, 2024b.</p></div>
<div class="para"><p>[Liu, 2009] Tie-Yan Liu. Learning to rank for information retrieval.<em> Foundations and Trends® in Informa-</em><em>tion Retrieval</em>, 3(3):225–331, 2009.</p></div>
<div class="para"><p>[Liu et al., 2023] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too.<em> AI Open</em>, 2023b.</p></div>
<div class="para"><p>258 <em>Inference</em></p></div>
<div class="para"><p>[Liu et al., 2023] Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai Wang, and Dongxia Wang. Prompting frameworks for large language models: A survey.<em> arXiv preprint</em> <em>arXiv:2311.12785</em>, 2023c.</p></div>
<div class="para"><p>[Liu et al., 2024] Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang,Tong Xiao, and Jingbo Zhu. Forgetting curve: A reliable method for evaluating memorization capability for long-context models.In<em> Proceedings of the 2024 Conference on Empirical Methods in Natural</em> <em>Language Processing</em>, pages 4667–4682, 2024c.</p></div>
<div class="para"><p>[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.<em> arXiv preprint arXiv:1907.11692</em>, 2019.</p></div>
<div class="para"><p>[Longpre et al., 2023] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay,Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning. In<em> International Conference on Machine Learning</em>,pages 22631–22648. PMLR, 2023.</p></div>
<div class="para"><p>[Ma et al., 2023] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig,Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In<em> The Eleventh</em> <em>International Conference on Learning Representations</em>, 2023.</p></div>
<div class="para"><p>[Ma et al., 2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length.<em> arXiv preprint arXiv:2404.08801</em>, 2024.</p></div>
<div class="para"><p>[Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-refine: Iterative refinement with self-feedback.<em> Advances in Neural Information Processing Sys-</em><em>tems</em>, 36, 2024.</p></div>
<div class="para"><p>[Manning, 2022] Christopher D Manning. Human language understanding &amp; reasoning.<em> Daedalus</em>, 151(2):127–138, 2022.</p></div>
<div class="para"><p>[Marcus, 1993] Gary F Marcus. Negative evidence in language acquisition.<em> Cognition</em>, 46(1):53–85, 1993.</p></div>
<div class="para"><p>[Martins et al., 2022] Pedro Henrique Martins, Zita Marinho, and André FT Martins.<em> ∞</em>-former: Infinite memory transformer-former: Infinite memory transformer. In<em> Proceedings of the 60th Annual Meeting</em> <em>of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 5468–5485, 2022.</p></div>
<div class="para"><p>[Mavi et al., 2024] Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. Multi-hop question answering.<em>Foundations and Trends® in Information Retrieval</em>, 17(5):457–586, 2024.</p></div>
<div class="para"><p>[Michel et al., 2019] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one?<em> Advances in neural information processing systems</em>, 32, 2019.</p></div>
<div class="para"><p>[Micikevicius et al., 2018] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In<em> Proceedings of International Conference on Learning Representations</em>,2018.</p></div>
<div class="para"><p>[Miettinen, 1999] Kaisa Miettinen.<em> Nonlinear multiobjective optimization</em>, volume 12. Springer Science&amp; Business Media, 1999.</p></div>
<div class="para"><p>[Mikolov et al., 2013] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In<em> Proceedings of the International Conference on Learning</em> <em>Representations (ICLR 2013)</em>, 2013a.</p></div>
<div class="para"><p>[Mikolov et al., 2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In<em> Proceedings of the 26th In-</em><em>ternational Conference on Neural Information Processing Systems - Volume 2</em>, pages 3111–3119, 2013b.</p></div>
<div class="para"><p><em>5.4 Summary</em> 259</p></div>
<div class="para"><p>[Min et al., 2019] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop reading comprehension through question decomposition and rescoring. In<em> Proceedings of the 57th Annual</em> <em>Meeting of the Association for Computational Linguistics</em>, pages 6097–6109, 2019.</p></div>
<div class="para"><p>[Minaee et al., 2024] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao.Large language models: A survey.<em>arXiv preprint</em> <em>arXiv:2402.06196</em>, 2024.</p></div>
<div class="para"><p>[Mishra et al., 2022] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Crosstask generalization via natural language crowdsourcing instructions. In<em> Proceedings of the 60th Annual</em> <em>Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 3470–3487,2022.</p></div>
<div class="para"><p>[Mnih et al., 2016] Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley,Timothy P Lillicrap, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In<em> Proceedings of the 33rd International Conference on International Conference on</em> <em>Machine Learning</em>, pages 1928–1937, 2016.</p></div>
<div class="para"><p>[Mohtashami and Jaggi, 2024] Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for transformers.<em> Advances in Neural Information Processing Systems</em>, 36, 2024.</p></div>
<div class="para"><p>[Mu et al., 2024] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.<em>Advances in Neural Information Processing Systems</em>, 36, 2024.</p></div>
<div class="para"><p>[Munkhdalai et al., 2024] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention.<em> arXiv preprint arXiv:2404.07143</em>,2024.</p></div>
<div class="para"><p>[Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.Webgpt: Browser-assisted question-answering with human feedback.<em> arXiv preprint arXiv:2112.09332</em>,2021.</p></div>
<div class="para"><p>[Narayanan et al., 2021] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. In<em> Proceedings of the International Conference for High Performance</em> <em>Computing, Networking, Storage and Analysis</em>, pages 1–15, 2021.</p></div>
<div class="para"><p>[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In<em> Proceedings of the Sixteenth International</em> <em>Conference on Machine Learning</em>, pages 278–287, 1999.</p></div>
<div class="para"><p>[Nvidia,2025]Nvidia.Nvidia nim llms benchmarking.https://docs.nvidia.com/nim/benchmarking/llm/latest/metrics.html, 2025. Retrieved 2025-03-17.</p></div>
<div class="para"><p>[OpenAI, 2024] OpenAI. Learning to reason with llms, September 2024. URL https://openai.com/index/learning-to-reason-with-llms/.</p></div>
<div class="para"><p>[Ouyang et al., 2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback.<em> Advances</em> <em>in Neural Information Processing Systems</em>, 35:27730–27744, 2022.</p></div>
<div class="para"><p>[Pal et al., 2023] Koyena Pal, Jiuding Sun, Andrew Yuan, Byron C Wallace, and David Bau. Future lens:Anticipating subsequent tokens from a single hidden state. In<em> Proceedings of the 27th Conference on</em> <em>Computational Natural Language Learning (CoNLL)</em>, pages 548–560, 2023.</p></div>
<div class="para"><p>260 <em>Inference</em></p></div>
<div class="para"><p>[Pan et al., 2022] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. In<em> International Conference on Learning Representa-</em><em>tions</em>, 2022.</p></div>
<div class="para"><p>[Pan et al., 2024] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang.Automatically correcting large language models: Surveying the landscape of diverse automated correction strategies.<em> Transactions of the Association for Computational Linguistics</em>,12:484–506, 2024.</p></div>
<div class="para"><p>[Parisi et al., 2022] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models.<em>arXiv preprint arXiv:2205.12255</em>, 2022.</p></div>
<div class="para"><p>[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter.Continual lifelong learning with neural networks: A review.<em> Neural networks</em>, 113:54–71, 2019.</p></div>
<div class="para"><p>[Parmar et al., 2018] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,Alexander Ku, and Dustin Tran. Image transformer. In<em> International conference on machine learn-</em><em>ing</em>, pages 4055–4064. PMLR, 2018.</p></div>
<div class="para"><p>[Patel et al., 2024] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In <em>2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)</em>, pages 118–132. IEEE, 2024.</p></div>
<div class="para"><p>[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.<em> arXiv</em> <em>preprint arXiv:2306.01116</em>, 2023.</p></div>
<div class="para"><p>[Peng et al., 2024] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. In<em> The Twelfth International Conference on Learning</em> <em>Representations</em>, 2024.</p></div>
<div class="para"><p>[Pennington et al., 2014] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In<em> Proceedings of Empirical Methods in Natural Language Processing</em><em>(EMNLP)</em>, pages 1532–1543, 2014.</p></div>
<div class="para"><p>[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In<em> Proceedings of the</em> <em>2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-</em><em>man Language Technologies, Volume 1 (Long Papers)</em>, 2018.</p></div>
<div class="para"><p>[Plackett, 1975] Robin L Plackett. The analysis of permutations.<em> Journal of the Royal Statistical Society</em> <em>Series C: Applied Statistics</em>, 24(2):193–202, 1975.</p></div>
<div class="para"><p>[Pope et al., 2023] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.In<em> Proceedings of Machine Learning and Systems</em>, 2023.</p></div>
<div class="para"><p>[Prasad et al., 2023] Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, editbased instruction search for prompting large language models. In<em> Proceedings of the 17th Conference of</em> <em>the European Chapter of the Association for Computational Linguistics</em>, pages 3845–3864, 2023.</p></div>
<div class="para"><p>[Press et al., 2022] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In<em> Proceedings of International Conference on Learning</em> <em>Representations</em>, 2022.</p></div>
<div class="para"><p>[Press et al., 2023] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.Measuring and narrowing the compositionality gap in language models. In<em> Findings of the Association</em> <em>for Computational Linguistics: EMNLP 2023</em>, pages 5687–5711, 2023.</p></div>
<div class="para"><p>[Pryzant et al., 2023] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng.</p></div>
<div class="para"><p><em>5.4 Summary</em> 261</p></div>
<div class="para"><p>Automatic prompt optimization with &quot;gradient descent&quot; and beam search. In<em> The 2023 Conference on</em> <em>Empirical Methods in Natural Language Processing</em>, 2023.</p></div>
<div class="para"><p>[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.Pre-trained models for natural language processing: A survey.<em> Science China Technological Sciences</em>,63(10):1872–1897, 2020.</p></div>
<div class="para"><p>[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training.<em> OpenAI</em>, 2018.</p></div>
<div class="para"><p>[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.<em> OpenAI blog</em>, 1(8), 2019.</p></div>
<div class="para"><p>[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In<em> International</em> <em>conference on machine learning</em>, pages 8748–8763. PMLR, 2021.</p></div>
<div class="para"><p>[Rae et al., 2019] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In<em> International Conference on</em> <em>Learning Representations</em>, 2019.</p></div>
<div class="para"><p>[Rafailov et al., 2024] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.<em> Advances in Neural Information Processing Systems</em>, 36, 2024.</p></div>
<div class="para"><p>[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.<em> Journal of Machine Learning Research</em>, 21(140):1–67, 2020.</p></div>
<div class="para"><p>[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions.<em> arXiv preprint arXiv:1710.05941</em>, 2017.</p></div>
<div class="para"><p>[Rolnick et al., 2019] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning.<em> Advances in Neural Information Processing Systems</em>,32, 2019.</p></div>
<div class="para"><p>[Rosenfeld et al., 2020] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. In<em> Proceedings of International Conference</em> <em>on Learning Representations</em>, 2020.</p></div>
<div class="para"><p>[Ruan et al., 2024] Junhao Ruan, Long Meng, Weiqiao Shan, Tong Xiao, and Jingbo Zhu. A survey of llm surveys. https://github.com/NiuTrans/ABigSurveyOfLLMs, 2024.</p></div>
<div class="para"><p>[Rubin et al., 2022] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In<em> Proceedings of the 2022 Conference of the North American Chapter of the</em> <em>Association for Computational Linguistics: Human Language Technologies</em>, pages 2655–2671, 2022.</p></div>
<div class="para"><p>[Russell, 2019] Stuart Russell.<em> Human Compatible: Artificial Intelligence and the Problem of Controls</em>.Viking, 2019.</p></div>
<div class="para"><p>[Sanh et al., 2020] Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning: Adaptive sparsity by fine-tuning.<em> Advances in Neural Information Processing Systems</em>, 33:20378–20389, 2020.</p></div>
<div class="para"><p>[Sanh et al., 2022] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak,Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In<em> Proceedings of International Conference on Learning Representations</em>, 2022.</p></div>
<div class="para"><p>262 <em>Inference</em></p></div>
<div class="para"><p>[Schick et al., 2023] Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. PEER: A collaborative language model. In<em> Proceedings of The Eleventh International Conference on Learning Representations</em>,2023.</p></div>
<div class="para"><p>[Schick et al., 2024] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.<em> Advances in Neural Information Processing Systems</em>, 36, 2024.</p></div>
<div class="para"><p>[Schmidhuber, 2015] Jürgen Schmidhuber. Deep learning in neural networks: An overview.<em> Neural net-</em><em>works</em>, 61:85–117, 2015.</p></div>
<div class="para"><p>[Schulman et al., 2015] John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter Abbeel.Trust region policy optimization. In<em> Proceedings of the 32nd International Conference on International</em> <em>Conference on Machine Learning-Volume 37</em>, pages 1889–1897, 2015.</p></div>
<div class="para"><p>[Schulman et al., 2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.Proximal policy optimization algorithms.<em> arXiv preprint arXiv:1707.06347</em>, 2017.</p></div>
<div class="para"><p>[Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In<em> Proceedings of the 54th Annual Meeting of the Association</em> <em>for Computational Linguistics (Volume 1: Long Papers)</em>, pages 86–96, 2016.</p></div>
<div class="para"><p>[Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. In<em> Proceedings of International Conference on Learning</em> <em>Representations</em>, 2017.</p></div>
<div class="para"><p>[Shannon, 1951] Claude E Shannon. Prediction and entropy of printed english.<em> Bell system technical</em> <em>journal</em>, 30(1):50–64, 1951.</p></div>
<div class="para"><p>[Shaw et al., 2018] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In<em> Proceedings of the 2018 Conference of the North American Chapter of the Associ-</em><em>ation for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 464–468, 2018.</p></div>
<div class="para"><p>[Shazeer, 2019] Noam Shazeer. Fast transformer decoding: One write-head is all you need.<em> arXiv preprint</em> <em>arXiv:1911.02150</em>, 2019.</p></div>
<div class="para"><p>[Shazeer, 2020] Noam Shazeer. Glu variants improve transformer.<em> arXiv preprint arXiv:2002.05202</em>, 2020.</p></div>
<div class="para"><p>[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In<em> Proceed-</em><em>ings of the AAAI Conference on Artificial Intelligence</em>, volume 34, pages 8815–8821, 2020.</p></div>
<div class="para"><p>[Shinn et al., 2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning.<em> Advances in Neural Information</em> <em>Processing Systems</em>, 36:8634–8652, 2023.</p></div>
<div class="para"><p>[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.<em> arXiv preprint arXiv:1909.08053</em>, 2019.</p></div>
<div class="para"><p>[Skalse et al., 2022] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming.<em> Advances in Neural Information Processing Systems</em>, 35:9460–9471,2022.</p></div>
<div class="para"><p>[Snell et al., 2022] Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context.<em> arXiv</em> <em>preprint arXiv:2209.15189</em>, 2022.</p></div>
<div class="para"><p>[Snell et al., 2024] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters.<em> arXiv preprint arXiv:2408.03314</em>, 2024.</p></div>
<div class="para"><p><em>5.4 Summary</em> 263</p></div>
<div class="para"><p>[Snell et al., 2025] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM testtime compute optimally can be more effective than scaling parameters for reasoning. In<em> The Thirteenth</em> <em>International Conference on Learning Representations</em>, 2025.</p></div>
<div class="para"><p>[Socher et al., 2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In<em> Proceedings of the 2013 conference on empirical methods in natural language</em> <em>processing</em>, pages 1631–1642, 2013.</p></div>
<div class="para"><p>[Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence pre-training for language generation. In<em> International Conference on Machine Learning</em>,pages 5926–5936. PMLR, 2019.</p></div>
<div class="para"><p>[Stiennon et al., 2020] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.<em>Advances in Neural Information Processing Systems</em>, 33:3008–3021, 2020.</p></div>
<div class="para"><p>[Su et al., 2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.<em> Neurocomputing</em>, 568:127063, 2024.</p></div>
<div class="para"><p>[Su et al., 2022] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A contrastive framework for neural text generation.<em> Advances in Neural Information Processing Systems</em>,35:21548–21561, 2022.</p></div>
<div class="para"><p>[Sun et al., 2020] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.Mobilebert: a compact task-agnostic bert for resource-limited devices. In<em> Proceedings of the 58th Annual</em> <em>Meeting of the Association for Computational Linguistics</em>, pages 2158–2170, 2020.</p></div>
<div class="para"><p>[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.<em> Advances in neural information processing systems</em>, 27, 2014.</p></div>
<div class="para"><p>[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Barto.<em> Reinforcement Learning: An Introduc-</em><em>tion (2nd ed.)</em>. The MIT Press, 2018.</p></div>
<div class="para"><p>[Szepesvári, 2010] Csaba Szepesvári. Algorithms for reinforcement learning.<em> Synthesis Lectures on Arti-</em><em>ficial Intelligence and Machine Learning</em>, 4(1):1–103, 2010.</p></div>
<div class="para"><p>[Talmor and Berant, 2018] Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions.<em> arXiv preprint arXiv:1803.06643</em>, 2018.</p></div>
<div class="para"><p>[Taori et al., 2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p></div>
<div class="para"><p>[Tay et al., 2020] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.<em> CoRR</em>, abs/2009.06732, 2020.</p></div>
<div class="para"><p>[Team et al., 2024] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al.Gemma 2: Improving open language models at a practical size.<em> arXiv preprint arXiv:2408.00118</em>, 2024.</p></div>
<div class="para"><p>[Teknium, 2023] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,2023. URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.</p></div>
<div class="para"><p>[Timonin et al., 2022] Denis Timonin, BoYang Hsueh, and Vinh Nguyen. Accelerated inference for large transformer models using nvidia triton inference server. https://developer.nvidia.com/blog/accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-tr 2022.</p></div>
<div class="para"><p>[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.<em> arXiv preprint arXiv:2302.13971</em>, 2023a.</p></div>
<div class="para"><p>264 <em>Inference</em></p></div>
<div class="para"><p>[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.<em> arXiv preprint arXiv:2307.09288</em>,2023b.</p></div>
<div class="para"><p>[Uesato et al., 2022] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback.<em> arXiv preprint arXiv:2211.14275</em>, 2022.</p></div>
<div class="para"><p>[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In<em> Proceedings of Advances in</em> <em>Neural Information Processing Systems</em>, volume 30, 2017.</p></div>
<div class="para"><p>[Viterbi, 1967] Andrew J Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.<em> IEEE Transactions on Information Theory</em>, 1967.</p></div>
<div class="para"><p>[Von Oswald et al., 2023] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento,Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent.In<em> Proceedings of International Conference on Machine Learning</em>, pages 35151–35174. PMLR, 2023.</p></div>
<div class="para"><p>[Wang et al., 2024] Chenglong Wang, Hang Zhou, Yimin Hu, Yifu Huo, Bei Li, Tongran Liu, Tong Xiao,and Jingbo Zhu. Esrl: Efficient sampling-based reinforcement learning for sequence generation. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, pages 19107–19115, 2024.</p></div>
<div class="para"><p>[Wang et al., 2023] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application.<em> arXiv preprint arXiv:2302.00487</em>, 2023a.</p></div>
<div class="para"><p>[Wang et al., 2019] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. In<em> Proceedings of the 57th</em> <em>Annual Meeting of the Association for Computational Linguistics</em>, pages 1810–1822, 2019.</p></div>
<div class="para"><p>[Wang et al., 2022] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.Rationale-augmented ensembles in language models.<em> arXiv preprint arXiv:2207.00747</em>, 2022a.</p></div>
<div class="para"><p>[Wang et al., 2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In<em> Proceedings of The Eleventh International Conference on Learning Representations</em>,2023b.</p></div>
<div class="para"><p>[Wang et al., 2022] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia,Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit,and Xudong Shen. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In<em> Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>,pages 5085–5109, 2022b.</p></div>
<div class="para"><p><em>5.4 Summary</em> 265</p></div>
<div class="para"><p>[Wang et al., 2023] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources.<em> Ad-</em><em>vances in Neural Information Processing Systems</em>, 36:74764–74786, 2023c.</p></div>
<div class="para"><p>[Wang et al., 2023] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In<em> Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em><em>(Volume 1: Long Papers)</em>, pages 13484–13508, 2023d.</p></div>
<div class="para"><p>[Wang et al., 2023] Zhenyi Wang, Enneng Yang, Li Shen, and Heng Huang. A comprehensive survey of forgetting in deep learning beyond continual learning.<em> arXiv preprint arXiv:2307.09218</em>, 2023e.</p></div>
<div class="para"><p>[Warstadt et al., 2019] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.<em> Transactions of the Association for Computational Linguistics</em>, 7:625–641, 2019.</p></div>
<div class="para"><p>[Wei et al., 2022] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In<em> Proceedings</em> <em>of International Conference on Learning Representations</em>, 2022a.</p></div>
<div class="para"><p>[Wei et al., 2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.<em> arXiv</em> <em>preprint arXiv:2206.07682</em>, 2022b.</p></div>
<div class="para"><p>[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models.<em> Advances in Neural Information Processing Systems</em>, 35:24824–24837, 2022c.</p></div>
<div class="para"><p>[Welleck et al., 2023] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In<em> Proceedings of The</em> <em>Eleventh International Conference on Learning Representations</em>, 2023.</p></div>
<div class="para"><p>[Weng, 2021] Lilian Weng. How to train really large models on many gpus?<em> lilianweng.github.io</em>, Sep 2021. URL https://lilianweng.github.io/posts/2021-09-25-train-large/.</p></div>
<div class="para"><p>[Wiener, 1960] Norbert Wiener. Some moral and technical consequences of automation: As machines learn they may develop unforeseen strategies at rates that baffle their programmers.<em> Science</em>, 131(3410):1355–1358, 1960.</p></div>
<div class="para"><p>[Williams et al., 2018] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In<em> Proceedings of the 2018 Conference of the North</em> <em>American Chapter of the Association for Computational Linguistics: Human Language Technologies,</em><em>Volume 1 (Long Papers)</em>, pages 1112–1122, 2018.</p></div>
<div class="para"><p>[Williams, 1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.<em> Machine learning</em>, 8:229–256, 1992.</p></div>
<div class="para"><p>[Wingate et al., 2022] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. In<em> Findings</em> <em>of the Association for Computational Linguistics: EMNLP 2022</em>, pages 5621–5634, 2022.</p></div>
<div class="para"><p>[Wozengraft and Reiffen, 1961] John M. Wozengraft and Barney Reiffen.<em> Sequential Decoding</em>. The MIT Press, 1961.</p></div>
<div class="para"><p>[Wu et al., 2023] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun,Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models.<em>arXiv preprint arXiv:2305.05920</em>, 2023a.</p></div>
<div class="para"><p>[Wu et al., 2024] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan for future tokens?<em> arXiv preprint arXiv:2404.00859</em>, 2024.</p></div>
<div class="para"><p>266 <em>Inference</em></p></div>
<div class="para"><p>[Wu et al., 2021] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In<em> Proceedings of International Conference on Learning Representations</em>, 2021.</p></div>
<div class="para"><p>[Wu et al., 2023] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu,Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. In<em> Thirty-seventh Conference on Neural Information Processing</em> <em>Systems</em>, 2023b.</p></div>
<div class="para"><p>[Xia et al., 2024] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.Less: Selecting influential data for targeted instruction tuning.<em> arXiv preprint arXiv:2402.04333</em>, 2024.</p></div>
<div class="para"><p>[Xiao et al., 2024] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In<em> Proceedings of The Twelfth International Conference</em> <em>on Learning Representations</em>, 2024.</p></div>
<div class="para"><p>[Xiao and Zhu, 2023] Tong Xiao and Jingbo Zhu. Introduction to transformers: an nlp perspective.<em> arXiv</em> <em>preprint arXiv:2311.17633</em>, 2023.</p></div>
<div class="para"><p>[Xiao et al., 2013] Tong Xiao, Jingbo Zhu, and Tongran Liu. Bagging and boosting statistical machine translation systems.<em> Artificial Intelligence</em>, 195:496–527, 2013.</p></div>
<div class="para"><p>[Xiao et al., 2019] Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and Tongran Liu. Sharing attention weights for fast transformer. In<em> Proceedings of the Twenty-Eighth International Joint Conference on</em> <em>Artificial Intelligence (IJCAI-19)</em>, pages 5292–5298, 2019.</p></div>
<div class="para"><p>[Xie et al., 2022] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In<em> Proceedings of International Conference on</em> <em>Learning Representations</em>, 2022.</p></div>
<div class="para"><p>[Xin et al., 2020] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for accelerating bert inference. In<em> Proceedings of the 58th Annual Meeting of the Association for</em> <em>Computational Linguistics</em>, pages 2246–2251, 2020.</p></div>
<div class="para"><p>[Xu et al., 2024] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In<em> The Twelfth International Conference on Learning Representations</em>, 2024.</p></div>
<div class="para"><p>[Yang et al., 2024] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report.<em> arXiv preprint</em> <em>arXiv:2412.15115</em>, 2024.</p></div>
<div class="para"><p>[Yang et al., 2019] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding.<em> Advances in</em> <em>neural information processing systems</em>, 32, 2019.</p></div>
<div class="para"><p>[Yao et al., 2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models.<em> Advances in</em> <em>Neural Information Processing Systems</em>, 36, 2024.</p></div>
<div class="para"><p>[Yarowsky, 1995] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods.In<em> Proceedings of the 33rd annual meeting of the association for computational linguistics</em>, pages 189–196, 1995.</p></div>
<div class="para"><p>[Yu et al., 2022] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun.Orca: A distributed serving system for<em> {</em>Transformer-Based<em>}</em> generative models. In<em> 16th USENIX Sym-</em><em>posium on Operating Systems Design and Implementation (OSDI 22)</em>, pages 521–538, 2022.</p></div>
<div class="para"><p>[Yu et al., 2023] Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, and Jiajun Chen. Towards better chain-ofthought prompting strategies: A survey.<em> arXiv preprint arXiv:2310.04959</em>, 2023.</p></div>
<div class="para"><p>[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C. Alberti,S. Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, L. Yang, and A. Ahmed. Big bird: Transformers for longer sequences.<em> Advances in neural information processing systems</em>, 33:17283–17297, 2020.</p></div>
<div class="para"><p><em>5.4 Summary</em> 267</p></div>
<div class="para"><p>[Zellers et al., 2018] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale adversarial dataset for grounded commonsense inference. In<em> Proceedings of the 2018 Conference on</em> <em>Empirical Methods in Natural Language Processing</em>, pages 93–104, 2018.</p></div>
<div class="para"><p>[Zhang and Sennrich, 2019] Biao Zhang and Rico Sennrich. Root mean square layer normalization.<em> Ad-</em><em>vances in Neural Information Processing Systems</em>, 32, 2019.</p></div>
<div class="para"><p>[Zhang et al., 2024] Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-correct reasoning. In<em> ACL (Findings)</em>, 2024.</p></div>
<div class="para"><p>[Zhang et al., 2023] Zhuosheng Zhang, Yao Yao, Aston Zhang, Xiangru Tang, Xinbei Ma, Zhiwei He,Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao.Igniting language intelligence: The hitchhiker’s guide from chain-of-thought reasoning to language agents.<em>arXiv preprint</em> <em>arXiv:2311.11797</em>, 2023a.</p></div>
<div class="para"><p>[Zhang et al., 2023] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In<em> The Eleventh International Conference on Learning Represen-</em><em>tations</em>, 2023b.</p></div>
<div class="para"><p>[Zhao et al., 2024] Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning.<em> arXiv preprint</em> <em>arXiv:2402.04833</em>, 2024.</p></div>
<div class="para"><p>[Zhao et al., 2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen,Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen.A survey of large language models.<em> arXiv preprint arXiv:2303.18223</em>, 2023.</p></div>
<div class="para"><p>[Zhong et al., 2024] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.<em> {</em>DistServe<em>}</em>: Disaggregating prefill and decoding for goodput-optimized large language model serving. In<em> 18th USENIX Symposium on Operating Systems Design and Implementation</em><em>(OSDI 24)</em>, pages 193–210, 2024.</p></div>
<div class="para"><p>[Zhou et al., 2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment.<em> arXiv preprint arXiv:2305.11206</em>, 2023a.</p></div>
<div class="para"><p>[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In<em> Proceedings of The Eleventh International Conference</em> <em>on Learning Representations</em>, 2023b.</p></div>
<div class="para"><p>[Zhou et al., 2020] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. Bert loses patience: Fast and robust inference with early exit.<em> Advances in Neural Information Processing</em> <em>Systems</em>, 33:18330–18341, 2020.</p></div>
<div class="para"><p>[Zhou et al., 2023] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In<em> The Eleventh</em> <em>International Conference on Learning Representations</em>, 2023c.</p></div>
<div class="para"><p>[Zoph and Le, 2016] Barret Zoph and Quoc Le. Neural architecture search with reinforcement learning. In <em>Proceedings of International Conference on Learning Representations</em>, 2016.</p></div>
<div class="para"><p>[Zoph et al., 2020] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk,and Quoc Le. Rethinking pre-training and self-training.<em> Advances in neural information processing</em> <em>systems</em>, 33:3833–3845, 2020.</p></div>
<h1>Index</h1>
<div class="para"><p><em>k</em>-NN, 76 <em>k</em>-NN LM, 76 <em>k</em>-NN language modeling, 76 <em>k</em>-nearest neighbors, 76</p></div>
<div class="para"><p>Extrapolation, 82</p></div>
<div class="para"><p>few-shot COT prompting, 54</p></div>
<div class="para"><p>gated linear unit, 59 gaussian error linear unit, 59 GeLU, 59 GLU, 59 GPT, 1 GQA, 80 Grouped query attention, 80</p></div>
<div class="para"><p>A2C, 178 action-value function, 174 advantage, 178 advantage actor-critic, 178 Agent, 47 ALiBi, 85 alignment, 46 attention with linear biases, 85 automated machine learning, 139 automatic prompt design, 139 AutoML, 139 autonomous agents, 137</p></div>
<div class="para"><p>hard prompts, 142 human preference alignment, 155</p></div>
<div class="para"><p>ICL, 53 ICT, 6 importance sampling, 183 in-context learning, 6, 53, 96 Inference Engine, 225 inference-time scaling, 234 input inversion, 166 instruction alignment, 155 instruction fine-tuning, 43, 157 interference, 29 internal memories, 74 Interpolation, 82 irreducible error, 65 iteration-based scheduling, 226</p></div>
<div class="para"><p>BART, 19 BERT, 1 Best-of-<em>N</em> sampling, 200 BoN sampling, 200 Bradley-Terry model, 181</p></div>
<div class="para"><p>calculation annotation, 115 catastrophic forgetting, 35 causal language modeling, 9 chain of thought, 115 chain-of-thought prompting, 53 completion, 6 compositional generalization, 124 Continuous batching, 226 CoT, 115 COT prompting, 53 cross-lingual language models, 28 cumulative reward, 175</p></div>
<div class="para"><p>key-value cache, 68, 207 KV cache, 68, 207</p></div>
<div class="para"><p>label mapping, 106 Learning from Human Feedback, 47 least-to-most prompting, 121 long-context LLMs, 66</p></div>
<div class="para"><p>Decoding, 209 deliberate-then-generate, 128 demonstrations, 6 direct preference optimization, 193 Document Rotation, 20 DPO, 193 DTG, 128</p></div>
<div class="para"><p>masked language modeling, 1, 9 mBERT, 28 memory-based methods, 74 MQA, 80 multi-lingual BERT, 28 multi-query attention, 80</p></div>
<div class="para"><p>NAS, 139 neural architecture search, 139 next sentence prediction, 12</p></div>
<div class="para"><p>emergent abilities, 64 external memories, 74</p></div>
<div class="para"><p>268</p></div>
<div class="para"><p><em>5.4 Summary</em> 269</p></div>
<div class="para"><p>NSP, 12 nucleus sampling, 216</p></div>
<div class="para"><p>RLHF, 47, 156 RoBERTa, 27</p></div>
<div class="para"><p>offline reinforcement learning, 196 one-shot COT prompting, 54 ORMs, 243 outcome reward models, 243 Outcome-based Approaches, 198 overoptimization problem, 192</p></div>
<div class="para"><p>sample efficient, 167 scaling laws, 63 Scheduler, 225 self-consistency, 132 self-instruct, 163 self-supervised learning, 3 self-training, 3 Sentence Reordering, 20 Sequence Encoding Models, 3 Sequence Generation Models, 4 sequential scaling, 240 SFT, 47, 155 single-round prediction, 158 soft prompts, 142 Span Masking, 19 Speculative decoding, 218 speculative execution, 218 state-value function, 174 Strong Ceiling Performance, 170 Sub-problem Generation, 120 Sub-problem Solving, 120 superficial alignment hypothesis, 167 Supervised Fine-tuning, 47 supervised fine-tuning, 155 supervised learning, 2 surrogate objective, 183</p></div>
<div class="para"><p>parallel scaling, 240 Performance Estimation, 139 performance function, 176 performance gap recovered, 170 permuted language modeling, 11 PGR, 170 Plackett-Luce model, 187 PPO, 51, 184 Prefilling, 207 prefix fine-tuning, 146 prefix language modeling, 16 PRM, 243 problem decomposition, 117 process reward model, 243 Process-based Approaches, 198 prompt embeddings, 150 prompt engineering, 96 prompt optimization, 139 Prompt Search Space, 139 prompting engineering, 51 proximal policy optimization, 51, 184</p></div>
<div class="para"><p>T5, 15 TD, 179 temporal difference, 179 text completion, 110 text transformation, 110 Token Deletion, 19 Token Masking, 19 Transformers, 1 translation language modeling, 29 trust regions, 184</p></div>
<div class="para"><p>Q-value function, 175</p></div>
<div class="para"><p>RAG, 77 ratio function, 183 rectified linear unit, 59 reinforcement learning from human feedback,</p></div>
<div class="para"><p>47, 156 rejection sampling, 201 relation extraction, 109 ReLU, 59 request-level scheduling, 225 retrieval-augmented generation, 77 return, 175 reward gaming, 192 reward hacking, 192 Reward Model, 48</p></div>
<div class="para"><p>unsupervised learning, 2</p></div>
<div class="para"><p>Weak Performance, 170 weak-to-strong generalization, 169 Weak-to-strong Performance, 170</p></div>
<div class="para"><p>XLMs, 28</p></div>
<div class="para"><p>270 <em>Inference</em></p></div>
<div class="para"><p>zero-shot COT, 54 zero-shot learning, 45</p></div>
</body>
</html>